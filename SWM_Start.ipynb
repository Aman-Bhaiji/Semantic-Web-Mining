{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc86a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LEAK CHECK HELPERS ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def show_df_info(df, name=\"df\", nhead=5):\n",
    "    print(f\"== {name} info ==\")\n",
    "    print(\"type:\", type(df))\n",
    "    try:\n",
    "        print(\"shape:\", df.shape)\n",
    "    except:\n",
    "        pass\n",
    "    print(\"columns (count):\", len(df.columns))\n",
    "    print(df.columns.tolist()[:50])\n",
    "    print(\"\\ndtypes:\")\n",
    "    display(df.dtypes)\n",
    "    print(\"\\nhead:\")\n",
    "    display(df.head(nhead))\n",
    "    print(\"\\ntail:\")\n",
    "    display(df.tail(nhead))\n",
    "    print(\"\\nindex sample:\", df.index[:5])\n",
    "\n",
    "def check_column_duplicates(df):\n",
    "    cols = list(df.columns)\n",
    "    dup = [c for c in set(cols) if cols.count(c) > 1]\n",
    "    print(\"Duplicate column names:\", dup)\n",
    "    return dup\n",
    "\n",
    "def near_equal_prop(df, col, ref_col, tol=1e-8):\n",
    "    a = np.asarray(df[col], dtype=float)\n",
    "    b = np.asarray(df[ref_col], dtype=float)\n",
    "    mask = np.isfinite(a) & np.isfinite(b)\n",
    "    if mask.sum() == 0:\n",
    "        return 0.0\n",
    "    return float(np.isclose(a[mask], b[mask], atol=tol, rtol=0).mean())\n",
    "\n",
    "def leak_smoke_tests(model_df):\n",
    "    print(\"**Leak smoke tests**\")\n",
    "    # must be numeric-only check for correlation\n",
    "    num_df = model_df.select_dtypes(include=[np.number]).copy()\n",
    "    print(\"Numeric cols count:\", len(num_df.columns))\n",
    "    # top correlations with target if available\n",
    "    if 'target_thresholded' in num_df.columns:\n",
    "        corr = num_df.corr()['target_thresholded'].abs().sort_values(ascending=False)\n",
    "        print(\"Top correlations with target:\")\n",
    "        display(corr.head(20))\n",
    "    else:\n",
    "        print(\"No 'target_thresholded' in numeric columns.\")\n",
    "    # check near-equality to future columns if present\n",
    "    for ref in ['close_future_h','future_return_h','close_next','future_return_1']:\n",
    "        if ref in model_df.columns:\n",
    "            print(f\"\\nChecking near-equality to {ref}:\")\n",
    "            suspicious = []\n",
    "            for c in model_df.select_dtypes(include=[np.number]).columns:\n",
    "                if c in [ref,'target_thresholded','close_future_h','future_return_h','close_next','future_return_1']:\n",
    "                    continue\n",
    "                p = near_equal_prop(model_df, c, ref)\n",
    "                if p > 0.01:\n",
    "                    suspicious.append((c, p))\n",
    "            if suspicious:\n",
    "                print(\"Suspicious columns (prop > 0.01):\", suspicious)\n",
    "            else:\n",
    "                print(\"No columns nearly equal to\", ref)\n",
    "    print(\"Done smoke tests.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9038fa50-a7c5-4a6b-bdcb-78513589fddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load: C:\\Users\\amanb\\OneDrive\\Desktop\\SWM Project\\CHARTS\\AMAZON30.csv\n",
      "\n",
      "=== Basic load diagnostics ===\n",
      "Raw shape: (7155, 7)\n",
      "\n",
      "First 12 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017.05.12</td>\n",
       "      <td>18:00</td>\n",
       "      <td>960.01</td>\n",
       "      <td>961.24</td>\n",
       "      <td>960.00</td>\n",
       "      <td>960.66</td>\n",
       "      <td>978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017.05.12</td>\n",
       "      <td>18:30</td>\n",
       "      <td>960.65</td>\n",
       "      <td>962.14</td>\n",
       "      <td>960.47</td>\n",
       "      <td>961.89</td>\n",
       "      <td>1017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017.05.12</td>\n",
       "      <td>19:00</td>\n",
       "      <td>961.91</td>\n",
       "      <td>962.76</td>\n",
       "      <td>960.63</td>\n",
       "      <td>961.74</td>\n",
       "      <td>1384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017.05.12</td>\n",
       "      <td>19:30</td>\n",
       "      <td>961.77</td>\n",
       "      <td>962.27</td>\n",
       "      <td>961.13</td>\n",
       "      <td>961.34</td>\n",
       "      <td>2110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>12:30</td>\n",
       "      <td>962.39</td>\n",
       "      <td>962.50</td>\n",
       "      <td>959.97</td>\n",
       "      <td>960.20</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>13:00</td>\n",
       "      <td>960.10</td>\n",
       "      <td>960.70</td>\n",
       "      <td>958.09</td>\n",
       "      <td>958.68</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>13:30</td>\n",
       "      <td>958.66</td>\n",
       "      <td>963.00</td>\n",
       "      <td>957.00</td>\n",
       "      <td>959.97</td>\n",
       "      <td>2512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>14:00</td>\n",
       "      <td>959.98</td>\n",
       "      <td>960.64</td>\n",
       "      <td>956.05</td>\n",
       "      <td>957.54</td>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>14:30</td>\n",
       "      <td>957.56</td>\n",
       "      <td>961.01</td>\n",
       "      <td>957.09</td>\n",
       "      <td>960.16</td>\n",
       "      <td>1727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>15:00</td>\n",
       "      <td>960.15</td>\n",
       "      <td>961.05</td>\n",
       "      <td>958.47</td>\n",
       "      <td>959.44</td>\n",
       "      <td>1424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>15:30</td>\n",
       "      <td>959.46</td>\n",
       "      <td>959.54</td>\n",
       "      <td>957.72</td>\n",
       "      <td>959.37</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>16:00</td>\n",
       "      <td>959.39</td>\n",
       "      <td>959.71</td>\n",
       "      <td>957.33</td>\n",
       "      <td>958.03</td>\n",
       "      <td>1068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date   time    open    high     low   close  volume\n",
       "0   2017.05.12  18:00  960.01  961.24  960.00  960.66     978\n",
       "1   2017.05.12  18:30  960.65  962.14  960.47  961.89    1017\n",
       "2   2017.05.12  19:00  961.91  962.76  960.63  961.74    1384\n",
       "3   2017.05.12  19:30  961.77  962.27  961.13  961.34    2110\n",
       "4   2017.05.15  12:30  962.39  962.50  959.97  960.20     121\n",
       "5   2017.05.15  13:00  960.10  960.70  958.09  958.68     372\n",
       "6   2017.05.15  13:30  958.66  963.00  957.00  959.97    2512\n",
       "7   2017.05.15  14:00  959.98  960.64  956.05  957.54    1887\n",
       "8   2017.05.15  14:30  957.56  961.01  957.09  960.16    1727\n",
       "9   2017.05.15  15:00  960.15  961.05  958.47  959.44    1424\n",
       "10  2017.05.15  15:30  959.46  959.54  957.72  959.37    1357\n",
       "11  2017.05.15  16:00  959.39  959.71  957.33  958.03    1068"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dtypes (before any coercion):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "date       object\n",
       "time       object\n",
       "open      float64\n",
       "high      float64\n",
       "low       float64\n",
       "close     float64\n",
       "volume      int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Check for obvious parsing issues in 'date'/'time' columns (show unique samples):\n",
      "date sample unique values (up to 10): ['2017.05.12' '2017.05.15' '2017.05.16' '2017.05.17' '2017.05.18'\n",
      " '2017.05.19' '2017.05.22' '2017.05.23' '2017.05.24' '2017.05.25']\n",
      "time sample unique values (up to 10): ['18:00' '18:30' '19:00' '19:30' '12:30' '13:00' '13:30' '14:00' '14:30'\n",
      " '15:00']\n",
      "\n",
      "Missing values per column (raw):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "date      0\n",
       "time      0\n",
       "open      0\n",
       "high      0\n",
       "low       0\n",
       "close     0\n",
       "volume    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'open': parsed numeric missing after coercion = 0 / 7155\n",
      "Column 'high': parsed numeric missing after coercion = 0 / 7155\n",
      "Column 'low': parsed numeric missing after coercion = 0 / 7155\n",
      "Column 'close': parsed numeric missing after coercion = 0 / 7155\n",
      "Column 'volume': parsed numeric missing after coercion = 0 / 7155\n",
      "\n",
      "If any of the numeric columns show many missing values above, paste the output and STOP.\n",
      "\n",
      "No non-numeric values detected in the 5 numerical columns (good).\n",
      "\n",
      "=== End of Cell 1 diagnostics ===\n"
     ]
    }
   ],
   "source": [
    "# === CELL 1: Load numerical CSV and run diagnostics ===\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = r\"C:\\Users\\amanb\\OneDrive\\Desktop\\SWM Project\\CHARTS\"\n",
    "FILENAME = \"AMAZON30.csv\"\n",
    "filepath = os.path.join(DATA_DIR, FILENAME)\n",
    "\n",
    "print(\"Attempting to load:\", filepath)\n",
    "if not os.path.exists(filepath):\n",
    "    raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "\n",
    "# Try to read with tab/whitespace separator first (sample looked tab-separated).\n",
    "# If that fails, fall back to comma.\n",
    "read_errors = []\n",
    "for sep in [\"\\t\", r\"\\s+\", \",\"]:\n",
    "    try:\n",
    "        df_raw = pd.read_csv(filepath, sep=sep, header=None, engine=\"python\")\n",
    "        # Heuristic: if we read expected 7 columns, accept it\n",
    "        if df_raw.shape[1] >= 7:\n",
    "            break\n",
    "    except Exception as e:\n",
    "        read_errors.append((sep, str(e)))\n",
    "else:\n",
    "    # if loop didn't break, raise with debug info\n",
    "    raise RuntimeError(\"Failed to read CSV with common separators. Attempts: \" + str(read_errors))\n",
    "\n",
    "# Assign expected column names (date, time, open, high, low, close, volume)\n",
    "expected_cols = ['date','time','open','high','low','close','volume']\n",
    "# If df_raw has more than 7 cols, take the first 7 as those values\n",
    "if df_raw.shape[1] < 7:\n",
    "    raise RuntimeError(f\"File read produced {df_raw.shape[1]} columns (<7). Please inspect the file.\")\n",
    "df_raw = df_raw.iloc[:, :7]\n",
    "df_raw.columns = expected_cols\n",
    "\n",
    "# create df copy for downstream operations\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Diagnostics:\n",
    "print(\"\\n=== Basic load diagnostics ===\")\n",
    "print(\"Raw shape:\", df_raw.shape)\n",
    "print(\"\\nFirst 12 rows:\")\n",
    "display(df.head(12))\n",
    "\n",
    "print(\"\\nDtypes (before any coercion):\")\n",
    "display(df.dtypes)\n",
    "\n",
    "print(\"\\nCheck for obvious parsing issues in 'date'/'time' columns (show unique samples):\")\n",
    "print(\"date sample unique values (up to 10):\", df['date'].unique()[:10])\n",
    "print(\"time sample unique values (up to 10):\", df['time'].unique()[:10])\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column (raw):\")\n",
    "display(df.isna().sum())\n",
    "\n",
    "# Quick sanity: try to coerce numeric columns and show stats (without overwriting df yet)\n",
    "for c in ['open','high','low','close','volume']:\n",
    "    coerced = pd.to_numeric(df[c], errors='coerce')\n",
    "    n_missing = coerced.isna().sum()\n",
    "    print(f\"Column '{c}': parsed numeric missing after coercion = {n_missing} / {len(df)}\")\n",
    "print(\"\\nIf any of the numeric columns show many missing values above, paste the output and STOP.\")\n",
    "\n",
    "# Print a small sample of rows where numeric coercion failed (if any)\n",
    "mask_any_na = False\n",
    "for c in ['open','high','low','close','volume']:\n",
    "    coerced = pd.to_numeric(df[c], errors='coerce')\n",
    "    if coerced.isna().any():\n",
    "        mask_any_na = True\n",
    "        print(f\"\\nRows with non-numeric '{c}':\")\n",
    "        display(df[coerced.isna()].head(10))\n",
    "\n",
    "if not mask_any_na:\n",
    "    print(\"\\nNo non-numeric values detected in the 5 numerical columns (good).\")\n",
    "\n",
    "print(\"\\n=== End of Cell 1 diagnostics ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6174ebdb-1b70-4f06-9e96-648a8a735318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed timestamps. Unparsable count: 0\n",
      "\n",
      "Missing values per column after conversion:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "open      0\n",
       "high      0\n",
       "low       0\n",
       "close     0\n",
       "volume    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index range: 2017-05-12 18:00:00 to 2019-02-01 20:00:00\n",
      "Total rows: 7155\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ts_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-05-12 18:00:00</th>\n",
       "      <td>2017.05.12</td>\n",
       "      <td>18:00</td>\n",
       "      <td>960.01</td>\n",
       "      <td>961.24</td>\n",
       "      <td>960.00</td>\n",
       "      <td>960.66</td>\n",
       "      <td>978</td>\n",
       "      <td>2017.05.12 18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-12 18:30:00</th>\n",
       "      <td>2017.05.12</td>\n",
       "      <td>18:30</td>\n",
       "      <td>960.65</td>\n",
       "      <td>962.14</td>\n",
       "      <td>960.47</td>\n",
       "      <td>961.89</td>\n",
       "      <td>1017</td>\n",
       "      <td>2017.05.12 18:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-12 19:00:00</th>\n",
       "      <td>2017.05.12</td>\n",
       "      <td>19:00</td>\n",
       "      <td>961.91</td>\n",
       "      <td>962.76</td>\n",
       "      <td>960.63</td>\n",
       "      <td>961.74</td>\n",
       "      <td>1384</td>\n",
       "      <td>2017.05.12 19:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-12 19:30:00</th>\n",
       "      <td>2017.05.12</td>\n",
       "      <td>19:30</td>\n",
       "      <td>961.77</td>\n",
       "      <td>962.27</td>\n",
       "      <td>961.13</td>\n",
       "      <td>961.34</td>\n",
       "      <td>2110</td>\n",
       "      <td>2017.05.12 19:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 12:30:00</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>12:30</td>\n",
       "      <td>962.39</td>\n",
       "      <td>962.50</td>\n",
       "      <td>959.97</td>\n",
       "      <td>960.20</td>\n",
       "      <td>121</td>\n",
       "      <td>2017.05.15 12:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 13:00:00</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>13:00</td>\n",
       "      <td>960.10</td>\n",
       "      <td>960.70</td>\n",
       "      <td>958.09</td>\n",
       "      <td>958.68</td>\n",
       "      <td>372</td>\n",
       "      <td>2017.05.15 13:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 13:30:00</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>13:30</td>\n",
       "      <td>958.66</td>\n",
       "      <td>963.00</td>\n",
       "      <td>957.00</td>\n",
       "      <td>959.97</td>\n",
       "      <td>2512</td>\n",
       "      <td>2017.05.15 13:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 14:00:00</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>14:00</td>\n",
       "      <td>959.98</td>\n",
       "      <td>960.64</td>\n",
       "      <td>956.05</td>\n",
       "      <td>957.54</td>\n",
       "      <td>1887</td>\n",
       "      <td>2017.05.15 14:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 14:30:00</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>14:30</td>\n",
       "      <td>957.56</td>\n",
       "      <td>961.01</td>\n",
       "      <td>957.09</td>\n",
       "      <td>960.16</td>\n",
       "      <td>1727</td>\n",
       "      <td>2017.05.15 14:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 15:00:00</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>15:00</td>\n",
       "      <td>960.15</td>\n",
       "      <td>961.05</td>\n",
       "      <td>958.47</td>\n",
       "      <td>959.44</td>\n",
       "      <td>1424</td>\n",
       "      <td>2017.05.15 15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 15:30:00</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>15:30</td>\n",
       "      <td>959.46</td>\n",
       "      <td>959.54</td>\n",
       "      <td>957.72</td>\n",
       "      <td>959.37</td>\n",
       "      <td>1357</td>\n",
       "      <td>2017.05.15 15:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 16:00:00</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>16:00</td>\n",
       "      <td>959.39</td>\n",
       "      <td>959.71</td>\n",
       "      <td>957.33</td>\n",
       "      <td>958.03</td>\n",
       "      <td>1068</td>\n",
       "      <td>2017.05.15 16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 16:30:00</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>16:30</td>\n",
       "      <td>957.99</td>\n",
       "      <td>958.81</td>\n",
       "      <td>956.97</td>\n",
       "      <td>957.61</td>\n",
       "      <td>1015</td>\n",
       "      <td>2017.05.15 16:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 17:00:00</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>17:00</td>\n",
       "      <td>957.62</td>\n",
       "      <td>958.34</td>\n",
       "      <td>957.51</td>\n",
       "      <td>957.59</td>\n",
       "      <td>687</td>\n",
       "      <td>2017.05.15 17:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 17:30:00</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>17:30</td>\n",
       "      <td>957.57</td>\n",
       "      <td>957.95</td>\n",
       "      <td>957.07</td>\n",
       "      <td>957.75</td>\n",
       "      <td>781</td>\n",
       "      <td>2017.05.15 17:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date   time    open    high     low   close  \\\n",
       "ts                                                                       \n",
       "2017-05-12 18:00:00  2017.05.12  18:00  960.01  961.24  960.00  960.66   \n",
       "2017-05-12 18:30:00  2017.05.12  18:30  960.65  962.14  960.47  961.89   \n",
       "2017-05-12 19:00:00  2017.05.12  19:00  961.91  962.76  960.63  961.74   \n",
       "2017-05-12 19:30:00  2017.05.12  19:30  961.77  962.27  961.13  961.34   \n",
       "2017-05-15 12:30:00  2017.05.15  12:30  962.39  962.50  959.97  960.20   \n",
       "2017-05-15 13:00:00  2017.05.15  13:00  960.10  960.70  958.09  958.68   \n",
       "2017-05-15 13:30:00  2017.05.15  13:30  958.66  963.00  957.00  959.97   \n",
       "2017-05-15 14:00:00  2017.05.15  14:00  959.98  960.64  956.05  957.54   \n",
       "2017-05-15 14:30:00  2017.05.15  14:30  957.56  961.01  957.09  960.16   \n",
       "2017-05-15 15:00:00  2017.05.15  15:00  960.15  961.05  958.47  959.44   \n",
       "2017-05-15 15:30:00  2017.05.15  15:30  959.46  959.54  957.72  959.37   \n",
       "2017-05-15 16:00:00  2017.05.15  16:00  959.39  959.71  957.33  958.03   \n",
       "2017-05-15 16:30:00  2017.05.15  16:30  957.99  958.81  956.97  957.61   \n",
       "2017-05-15 17:00:00  2017.05.15  17:00  957.62  958.34  957.51  957.59   \n",
       "2017-05-15 17:30:00  2017.05.15  17:30  957.57  957.95  957.07  957.75   \n",
       "\n",
       "                     volume            ts_str  \n",
       "ts                                             \n",
       "2017-05-12 18:00:00     978  2017.05.12 18:00  \n",
       "2017-05-12 18:30:00    1017  2017.05.12 18:30  \n",
       "2017-05-12 19:00:00    1384  2017.05.12 19:00  \n",
       "2017-05-12 19:30:00    2110  2017.05.12 19:30  \n",
       "2017-05-15 12:30:00     121  2017.05.15 12:30  \n",
       "2017-05-15 13:00:00     372  2017.05.15 13:00  \n",
       "2017-05-15 13:30:00    2512  2017.05.15 13:30  \n",
       "2017-05-15 14:00:00    1887  2017.05.15 14:00  \n",
       "2017-05-15 14:30:00    1727  2017.05.15 14:30  \n",
       "2017-05-15 15:00:00    1424  2017.05.15 15:00  \n",
       "2017-05-15 15:30:00    1357  2017.05.15 15:30  \n",
       "2017-05-15 16:00:00    1068  2017.05.15 16:00  \n",
       "2017-05-15 16:30:00    1015  2017.05.15 16:30  \n",
       "2017-05-15 17:00:00     687  2017.05.15 17:00  \n",
       "2017-05-15 17:30:00     781  2017.05.15 17:30  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate timestamps: 0\n",
      "\n",
      "Top gaps (minutes) between consecutive rows (largest first):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ts\n",
       "2017-05-30 12:30:00    5340.0\n",
       "2017-09-05 12:30:00    5340.0\n",
       "2017-12-26 13:30:00    5340.0\n",
       "2018-01-02 13:30:00    5340.0\n",
       "2018-01-16 13:30:00    5340.0\n",
       "2018-02-20 13:30:00    5340.0\n",
       "2018-05-29 12:30:00    5340.0\n",
       "2018-09-04 12:30:00    5340.0\n",
       "2019-01-22 13:30:00    5340.0\n",
       "2018-04-02 12:30:00    5280.0\n",
       "2017-11-27 13:30:00    4080.0\n",
       "2018-11-26 13:30:00    4050.0\n",
       "2017-11-06 13:30:00    3960.0\n",
       "2018-11-05 13:30:00    3960.0\n",
       "2017-05-15 12:30:00    3900.0\n",
       "2017-05-22 12:30:00    3900.0\n",
       "2017-06-05 12:30:00    3900.0\n",
       "2017-06-12 12:30:00    3900.0\n",
       "2017-06-19 12:30:00    3900.0\n",
       "2017-06-26 12:30:00    3900.0\n",
       "Name: ts, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Volume percentiles:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00       1.00\n",
       "0.01      20.54\n",
       "0.05      63.00\n",
       "0.25     856.50\n",
       "0.50    1578.00\n",
       "0.75    2464.50\n",
       "0.95    3753.00\n",
       "0.99    4488.84\n",
       "1.00    5639.00\n",
       "Name: volume, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== End of CELL 2 diagnostics ===\n"
     ]
    }
   ],
   "source": [
    "# === CELL 2: parse date+time into datetime index, set index, and diagnostics ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# df should exist from CELL 1\n",
    "if 'df' not in globals():\n",
    "    raise RuntimeError(\"DataFrame 'df' not found. Run CELL 1 first.\")\n",
    "\n",
    "# Create a combined timestamp string and try parsing with known format\n",
    "ts_str = df['date'].astype(str).str.strip() + \" \" + df['time'].astype(str).str.strip()\n",
    "\n",
    "# Try parsing with a couple of formats; fall back to pandas' parser if needed\n",
    "parse_errors = 0\n",
    "try:\n",
    "    ts = pd.to_datetime(ts_str, format=\"%Y.%m.%d %H:%M\", errors='coerce')\n",
    "    parse_errors = ts.isna().sum()\n",
    "    if parse_errors > 0:\n",
    "        # try a more flexible parse if exact format didn't work for some rows\n",
    "        ts2 = pd.to_datetime(ts_str, errors='coerce', infer_datetime_format=True)\n",
    "        parse_errors = ts2.isna().sum()\n",
    "        ts = ts2\n",
    "except Exception as e:\n",
    "    # fallback\n",
    "    ts = pd.to_datetime(ts_str, errors='coerce', infer_datetime_format=True)\n",
    "    parse_errors = ts.isna().sum()\n",
    "\n",
    "# attach ts to df\n",
    "df = df.copy()\n",
    "df['ts'] = ts\n",
    "df['ts_str'] = ts_str  # keep original string for debugging if needed\n",
    "\n",
    "print(\"Parsed timestamps. Unparsable count:\", parse_errors)\n",
    "\n",
    "# Drop rows with unparsable timestamps (should be zero). If non-zero, show samples.\n",
    "if parse_errors > 0:\n",
    "    print(\"\\nSample rows with unparsable timestamps:\")\n",
    "    display(df[df['ts'].isna()].head(20))\n",
    "    raise RuntimeError(\"Some timestamps could not be parsed. Fix the source or adjust parsing logic.\")\n",
    "\n",
    "# Set datetime index and sort\n",
    "df.index = df['ts']\n",
    "df.index.name = 'ts'\n",
    "df = df.sort_index()\n",
    "\n",
    "# Quick checks\n",
    "print(\"\\nMissing values per column after conversion:\")\n",
    "display(df[['open','high','low','close','volume']].isna().sum())\n",
    "\n",
    "print(\"\\nIndex range: {} to {}\".format(df.index.min(), df.index.max()))\n",
    "print(\"Total rows:\", len(df))\n",
    "\n",
    "# Show small sample with index as datetime\n",
    "display(df.loc[df.index[:15], ['date','time','open','high','low','close','volume','ts_str']])\n",
    "\n",
    "# Duplicate timestamp check\n",
    "dup_count = df.index.duplicated().sum()\n",
    "print(\"\\nDuplicate timestamps:\", dup_count)\n",
    "\n",
    "# Largest gaps between consecutive timestamps (minutes)\n",
    "diffs_min = df.index.to_series().diff().dt.total_seconds().div(60)\n",
    "top_gaps = diffs_min.nlargest(20)\n",
    "print(\"\\nTop gaps (minutes) between consecutive rows (largest first):\")\n",
    "display(top_gaps)\n",
    "\n",
    "# Volume percentiles (sanity)\n",
    "print(\"\\nVolume percentiles:\")\n",
    "display(df['volume'].quantile([0,0.01,0.05,0.25,0.5,0.75,0.95,0.99,1.0]))\n",
    "\n",
    "# keep df back in globals\n",
    "globals()['df'] = df\n",
    "\n",
    "print(\"\\n=== End of CELL 2 diagnostics ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e9c7d6c-8b09-4001-9fc3-18f5bccb0665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-engineering complete.\n",
      "Shape before dropna: (7155, 27)\n",
      "\n",
      "Top columns by missing values (should mainly be from first few rows):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "roll_std_6        2\n",
       "roll_std_3        2\n",
       "vol_ratio_3_12    2\n",
       "roll_std_12       2\n",
       "log_return_1      1\n",
       "return_1          1\n",
       "roll_min_3        1\n",
       "roll_mean_6       1\n",
       "roll_max_3        1\n",
       "roll_mean_3       1\n",
       "roll_min_6        1\n",
       "roll_max_6        1\n",
       "roll_max_12       1\n",
       "roll_mean_12      1\n",
       "roll_min_12       1\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropna: (7153, 27)\n",
      "\n",
      "Sample of engineered columns (tail):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>return_1</th>\n",
       "      <th>log_return_1</th>\n",
       "      <th>roll_mean_3</th>\n",
       "      <th>roll_std_3</th>\n",
       "      <th>mom_3</th>\n",
       "      <th>vol_change</th>\n",
       "      <th>vol_roll_mean_6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-02-01 15:30:00</th>\n",
       "      <td>1643.65</td>\n",
       "      <td>-0.003232</td>\n",
       "      <td>-0.003238</td>\n",
       "      <td>1643.160000</td>\n",
       "      <td>6.029959</td>\n",
       "      <td>5.820000</td>\n",
       "      <td>-0.231334</td>\n",
       "      <td>2940.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 16:00:00</th>\n",
       "      <td>1646.63</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>1645.396667</td>\n",
       "      <td>3.103584</td>\n",
       "      <td>-1.746667</td>\n",
       "      <td>-0.021132</td>\n",
       "      <td>2753.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 16:30:00</th>\n",
       "      <td>1648.77</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>1646.420000</td>\n",
       "      <td>2.671198</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>-0.045961</td>\n",
       "      <td>2854.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 17:00:00</th>\n",
       "      <td>1649.28</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>1646.350000</td>\n",
       "      <td>2.571459</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>-0.154015</td>\n",
       "      <td>3152.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 17:30:00</th>\n",
       "      <td>1646.83</td>\n",
       "      <td>-0.001485</td>\n",
       "      <td>-0.001487</td>\n",
       "      <td>1648.226667</td>\n",
       "      <td>1.406070</td>\n",
       "      <td>1.053333</td>\n",
       "      <td>0.066437</td>\n",
       "      <td>3350.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 18:00:00</th>\n",
       "      <td>1642.00</td>\n",
       "      <td>-0.002933</td>\n",
       "      <td>-0.002937</td>\n",
       "      <td>1648.293333</td>\n",
       "      <td>1.292685</td>\n",
       "      <td>-1.463333</td>\n",
       "      <td>0.139563</td>\n",
       "      <td>2858.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 18:30:00</th>\n",
       "      <td>1632.75</td>\n",
       "      <td>-0.005633</td>\n",
       "      <td>-0.005649</td>\n",
       "      <td>1646.036667</td>\n",
       "      <td>3.704272</td>\n",
       "      <td>-4.036667</td>\n",
       "      <td>0.347178</td>\n",
       "      <td>2692.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 19:00:00</th>\n",
       "      <td>1629.19</td>\n",
       "      <td>-0.002180</td>\n",
       "      <td>-0.002183</td>\n",
       "      <td>1640.526667</td>\n",
       "      <td>7.154693</td>\n",
       "      <td>-7.776667</td>\n",
       "      <td>0.144137</td>\n",
       "      <td>2835.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 19:30:00</th>\n",
       "      <td>1627.16</td>\n",
       "      <td>-0.001246</td>\n",
       "      <td>-0.001247</td>\n",
       "      <td>1634.646667</td>\n",
       "      <td>6.612264</td>\n",
       "      <td>-5.456667</td>\n",
       "      <td>-0.092584</td>\n",
       "      <td>3080.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 20:00:00</th>\n",
       "      <td>1624.95</td>\n",
       "      <td>-0.001358</td>\n",
       "      <td>-0.001359</td>\n",
       "      <td>1629.700000</td>\n",
       "      <td>2.829682</td>\n",
       "      <td>-2.540000</td>\n",
       "      <td>-0.647716</td>\n",
       "      <td>3280.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       close  return_1  log_return_1  roll_mean_3  roll_std_3  \\\n",
       "ts                                                                              \n",
       "2019-02-01 15:30:00  1643.65 -0.003232     -0.003238  1643.160000    6.029959   \n",
       "2019-02-01 16:00:00  1646.63  0.001813      0.001811  1645.396667    3.103584   \n",
       "2019-02-01 16:30:00  1648.77  0.001300      0.001299  1646.420000    2.671198   \n",
       "2019-02-01 17:00:00  1649.28  0.000309      0.000309  1646.350000    2.571459   \n",
       "2019-02-01 17:30:00  1646.83 -0.001485     -0.001487  1648.226667    1.406070   \n",
       "2019-02-01 18:00:00  1642.00 -0.002933     -0.002937  1648.293333    1.292685   \n",
       "2019-02-01 18:30:00  1632.75 -0.005633     -0.005649  1646.036667    3.704272   \n",
       "2019-02-01 19:00:00  1629.19 -0.002180     -0.002183  1640.526667    7.154693   \n",
       "2019-02-01 19:30:00  1627.16 -0.001246     -0.001247  1634.646667    6.612264   \n",
       "2019-02-01 20:00:00  1624.95 -0.001358     -0.001359  1629.700000    2.829682   \n",
       "\n",
       "                        mom_3  vol_change  vol_roll_mean_6  \n",
       "ts                                                          \n",
       "2019-02-01 15:30:00  5.820000   -0.231334      2940.833333  \n",
       "2019-02-01 16:00:00 -1.746667   -0.021132      2753.500000  \n",
       "2019-02-01 16:30:00  0.210000   -0.045961      2854.333333  \n",
       "2019-02-01 17:00:00  2.420000   -0.154015      3152.500000  \n",
       "2019-02-01 17:30:00  1.053333    0.066437      3350.000000  \n",
       "2019-02-01 18:00:00 -1.463333    0.139563      2858.833333  \n",
       "2019-02-01 18:30:00 -4.036667    0.347178      2692.166667  \n",
       "2019-02-01 19:00:00 -7.776667    0.144137      2835.666667  \n",
       "2019-02-01 19:30:00 -5.456667   -0.092584      3080.666667  \n",
       "2019-02-01 20:00:00 -2.540000   -0.647716      3280.666667  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== End of CELL 3 diagnostics ===\n"
     ]
    }
   ],
   "source": [
    "# === CELL 3: Safe feature engineering (lagged/rolling only, no future info) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Work on a copy of df\n",
    "df_feat = df.copy()\n",
    "\n",
    "# Basic lag features\n",
    "df_feat['return_1'] = df_feat['close'].pct_change()               # current return (still safe)\n",
    "df_feat['log_return_1'] = np.log(df_feat['close']).diff()\n",
    "\n",
    "# Rolling statistics — all shifted by 1 to use only past data\n",
    "wins = [3, 6, 12]\n",
    "for w in wins:\n",
    "    df_feat[f'roll_mean_{w}'] = df_feat['close'].shift(1).rolling(window=w, min_periods=1).mean()\n",
    "    df_feat[f'roll_std_{w}']  = df_feat['close'].shift(1).rolling(window=w, min_periods=1).std()\n",
    "    df_feat[f'roll_max_{w}']  = df_feat['close'].shift(1).rolling(window=w, min_periods=1).max()\n",
    "    df_feat[f'roll_min_{w}']  = df_feat['close'].shift(1).rolling(window=w, min_periods=1).min()\n",
    "\n",
    "# Momentum / volatility ratio features\n",
    "df_feat['mom_3'] = df_feat['close'].shift(1) - df_feat['roll_mean_3']\n",
    "df_feat['vol_ratio_3_12'] = df_feat['roll_std_3'] / (df_feat['roll_std_12'] + 1e-9)\n",
    "\n",
    "# Volume dynamics\n",
    "df_feat['vol_change'] = df_feat['volume'].pct_change()\n",
    "df_feat['vol_roll_mean_6'] = df_feat['volume'].shift(1).rolling(6, min_periods=1).mean()\n",
    "\n",
    "# Diagnostics\n",
    "print(\"Feature-engineering complete.\")\n",
    "print(\"Shape before dropna:\", df_feat.shape)\n",
    "nan_summary = df_feat.isna().sum()\n",
    "print(\"\\nTop columns by missing values (should mainly be from first few rows):\")\n",
    "display(nan_summary[nan_summary > 0].sort_values(ascending=False).head(15))\n",
    "\n",
    "# Drop NaNs produced by rolling (safe — only first few rows)\n",
    "df_feat = df_feat.dropna().copy()\n",
    "print(\"Shape after dropna:\", df_feat.shape)\n",
    "\n",
    "# Show sample of engineered features\n",
    "print(\"\\nSample of engineered columns (tail):\")\n",
    "cols_to_show = ['close','return_1','log_return_1','roll_mean_3','roll_std_3','mom_3','vol_change','vol_roll_mean_6']\n",
    "display(df_feat[cols_to_show].tail(10))\n",
    "\n",
    "print(\"\\n=== End of CELL 3 diagnostics ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8421545-1ffa-461b-a668-75a774f78ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after adding target and dropping NaNs: 7152\n",
      "\n",
      "Target class distribution (counts):\n",
      "target_thresholded\n",
      "0    4236\n",
      "1    2916\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target class distribution (proportions):\n",
      "target_thresholded\n",
      "0    0.592282\n",
      "1    0.407718\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>close_future_h</th>\n",
       "      <th>future_return_h</th>\n",
       "      <th>target_thresholded</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-02-01 15:00:00</th>\n",
       "      <td>1648.98</td>\n",
       "      <td>1643.65</td>\n",
       "      <td>-0.003232</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 15:30:00</th>\n",
       "      <td>1643.65</td>\n",
       "      <td>1646.63</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 16:00:00</th>\n",
       "      <td>1646.63</td>\n",
       "      <td>1648.77</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 16:30:00</th>\n",
       "      <td>1648.77</td>\n",
       "      <td>1649.28</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 17:00:00</th>\n",
       "      <td>1649.28</td>\n",
       "      <td>1646.83</td>\n",
       "      <td>-0.001485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 17:30:00</th>\n",
       "      <td>1646.83</td>\n",
       "      <td>1642.00</td>\n",
       "      <td>-0.002933</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 18:00:00</th>\n",
       "      <td>1642.00</td>\n",
       "      <td>1632.75</td>\n",
       "      <td>-0.005633</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 18:30:00</th>\n",
       "      <td>1632.75</td>\n",
       "      <td>1629.19</td>\n",
       "      <td>-0.002180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 19:00:00</th>\n",
       "      <td>1629.19</td>\n",
       "      <td>1627.16</td>\n",
       "      <td>-0.001246</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 19:30:00</th>\n",
       "      <td>1627.16</td>\n",
       "      <td>1624.95</td>\n",
       "      <td>-0.001358</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       close  close_future_h  future_return_h  \\\n",
       "ts                                                              \n",
       "2019-02-01 15:00:00  1648.98         1643.65        -0.003232   \n",
       "2019-02-01 15:30:00  1643.65         1646.63         0.001813   \n",
       "2019-02-01 16:00:00  1646.63         1648.77         0.001300   \n",
       "2019-02-01 16:30:00  1648.77         1649.28         0.000309   \n",
       "2019-02-01 17:00:00  1649.28         1646.83        -0.001485   \n",
       "2019-02-01 17:30:00  1646.83         1642.00        -0.002933   \n",
       "2019-02-01 18:00:00  1642.00         1632.75        -0.005633   \n",
       "2019-02-01 18:30:00  1632.75         1629.19        -0.002180   \n",
       "2019-02-01 19:00:00  1629.19         1627.16        -0.001246   \n",
       "2019-02-01 19:30:00  1627.16         1624.95        -0.001358   \n",
       "\n",
       "                     target_thresholded  \n",
       "ts                                       \n",
       "2019-02-01 15:00:00                   0  \n",
       "2019-02-01 15:30:00                   1  \n",
       "2019-02-01 16:00:00                   1  \n",
       "2019-02-01 16:30:00                   0  \n",
       "2019-02-01 17:00:00                   0  \n",
       "2019-02-01 17:30:00                   0  \n",
       "2019-02-01 18:00:00                   0  \n",
       "2019-02-01 18:30:00                   0  \n",
       "2019-02-01 19:00:00                   0  \n",
       "2019-02-01 19:30:00                   0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target type: <class 'pandas.core.series.Series'>  dtype: int64\n",
      "\n",
      "Duplicate column names (if any): []\n",
      "\n",
      "Top 20 numeric features by absolute correlation with target_thresholded:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target_thresholded    1.000000\n",
       "future_return_h       0.592738\n",
       "roll_std_3            0.046107\n",
       "roll_std_6            0.043691\n",
       "roll_std_12           0.041818\n",
       "close_future_h        0.032048\n",
       "vol_ratio_3_12        0.030196\n",
       "volume                0.029969\n",
       "high                  0.020800\n",
       "roll_max_6            0.020717\n",
       "roll_max_3            0.020704\n",
       "roll_max_12           0.020682\n",
       "open                  0.020433\n",
       "roll_mean_3           0.020270\n",
       "close                 0.020164\n",
       "roll_mean_6           0.020139\n",
       "low                   0.019945\n",
       "roll_mean_12          0.019790\n",
       "roll_min_3            0.019741\n",
       "roll_min_6            0.019152\n",
       "Name: target_thresholded, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Near-equality checks (proportion of rows where feature ≈ future):\n",
      "  No suspicious columns nearly equal to close_future_h (>1%).\n",
      "  No suspicious columns nearly equal to future_return_h (>1%).\n",
      "\n",
      "=== End of CELL 4 (fixed) diagnostics ===\n"
     ]
    }
   ],
   "source": [
    "# === CELL 4 (fixed) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Use df_feat from previous cell\n",
    "if 'df_feat' not in globals():\n",
    "    raise RuntimeError(\"df_feat not found. Run CELL 3 first.\")\n",
    "\n",
    "df_label = df_feat.copy()\n",
    "\n",
    "# Parameters\n",
    "HORIZON = globals().get('HORIZON', 1)\n",
    "THRESHOLD = globals().get('THRESHOLD', 0.0005)\n",
    "\n",
    "# Create future close and return (safe)\n",
    "df_label['close_future_h'] = df_label['close'].shift(-HORIZON)\n",
    "df_label['future_return_h'] = (df_label['close_future_h'] / df_label['close']) - 1\n",
    "\n",
    "# Binary target\n",
    "df_label['target_thresholded'] = (df_label['future_return_h'] > THRESHOLD).astype(int)\n",
    "\n",
    "# Drop rows where future_return_h is NaN (last horizon rows)\n",
    "df_label = df_label.dropna(subset=['future_return_h']).copy()\n",
    "\n",
    "# Basic prints\n",
    "print(\"Rows after adding target and dropping NaNs:\", len(df_label))\n",
    "print(\"\\nTarget class distribution (counts):\")\n",
    "print(df_label['target_thresholded'].value_counts())\n",
    "print(\"\\nTarget class distribution (proportions):\")\n",
    "print(df_label['target_thresholded'].value_counts(normalize=True))\n",
    "\n",
    "# Show a sample of last few rows for verification\n",
    "display(df_label[['close','close_future_h','future_return_h','target_thresholded']].tail(10))\n",
    "\n",
    "# Ensure target is 1-D numeric Series\n",
    "t = df_label['target_thresholded']\n",
    "print(\"\\nTarget type:\", type(t), \" dtype:\", getattr(t, 'dtype', None))\n",
    "# If by any chance it's a DataFrame-like, coerce to Series\n",
    "if isinstance(t, pd.DataFrame):\n",
    "    print(\"WARNING: target is a DataFrame — selecting the rightmost column as the target.\")\n",
    "    df_label['target_thresholded'] = t.iloc[:, -1].astype(int)\n",
    "\n",
    "# Check duplicate column names\n",
    "cols = list(df_label.columns)\n",
    "dups = [c for c in set(cols) if cols.count(c) > 1]\n",
    "print(\"\\nDuplicate column names (if any):\", dups)\n",
    "\n",
    "# Correlations: only numeric columns (safe)\n",
    "num_df = df_label.select_dtypes(include=[np.number]).copy()\n",
    "if 'target_thresholded' not in num_df.columns:\n",
    "    raise RuntimeError(\"'target_thresholded' not in numeric columns — unexpected.\")\n",
    "\n",
    "corr_with_target = num_df.corr()['target_thresholded'].abs().sort_values(ascending=False)\n",
    "print(\"\\nTop 20 numeric features by absolute correlation with target_thresholded:\")\n",
    "display(corr_with_target.head(20))\n",
    "\n",
    "# Simple near-equality leak smoke tests (checks if any feature equals the future close/return often)\n",
    "def near_equal_prop(arr_a, arr_b, tol=1e-8):\n",
    "    a = np.asarray(arr_a, dtype=float)\n",
    "    b = np.asarray(arr_b, dtype=float)\n",
    "    mask = np.isfinite(a) & np.isfinite(b)\n",
    "    if mask.sum() == 0:\n",
    "        return 0.0\n",
    "    return float(np.isclose(a[mask], b[mask], atol=tol, rtol=0).mean())\n",
    "\n",
    "print(\"\\nNear-equality checks (proportion of rows where feature ≈ future):\")\n",
    "for ref in ['close_future_h', 'future_return_h']:\n",
    "    if ref not in num_df.columns:\n",
    "        print(f\"  {ref}: (not present in numeric columns)\")\n",
    "        continue\n",
    "    suspicious = []\n",
    "    for c in num_df.columns:\n",
    "        if c in [ref, 'target_thresholded']:\n",
    "            continue\n",
    "        p = near_equal_prop(num_df[c], num_df[ref], tol=1e-8)\n",
    "        if p > 0.01:  # more than 1% equal -> suspicious\n",
    "            suspicious.append((c, p))\n",
    "    if suspicious:\n",
    "        print(f\"  Suspicious near-equalities to {ref}:\")\n",
    "        for c,p in suspicious:\n",
    "            print(f\"    {c}: {p:.4f}\")\n",
    "    else:\n",
    "        print(f\"  No suspicious columns nearly equal to {ref} (>1%).\")\n",
    "\n",
    "print(\"\\n=== End of CELL 4 (fixed) diagnostics ===\")\n",
    "\n",
    "# Keep df_label in globals for next cells\n",
    "globals()['df_label'] = df_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a9aabb2-5a26-418a-adfa-1bcf9ead9817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed timestamps. Unparsable count: 0\n",
      "\n",
      "Missing values per column after conversion:\n",
      "open      0\n",
      "high      0\n",
      "low       0\n",
      "close     0\n",
      "volume    0\n",
      "dtype: int64\n",
      "\n",
      "Index range: 2017-05-12 18:00:00 to 2019-02-01 20:00:00\n",
      "Total rows: 7155\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ts_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-05-12 18:00:00</th>\n",
       "      <td>2017.05.12</td>\n",
       "      <td>18:00</td>\n",
       "      <td>960.01</td>\n",
       "      <td>961.24</td>\n",
       "      <td>960.00</td>\n",
       "      <td>960.66</td>\n",
       "      <td>978</td>\n",
       "      <td>2017.05.12 18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-12 18:30:00</th>\n",
       "      <td>2017.05.12</td>\n",
       "      <td>18:30</td>\n",
       "      <td>960.65</td>\n",
       "      <td>962.14</td>\n",
       "      <td>960.47</td>\n",
       "      <td>961.89</td>\n",
       "      <td>1017</td>\n",
       "      <td>2017.05.12 18:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-12 19:00:00</th>\n",
       "      <td>2017.05.12</td>\n",
       "      <td>19:00</td>\n",
       "      <td>961.91</td>\n",
       "      <td>962.76</td>\n",
       "      <td>960.63</td>\n",
       "      <td>961.74</td>\n",
       "      <td>1384</td>\n",
       "      <td>2017.05.12 19:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-12 19:30:00</th>\n",
       "      <td>2017.05.12</td>\n",
       "      <td>19:30</td>\n",
       "      <td>961.77</td>\n",
       "      <td>962.27</td>\n",
       "      <td>961.13</td>\n",
       "      <td>961.34</td>\n",
       "      <td>2110</td>\n",
       "      <td>2017.05.12 19:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 12:30:00</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>12:30</td>\n",
       "      <td>962.39</td>\n",
       "      <td>962.50</td>\n",
       "      <td>959.97</td>\n",
       "      <td>960.20</td>\n",
       "      <td>121</td>\n",
       "      <td>2017.05.15 12:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 13:00:00</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>13:00</td>\n",
       "      <td>960.10</td>\n",
       "      <td>960.70</td>\n",
       "      <td>958.09</td>\n",
       "      <td>958.68</td>\n",
       "      <td>372</td>\n",
       "      <td>2017.05.15 13:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 13:30:00</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>13:30</td>\n",
       "      <td>958.66</td>\n",
       "      <td>963.00</td>\n",
       "      <td>957.00</td>\n",
       "      <td>959.97</td>\n",
       "      <td>2512</td>\n",
       "      <td>2017.05.15 13:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 14:00:00</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>14:00</td>\n",
       "      <td>959.98</td>\n",
       "      <td>960.64</td>\n",
       "      <td>956.05</td>\n",
       "      <td>957.54</td>\n",
       "      <td>1887</td>\n",
       "      <td>2017.05.15 14:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 14:30:00</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>14:30</td>\n",
       "      <td>957.56</td>\n",
       "      <td>961.01</td>\n",
       "      <td>957.09</td>\n",
       "      <td>960.16</td>\n",
       "      <td>1727</td>\n",
       "      <td>2017.05.15 14:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 15:00:00</th>\n",
       "      <td>2017.05.15</td>\n",
       "      <td>15:00</td>\n",
       "      <td>960.15</td>\n",
       "      <td>961.05</td>\n",
       "      <td>958.47</td>\n",
       "      <td>959.44</td>\n",
       "      <td>1424</td>\n",
       "      <td>2017.05.15 15:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date   time    open    high     low   close  \\\n",
       "ts                                                                       \n",
       "2017-05-12 18:00:00  2017.05.12  18:00  960.01  961.24  960.00  960.66   \n",
       "2017-05-12 18:30:00  2017.05.12  18:30  960.65  962.14  960.47  961.89   \n",
       "2017-05-12 19:00:00  2017.05.12  19:00  961.91  962.76  960.63  961.74   \n",
       "2017-05-12 19:30:00  2017.05.12  19:30  961.77  962.27  961.13  961.34   \n",
       "2017-05-15 12:30:00  2017.05.15  12:30  962.39  962.50  959.97  960.20   \n",
       "2017-05-15 13:00:00  2017.05.15  13:00  960.10  960.70  958.09  958.68   \n",
       "2017-05-15 13:30:00  2017.05.15  13:30  958.66  963.00  957.00  959.97   \n",
       "2017-05-15 14:00:00  2017.05.15  14:00  959.98  960.64  956.05  957.54   \n",
       "2017-05-15 14:30:00  2017.05.15  14:30  957.56  961.01  957.09  960.16   \n",
       "2017-05-15 15:00:00  2017.05.15  15:00  960.15  961.05  958.47  959.44   \n",
       "\n",
       "                     volume            ts_str  \n",
       "ts                                             \n",
       "2017-05-12 18:00:00     978  2017.05.12 18:00  \n",
       "2017-05-12 18:30:00    1017  2017.05.12 18:30  \n",
       "2017-05-12 19:00:00    1384  2017.05.12 19:00  \n",
       "2017-05-12 19:30:00    2110  2017.05.12 19:30  \n",
       "2017-05-15 12:30:00     121  2017.05.15 12:30  \n",
       "2017-05-15 13:00:00     372  2017.05.15 13:00  \n",
       "2017-05-15 13:30:00    2512  2017.05.15 13:30  \n",
       "2017-05-15 14:00:00    1887  2017.05.15 14:00  \n",
       "2017-05-15 14:30:00    1727  2017.05.15 14:30  \n",
       "2017-05-15 15:00:00    1424  2017.05.15 15:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>open</th>\n",
       "      <td>7155.0</td>\n",
       "      <td>1415.360063</td>\n",
       "      <td>335.949233</td>\n",
       "      <td>932.45</td>\n",
       "      <td>1015.645</td>\n",
       "      <td>1486.00</td>\n",
       "      <td>1682.965</td>\n",
       "      <td>2047.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>7155.0</td>\n",
       "      <td>1418.858921</td>\n",
       "      <td>337.081782</td>\n",
       "      <td>934.99</td>\n",
       "      <td>1018.335</td>\n",
       "      <td>1490.99</td>\n",
       "      <td>1686.360</td>\n",
       "      <td>2050.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>7155.0</td>\n",
       "      <td>1411.589758</td>\n",
       "      <td>334.653916</td>\n",
       "      <td>927.89</td>\n",
       "      <td>1012.180</td>\n",
       "      <td>1481.00</td>\n",
       "      <td>1677.970</td>\n",
       "      <td>2041.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>close</th>\n",
       "      <td>7155.0</td>\n",
       "      <td>1415.261593</td>\n",
       "      <td>335.846736</td>\n",
       "      <td>932.47</td>\n",
       "      <td>1016.265</td>\n",
       "      <td>1486.15</td>\n",
       "      <td>1682.960</td>\n",
       "      <td>2047.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volume</th>\n",
       "      <td>7155.0</td>\n",
       "      <td>1683.958910</td>\n",
       "      <td>1151.533643</td>\n",
       "      <td>1.00</td>\n",
       "      <td>856.500</td>\n",
       "      <td>1578.00</td>\n",
       "      <td>2464.500</td>\n",
       "      <td>5639.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count         mean          std     min       25%      50%       75%  \\\n",
       "open    7155.0  1415.360063   335.949233  932.45  1015.645  1486.00  1682.965   \n",
       "high    7155.0  1418.858921   337.081782  934.99  1018.335  1490.99  1686.360   \n",
       "low     7155.0  1411.589758   334.653916  927.89  1012.180  1481.00  1677.970   \n",
       "close   7155.0  1415.261593   335.846736  932.47  1016.265  1486.15  1682.960   \n",
       "volume  7155.0  1683.958910  1151.533643    1.00   856.500  1578.00  2464.500   \n",
       "\n",
       "            max  \n",
       "open    2047.08  \n",
       "high    2050.17  \n",
       "low     2041.75  \n",
       "close   2047.08  \n",
       "volume  5639.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 5 — Parse datetime, convert dtypes, set index and show basic stats\n",
    "# Replace previous CELL 5 with this cell.\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Clean strings and combine\n",
    "df['date'] = df['date'].astype(str).str.strip()\n",
    "df['time'] = df['time'].astype(str).str.strip()\n",
    "df['ts_str'] = df['date'] + ' ' + df['time']\n",
    "\n",
    "# Parse timestamps using the sample format: \"2017.05.12 18:00\"\n",
    "df['ts'] = pd.to_datetime(df['ts_str'], format=\"%Y.%m.%d %H:%M\", errors='coerce')\n",
    "n_bad = df['ts'].isna().sum()\n",
    "print(\"Parsed timestamps. Unparsable count:\", n_bad)\n",
    "if n_bad > 0:\n",
    "    display(df[df['ts'].isna()].head())\n",
    "\n",
    "# Convert numeric columns (coerce errors to NaN so we can inspect)\n",
    "for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Quick checks\n",
    "print(\"\\nMissing values per column after conversion:\")\n",
    "print(df[['open','high','low','close','volume']].isna().sum())\n",
    "\n",
    "# Set index and sort\n",
    "df = df.set_index('ts').sort_index()\n",
    "print(\"\\nIndex range:\", df.index.min(), \"to\", df.index.max())\n",
    "print(\"Total rows:\", len(df))\n",
    "display(df.head(10))\n",
    "display(df[['open','high','low','close','volume']].describe().T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0f96150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using df_label as source (has target).\n",
      "Total candidate features: 24\n",
      "Example feature names (first 20): ['open', 'high', 'low', 'close', 'volume', 'ts', 'return_1', 'log_return_1', 'roll_mean_3', 'roll_std_3', 'roll_max_3', 'roll_min_3', 'roll_mean_6', 'roll_std_6', 'roll_max_6', 'roll_min_6', 'roll_mean_12', 'roll_std_12', 'roll_max_12', 'roll_min_12']\n",
      "WARNING: Non-numeric features detected and will be removed: ['ts']\n",
      "\n",
      "Train shape: (5721, 23)  Test shape: (1431, 23)\n",
      "Train time range: 2017-05-12 19:00:00 → 2018-09-26 17:00:00\n",
      "Test  time range: 2018-09-26 17:30:00 → 2019-02-01 19:30:00\n",
      "\n",
      "Label distribution (train counts):\n",
      "target_thresholded\n",
      "0    3407\n",
      "1    2314\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label distribution (test counts):\n",
      "target_thresholded\n",
      "0    829\n",
      "1    602\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === RUN THIS: build modeling dataset and chronological train/test split ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Prefer df_label (has target); otherwise use df_feat and build minimal label\n",
    "if 'df_label' in globals():\n",
    "    model_src = df_label.copy()\n",
    "    print(\"Using df_label as source (has target).\")\n",
    "elif 'df_feat' in globals():\n",
    "    print(\"df_label not found; building model_src from df_feat.\")\n",
    "    tmp = df_feat.copy()\n",
    "    HORIZON = globals().get('HORIZON', 1)\n",
    "    THRESHOLD = globals().get('THRESHOLD', 0.0005)\n",
    "    tmp['close_future_h'] = tmp['close'].shift(-HORIZON)\n",
    "    tmp['future_return_h'] = (tmp['close_future_h'] / tmp['close']) - 1\n",
    "    tmp = tmp.dropna(subset=['future_return_h'])\n",
    "    tmp['target_thresholded'] = (tmp['future_return_h'] > THRESHOLD).astype(int)\n",
    "    model_src = tmp\n",
    "else:\n",
    "    raise RuntimeError(\"Neither 'df_label' nor 'df_feat' found. Run parsing and feature cells first.\")\n",
    "\n",
    "model_df = model_src.copy()\n",
    "\n",
    "# Columns to exclude from features (identifiers and any future/leaky columns)\n",
    "exclude_cols = ['date','time','ts_str','close_future_h','future_return_h','close_next','future_return_1']\n",
    "\n",
    "# Build feature list (numeric features only)\n",
    "features = [c for c in model_df.columns if c not in exclude_cols + ['target_thresholded']]\n",
    "print(\"Total candidate features:\", len(features))\n",
    "print(\"Example feature names (first 20):\", features[:20])\n",
    "\n",
    "# Ensure selected features are numeric\n",
    "non_numeric = model_df[features].select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "if non_numeric:\n",
    "    print(\"WARNING: Non-numeric features detected and will be removed:\", non_numeric)\n",
    "    features = [c for c in features if c not in non_numeric]\n",
    "\n",
    "# Prepare X and y\n",
    "X = model_df[features].copy()\n",
    "y = model_df['target_thresholded'].copy()\n",
    "\n",
    "# Chronological 80/20 split (no shuffling)\n",
    "split_idx = int(len(model_df) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx].copy(), X.iloc[split_idx:].copy()\n",
    "y_train, y_test = y.iloc[:split_idx].copy(), y.iloc[split_idx:].copy()\n",
    "\n",
    "print(\"\\nTrain shape:\", X_train.shape, \" Test shape:\", X_test.shape)\n",
    "print(\"Train time range:\", X_train.index.min(), \"→\", X_train.index.max())\n",
    "print(\"Test  time range:\", X_test.index.min(),  \"→\", X_test.index.max())\n",
    "\n",
    "print(\"\\nLabel distribution (train counts):\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nLabel distribution (test counts):\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Export to globals for the training cell\n",
    "globals().update({\n",
    "    'model_df': model_df,\n",
    "    'X': X, 'y': y,\n",
    "    'X_train': X_train, 'X_test': X_test,\n",
    "    'y_train': y_train, 'y_test': y_test\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ec4bd50-3629-4c44-b324-db7ff0ae4299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 3 models on 5721 training samples ...\n",
      "Models:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "--- Training LogisticRegression ---\n",
      "Model: LogisticRegression\n",
      "------------------\n",
      "Accuracy:  0.581\n",
      "Precision: 0.506\n",
      "Recall:    0.151\n",
      "F1-score:  0.233\n",
      "ROC-AUC:   0.527\n",
      "Confusion matrix:\n",
      "[[740  89]\n",
      " [511  91]]\n",
      "\n",
      "Top absolute LR coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "high              0.428117\n",
       "roll_max_12       0.415224\n",
       "roll_mean_12      0.290595\n",
       "open              0.281830\n",
       "roll_min_12       0.243565\n",
       "low               0.162222\n",
       "roll_min_6        0.124023\n",
       "roll_std_12       0.119404\n",
       "close             0.109290\n",
       "vol_ratio_3_12    0.084081\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training SVM_RBF ---\n",
      "Model: SVM_RBF\n",
      "-------\n",
      "Accuracy:  0.568\n",
      "Precision: 0.367\n",
      "Recall:    0.037\n",
      "F1-score:  0.066\n",
      "ROC-AUC:   0.498\n",
      "Confusion matrix:\n",
      "[[791  38]\n",
      " [580  22]]\n",
      "Models:  67%|██████▋   | 2/3 [00:07<00:03,  3.76s/it]\n",
      "--- Training XGBoost ---\n",
      "Model: XGBoost\n",
      "-------\n",
      "Accuracy:  0.521\n",
      "Precision: 0.398\n",
      "Recall:    0.269\n",
      "F1-score:  0.321\n",
      "ROC-AUC:   0.490\n",
      "Confusion matrix:\n",
      "[[584 245]\n",
      " [440 162]]\n",
      "\n",
      "Top feature importances (by XGBoost):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amanb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py:199: UserWarning: [17:57:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "roll_max_3     0.060421\n",
       "roll_min_6     0.054324\n",
       "vol_change     0.048849\n",
       "roll_min_12    0.048371\n",
       "roll_max_6     0.047675\n",
       "high           0.046231\n",
       "volume         0.046171\n",
       "roll_std_3     0.045901\n",
       "return_1       0.044619\n",
       "roll_std_6     0.044503\n",
       "dtype: float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: 100%|██████████| 3/3 [00:07<00:00,  2.64s/it]\n",
      "\n",
      "All baseline models trained. Summary:\n",
      "LogisticRegression:  F1=0.233  Acc=0.581  AUC=0.527\n",
      "SVM_RBF:  F1=0.066  Acc=0.568  AUC=0.498\n",
      "XGBoost:  F1=0.321  Acc=0.521  AUC=0.490\n"
     ]
    }
   ],
   "source": [
    "# === CELL 6: Train baseline models (LogisticRegression, SVM, XGBoost) with a progress bar ===\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# --- Safety check: required variables\n",
    "for name in ['X_train','X_test','y_train','y_test']:\n",
    "    if name not in globals():\n",
    "        raise RuntimeError(f\"Required variable '{name}' not found in globals. Run the train/test split cell first.\")\n",
    "\n",
    "# --- Model pipelines to train\n",
    "models = [\n",
    "    (\"LogisticRegression\", Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=2000, random_state=42))])),\n",
    "    (\"SVM_RBF\",            Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(probability=True, kernel='rbf', random_state=42))])),\n",
    "    (\"XGBoost\",            Pipeline([(\"scaler\", StandardScaler()), (\"clf\", XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))]))\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(f\"Training {len(models)} models on {len(X_train)} training samples ...\")\n",
    "\n",
    "# train each model and report metrics\n",
    "for name, pipe in tqdm(models, desc=\"Models\", file=sys.stdout):\n",
    "    print(f\"\\n--- Training {name} ---\")\n",
    "    # Fit\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    y_proba = None\n",
    "    try:\n",
    "        y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    except Exception:\n",
    "        y_proba = None\n",
    "\n",
    "    # Metrics\n",
    "    acc  = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec  = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1   = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc  = roc_auc_score(y_test, y_proba) if (y_proba is not None) else None\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Model: {name}\")\n",
    "    print(\"-\" * max(len(name), 6))\n",
    "    print(f\"Accuracy:  {acc:.3f}\")\n",
    "    print(f\"Precision: {prec:.3f}\")\n",
    "    print(f\"Recall:    {rec:.3f}\")\n",
    "    print(f\"F1-score:  {f1:.3f}\")\n",
    "    if auc is not None:\n",
    "        print(f\"ROC-AUC:   {auc:.3f}\")\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # Save results\n",
    "    results[name] = {'model': pipe, 'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'auc': auc}\n",
    "\n",
    "    # Show top coefficients / feature importances where possible\n",
    "    try:\n",
    "        clf = pipe.named_steps['clf']\n",
    "        feat_names = X_train.columns if hasattr(X_train, \"columns\") else None\n",
    "        if hasattr(clf, 'coef_') and feat_names is not None:\n",
    "            coefs = clf.coef_.ravel()\n",
    "            top_coef = pd.Series(coefs, index=feat_names).abs().sort_values(ascending=False).head(10)\n",
    "            print(\"\\nTop absolute LR coefficients:\")\n",
    "            display(top_coef)\n",
    "        elif hasattr(clf, 'feature_importances_') and feat_names is not None:\n",
    "            fi = clf.feature_importances_\n",
    "            top_fi = pd.Series(fi, index=feat_names).sort_values(ascending=False).head(10)\n",
    "            print(\"\\nTop feature importances (by XGBoost):\")\n",
    "            display(top_fi)\n",
    "    except Exception as e:\n",
    "        print(\"Could not extract model coefficients/importances:\", repr(e))\n",
    "\n",
    "# Summary print (safe formatting)\n",
    "print(\"\\nAll baseline models trained. Summary:\")\n",
    "for name, stats in results.items():\n",
    "    auc_display = \"None\" if stats[\"auc\"] is None else f\"{stats['auc']:.3f}\"\n",
    "    print(f\"{name}:  F1={stats['f1']:.3f}  Acc={stats['acc']:.3f}  AUC={auc_display}\")\n",
    "\n",
    "# Save results for next steps\n",
    "globals()['baseline_results'] = results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74f2a131-072d-494d-b432-c0a9f9c23723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running RandomizedSearchCV for Logistic Regression (n_iter=12) ...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "-> Logistic Regression best CV F1: 0.1626\n",
      "-> Best params: {'clf__penalty': 'l2', 'clf__C': np.float64(231.0129700083158)}\n",
      "\n",
      "Running RandomizedSearchCV for SVM (RBF) (n_iter=12) ...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "-> SVM (RBF) best CV F1: 0.2999\n",
      "-> Best params: {'clf__kernel': 'rbf', 'clf__gamma': 'scale', 'clf__C': 5}\n",
      "\n",
      "Running RandomizedSearchCV for XGBoost (n_iter=12) ...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "-> XGBoost best CV F1: 0.3432\n",
      "-> Best params: {'clf__subsample': 1.0, 'clf__n_estimators': 400, 'clf__max_depth': 8, 'clf__learning_rate': 0.05, 'clf__colsample_bytree': 1.0}\n",
      "\n",
      "Tuning finished. Best estimators stored as: best_lr, best_svm, best_xgb\n"
     ]
    }
   ],
   "source": [
    "# === CELL 7: TimeSeries-aware RandomizedSearchCV tuning for LR, SVM, XGBoost ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Safety checks\n",
    "for name in ['X_train','y_train']:\n",
    "    if name not in globals():\n",
    "        raise RuntimeError(f\"Required variable '{name}' not found in globals. Run the train/test split cell first.\")\n",
    "\n",
    "# CV setup\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# Pipelines\n",
    "pipe_lr  = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=2000, random_state=42))])\n",
    "pipe_svm = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(probability=True, random_state=42))])\n",
    "pipe_xgb = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))])\n",
    "\n",
    "# Parameter distributions (use modest sizes for speed; increase n_iter if you want more thorough search)\n",
    "param_lr = {\n",
    "    \"clf__C\": np.logspace(-4, 3, 12),\n",
    "    \"clf__penalty\": [\"l2\"],\n",
    "    # consider 'class_weight': ['balanced'] as an alternative experiment\n",
    "}\n",
    "\n",
    "param_svm = {\n",
    "    \"clf__C\": [0.01, 0.1, 1, 5, 10],\n",
    "    \"clf__gamma\": [\"scale\", 0.01, 0.001],\n",
    "    \"clf__kernel\": [\"rbf\"]\n",
    "}\n",
    "\n",
    "param_xgb = {\n",
    "    \"clf__n_estimators\": [100, 200, 400],\n",
    "    \"clf__max_depth\": [3, 5, 8],\n",
    "    \"clf__learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"clf__subsample\": [0.7, 0.9, 1.0],\n",
    "    \"clf__colsample_bytree\": [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Helper to run RandomizedSearchCV\n",
    "def run_rand_search(name, pipeline, param_dist, n_iter=12):\n",
    "    print(f\"\\nRunning RandomizedSearchCV for {name} (n_iter={n_iter}) ...\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        scoring=f1_scorer,\n",
    "        cv=tscv,\n",
    "        random_state=42,\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        refit=True\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    print(f\"-> {name} best CV F1: {search.best_score_:.4f}\")\n",
    "    print(\"-> Best params:\", search.best_params_)\n",
    "    return search\n",
    "\n",
    "# Run searches (adjust n_iter to taste; start with 12 then increase for final runs)\n",
    "search_lr  = run_rand_search(\"Logistic Regression\", pipe_lr, param_lr, n_iter=12)\n",
    "search_svm = run_rand_search(\"SVM (RBF)\", pipe_svm, param_svm, n_iter=12)\n",
    "search_xgb = run_rand_search(\"XGBoost\", pipe_xgb, param_xgb, n_iter=12)\n",
    "\n",
    "# Save to globals\n",
    "globals().update({\n",
    "    'search_lr': search_lr, 'search_svm': search_svm, 'search_xgb': search_xgb,\n",
    "    'best_lr': search_lr.best_estimator_, 'best_svm': search_svm.best_estimator_, 'best_xgb': search_xgb.best_estimator_\n",
    "})\n",
    "print(\"\\nTuning finished. Best estimators stored as: best_lr, best_svm, best_xgb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32b944d9-a178-4a39-a452-e535a13f3313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuned LogisticRegression\n",
      "------------------------\n",
      "Accuracy:  0.584\n",
      "Precision: 0.517\n",
      "Recall:    0.176\n",
      "F1-score:  0.263\n",
      "ROC-AUC:   0.533\n",
      "Confusion matrix:\n",
      "[[730  99]\n",
      " [496 106]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "high               0.805324\n",
       "roll_max_12        0.785361\n",
       "roll_mean_12       0.547652\n",
       "open               0.528647\n",
       "roll_min_12        0.456421\n",
       "low                0.302476\n",
       "roll_min_6         0.236303\n",
       "close              0.202550\n",
       "return_1           0.129628\n",
       "roll_min_3         0.126420\n",
       "roll_mean_3        0.123471\n",
       "roll_max_6         0.121680\n",
       "roll_std_12        0.121226\n",
       "roll_max_3         0.119111\n",
       "log_return_1       0.089563\n",
       "vol_ratio_3_12     0.084365\n",
       "vol_roll_mean_6    0.077049\n",
       "mom_3              0.060250\n",
       "vol_change         0.043246\n",
       "volume             0.028066\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuned SVM (RBF)\n",
      "---------------\n",
      "Accuracy:  0.544\n",
      "Precision: 0.367\n",
      "Recall:    0.115\n",
      "F1-score:  0.175\n",
      "ROC-AUC:   0.485\n",
      "Confusion matrix:\n",
      "[[710 119]\n",
      " [533  69]]\n",
      "\n",
      "Tuned XGBoost\n",
      "-------------\n",
      "Accuracy:  0.527\n",
      "Precision: 0.409\n",
      "Recall:    0.279\n",
      "F1-score:  0.332\n",
      "ROC-AUC:   0.497\n",
      "Confusion matrix:\n",
      "[[586 243]\n",
      " [434 168]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "roll_max_3         0.050773\n",
       "roll_min_12        0.049184\n",
       "roll_mean_12       0.048989\n",
       "roll_mean_6        0.048494\n",
       "high               0.048387\n",
       "roll_min_6         0.047054\n",
       "low                0.046676\n",
       "vol_change         0.046378\n",
       "roll_mean_3        0.046340\n",
       "roll_max_12        0.045800\n",
       "roll_max_6         0.045758\n",
       "volume             0.045121\n",
       "vol_ratio_3_12     0.044807\n",
       "roll_min_3         0.044519\n",
       "roll_std_3         0.044425\n",
       "roll_std_6         0.043819\n",
       "vol_roll_mean_6    0.043704\n",
       "roll_std_12        0.043513\n",
       "return_1           0.043318\n",
       "mom_3              0.042934\n",
       "dtype: float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === CELL 8: Evaluate tuned models on test set ===\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Required globals check\n",
    "for v in ['best_lr','best_svm','best_xgb','X_test','y_test']:\n",
    "    if v not in globals():\n",
    "        raise RuntimeError(f\"Required variable '{v}' missing. Run CELL 7 first.\")\n",
    "\n",
    "models = [\n",
    "    (\"Tuned LogisticRegression\", globals()['best_lr']),\n",
    "    (\"Tuned SVM (RBF)\", globals()['best_svm']),\n",
    "    (\"Tuned XGBoost\", globals()['best_xgb'])\n",
    "]\n",
    "\n",
    "def evaluate_and_report(name, model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X_test)[:,1]\n",
    "    except Exception:\n",
    "        y_proba = None\n",
    "    acc  = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec  = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1   = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc  = roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
    "\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"-\"*len(name))\n",
    "    print(f\"Accuracy:  {acc:.3f}\")\n",
    "    print(f\"Precision: {prec:.3f}\")\n",
    "    print(f\"Recall:    {rec:.3f}\")\n",
    "    print(f\"F1-score:  {f1:.3f}\")\n",
    "    if auc is not None: print(f\"ROC-AUC:   {auc:.3f}\")\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # show top features if available\n",
    "    try:\n",
    "        clf = model.named_steps['clf']\n",
    "        feat_names = X_train.columns\n",
    "        if hasattr(clf, 'coef_'):\n",
    "            coefs = clf.coef_.ravel()\n",
    "            display(pd.Series(coefs, index=feat_names).abs().sort_values(ascending=False).head(20))\n",
    "        elif hasattr(clf, 'feature_importances_'):\n",
    "            display(pd.Series(clf.feature_importances_, index=feat_names).sort_values(ascending=False).head(20))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "for name, m in models:\n",
    "    evaluate_and_report(name, m, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c0d756c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 5721  positives: 2314  negatives: 3407  scale_pos_weight: 1.472\n",
      "\n",
      "Training with class-imbalance adjustments...\n",
      "\n",
      "Models:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "--- Training LR_balanced ---\n",
      "Model: LR_balanced\n",
      "-----------\n",
      "Accuracy:  0.481\n",
      "Precision: 0.429\n",
      "Recall:    0.704\n",
      "F1-score:  0.533\n",
      "ROC-AUC:   0.527\n",
      "Confusion matrix:\n",
      "[[265 564]\n",
      " [178 424]]\n",
      "\n",
      "--- Training SVM_balanced ---\n",
      "Model: SVM_balanced\n",
      "------------\n",
      "Accuracy:  0.501\n",
      "Precision: 0.412\n",
      "Recall:    0.437\n",
      "F1-score:  0.424\n",
      "ROC-AUC:   0.500\n",
      "Confusion matrix:\n",
      "[[454 375]\n",
      " [339 263]]\n",
      "Models:  67%|██████▋   | 2/3 [00:08<00:04,  4.17s/it]\n",
      "--- Training XGB_spw ---\n",
      "Model: XGB_spw\n",
      "-------\n",
      "Accuracy:  0.493\n",
      "Precision: 0.390\n",
      "Recall:    0.364\n",
      "F1-score:  0.377\n",
      "ROC-AUC:   0.480\n",
      "Confusion matrix:\n",
      "[[487 342]\n",
      " [383 219]]\n",
      "Models: 100%|██████████| 3/3 [00:08<00:00,  2.83s/it]\n",
      "\n",
      "Summary (imbalance-handling):\n",
      "LR_balanced: F1=0.533 Acc=0.481 AUC=0.527  Prec=0.429 Rec=0.704\n",
      "SVM_balanced: F1=0.424 Acc=0.501 AUC=0.500  Prec=0.412 Rec=0.437\n",
      "XGB_spw: F1=0.377 Acc=0.493 AUC=0.480  Prec=0.390 Rec=0.364\n"
     ]
    }
   ],
   "source": [
    "# === CELL 9 — Class imbalance experiments ===\n",
    "# Run this cell after you have X_train, X_test, y_train, y_test in globals.\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Safety check\n",
    "for name in ['X_train','X_test','y_train','y_test']:\n",
    "    if name not in globals():\n",
    "        raise RuntimeError(f\"Required variable '{name}' not found - run the modeling split cell first.\")\n",
    "\n",
    "# Compute scale_pos_weight for XGBoost from training set\n",
    "n_pos = int(y_train.sum())\n",
    "n_neg = int(len(y_train) - n_pos)\n",
    "scale_pos_weight = (n_neg / n_pos) if n_pos > 0 else 1.0\n",
    "print(f\"Train samples: {len(y_train)}  positives: {n_pos}  negatives: {n_neg}  scale_pos_weight: {scale_pos_weight:.3f}\\n\")\n",
    "\n",
    "# Models with imbalance handling\n",
    "models = [\n",
    "    (\"LR_balanced\", Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=2000, class_weight='balanced', random_state=42))])),\n",
    "    (\"SVM_balanced\", Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(probability=True, class_weight='balanced', kernel='rbf', random_state=42))])),\n",
    "    (\"XGB_spw\", Pipeline([(\"scaler\", StandardScaler()), (\"clf\", XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42,\n",
    "                                                                               scale_pos_weight=scale_pos_weight))]))\n",
    "]\n",
    "\n",
    "results_imb = {}\n",
    "\n",
    "print(\"Training with class-imbalance adjustments...\\n\")\n",
    "\n",
    "for name, pipe in tqdm(models, desc=\"Models\", file=sys.stdout):\n",
    "    print(f\"\\n--- Training {name} ---\")\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    y_proba = None\n",
    "    try:\n",
    "        y_proba = pipe.predict_proba(X_test)[:,1]\n",
    "    except Exception:\n",
    "        y_proba = None\n",
    "\n",
    "    acc  = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec  = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1   = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc  = roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
    "\n",
    "    print(f\"Model: {name}\")\n",
    "    print(\"-\"*max(len(name),6))\n",
    "    print(f\"Accuracy:  {acc:.3f}\")\n",
    "    print(f\"Precision: {prec:.3f}\")\n",
    "    print(f\"Recall:    {rec:.3f}\")\n",
    "    print(f\"F1-score:  {f1:.3f}\")\n",
    "    if auc is not None:\n",
    "        print(f\"ROC-AUC:   {auc:.3f}\")\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # store\n",
    "    results_imb[name] = {'model': pipe, 'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'auc': auc}\n",
    "\n",
    "print(\"\\nSummary (imbalance-handling):\")\n",
    "for n, s in results_imb.items():\n",
    "    auc_display = \"None\" if s['auc'] is None else f\"{s['auc']:.3f}\"\n",
    "    print(f\"{n}: F1={s['f1']:.3f} Acc={s['acc']:.3f} AUC={auc_display}  Prec={s['prec']:.3f} Rec={s['rec']:.3f}\")\n",
    "\n",
    "# Save for later comparisons\n",
    "globals()['results_imbalance'] = results_imb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a6b6076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PurgedTimeSeriesSplit(n_splits=5, embargo=1) on X_train/y_train\n",
      "\n",
      "Fold 0: train_size=952, test_size=953, pos_train=384, pos_test=348\n",
      "  -> Fold 0 F1: 0.4730\n",
      "\n",
      "Fold 1: train_size=1905, test_size=953, pos_train=732, pos_test=377\n",
      "  -> Fold 1 F1: 0.0732\n",
      "\n",
      "Fold 2: train_size=2858, test_size=953, pos_train=1108, pos_test=402\n",
      "  -> Fold 2 F1: 0.5637\n",
      "\n",
      "Fold 3: train_size=3811, test_size=953, pos_train=1510, pos_test=390\n",
      "  -> Fold 3 F1: 0.5134\n",
      "\n",
      "Fold 4: train_size=4764, test_size=953, pos_train=1901, pos_test=413\n",
      "  -> Fold 4 F1: 0.5176\n",
      "\n",
      "Fold F1s: [0.473  0.0732 0.5637 0.5134 0.5176]\n",
      "Mean F1 (valid folds): 0.4282  Std: 0.1798\n"
     ]
    }
   ],
   "source": [
    "# === CELL 10: Corrected Purged / Embargoed Time-Series CV with diagnostics ===\n",
    "import numpy as np\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "class PurgedTimeSeriesSplit(BaseCrossValidator):\n",
    "    \"\"\"\n",
    "    Purged time-series split where the training set grows each fold and test folds are contiguous.\n",
    "    Optionally apply an embargo (number of samples) to remove immediately preceding indices from training.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits=5, embargo=0):\n",
    "        assert n_splits >= 2, \"n_splits must be >=2\"\n",
    "        self.n_splits = n_splits\n",
    "        self.embargo = int(embargo)\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        # choose test_size so that we have n_splits test blocks\n",
    "        test_size = n_samples // (self.n_splits + 1)\n",
    "        if test_size < 1:\n",
    "            raise ValueError(\"Not enough samples for the requested number of splits.\")\n",
    "        indices = np.arange(n_samples)\n",
    "        # For i-th split: train_end = (i+1)*test_size, test_start = train_end, test_stop = test_start+test_size\n",
    "        for i in range(self.n_splits):\n",
    "            train_end = (i + 1) * test_size\n",
    "            test_start = train_end\n",
    "            test_stop = test_start + test_size if (test_start + test_size) <= n_samples else n_samples\n",
    "            # apply embargo: remove last `embargo` samples ending at train_end from training\n",
    "            embargo_start = max(0, train_end - self.embargo)\n",
    "            train_indices = indices[:embargo_start]\n",
    "            test_indices = indices[test_start:test_stop]\n",
    "            yield train_indices, test_indices\n",
    "\n",
    "# Diagnostics run\n",
    "purged_cv = PurgedTimeSeriesSplit(n_splits=5, embargo=1)  # adjust embargo if you want\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "pipe_lr_bal = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "print(\"Using PurgedTimeSeriesSplit(n_splits=5, embargo=1) on X_train/y_train\")\n",
    "\n",
    "fold_f1s = []\n",
    "fold_info = []\n",
    "for fold, (train_idx, test_idx) in enumerate(purged_cv.split(X_train, y_train)):\n",
    "    y_tr = y_train.iloc[train_idx]\n",
    "    y_te = y_train.iloc[test_idx]\n",
    "    # basic checks\n",
    "    n_tr, n_te = len(train_idx), len(test_idx)\n",
    "    pos_tr, pos_te = int(y_tr.sum()) if n_tr>0 else 0, int(y_te.sum()) if n_te>0 else 0\n",
    "    fold_info.append((fold, n_tr, n_te, pos_tr, pos_te))\n",
    "    print(f\"\\nFold {fold}: train_size={n_tr}, test_size={n_te}, pos_train={pos_tr}, pos_test={pos_te}\")\n",
    "\n",
    "    # If train is empty, skip (shouldn't happen with this splitter)\n",
    "    if n_tr == 0:\n",
    "        print(\"  -> Skipping fold because training set is empty.\")\n",
    "        fold_f1s.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    # Fit & evaluate on this fold (using pipeline) with safe zero_division\n",
    "    pipe_lr_bal.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n",
    "    ypred = pipe_lr_bal.predict(X_train.iloc[test_idx])\n",
    "    f1 = f1_score(y_train.iloc[test_idx], ypred, zero_division=0)\n",
    "    fold_f1s.append(f1)\n",
    "    print(f\"  -> Fold {fold} F1: {f1:.4f}\")\n",
    "\n",
    "# Summary\n",
    "fold_f1s_arr = np.array(fold_f1s, dtype=float)\n",
    "valid = ~np.isnan(fold_f1s_arr)\n",
    "if valid.sum() == 0:\n",
    "    print(\"\\nAll folds produced NaN F1 — check data and splitter settings.\")\n",
    "else:\n",
    "    print(\"\\nFold F1s:\", np.round(fold_f1s_arr, 4))\n",
    "    print(f\"Mean F1 (valid folds): {np.nanmean(fold_f1s_arr):.4f}  Std: {np.nanstd(fold_f1s_arr):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9358f8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n",
      "train_size=952, test_size=953\n",
      "pos_train: 384 neg_train: 568\n",
      "pos_test: 348 neg_test: 605\n",
      "confusion_matrix:\n",
      " [[288 317]\n",
      " [142 206]]\n",
      "precision=0.394  recall=0.592  f1=0.473\n",
      "\n",
      "Top feature means (train vs test) for top 10 train-means:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "      <th>train_mean</th>\n",
       "      <th>test_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vol_roll_mean_6</td>\n",
       "      <td>1171.320816</td>\n",
       "      <td>1063.697971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>volume</td>\n",
       "      <td>1170.585084</td>\n",
       "      <td>1064.801679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roll_max_12</td>\n",
       "      <td>999.213971</td>\n",
       "      <td>978.250514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roll_max_6</td>\n",
       "      <td>996.922805</td>\n",
       "      <td>976.438290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high</td>\n",
       "      <td>995.505861</td>\n",
       "      <td>975.332025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>roll_max_3</td>\n",
       "      <td>995.365158</td>\n",
       "      <td>975.179119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>open</td>\n",
       "      <td>993.792626</td>\n",
       "      <td>973.907009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>close</td>\n",
       "      <td>993.734979</td>\n",
       "      <td>973.836159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>roll_mean_3</td>\n",
       "      <td>993.684207</td>\n",
       "      <td>973.853204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>roll_mean_6</td>\n",
       "      <td>993.643502</td>\n",
       "      <td>973.871221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feat   train_mean    test_mean\n",
       "0  vol_roll_mean_6  1171.320816  1063.697971\n",
       "1           volume  1170.585084  1064.801679\n",
       "2      roll_max_12   999.213971   978.250514\n",
       "3       roll_max_6   996.922805   976.438290\n",
       "4             high   995.505861   975.332025\n",
       "5       roll_max_3   995.365158   975.179119\n",
       "6             open   993.792626   973.907009\n",
       "7            close   993.734979   973.836159\n",
       "8      roll_mean_3   993.684207   973.853204\n",
       "9      roll_mean_6   993.643502   973.871221"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n",
      "train_size=1905, test_size=953\n",
      "pos_train: 732 neg_train: 1173\n",
      "pos_test: 377 neg_test: 576\n",
      "confusion_matrix:\n",
      " [[558  18]\n",
      " [362  15]]\n",
      "precision=0.455  recall=0.040  f1=0.073\n",
      "\n",
      "Top feature means (train vs test) for top 10 train-means:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "      <th>train_mean</th>\n",
       "      <th>test_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>volume</td>\n",
       "      <td>1117.777953</td>\n",
       "      <td>1473.750262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vol_roll_mean_6</td>\n",
       "      <td>1117.011942</td>\n",
       "      <td>1474.888073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roll_max_12</td>\n",
       "      <td>988.732031</td>\n",
       "      <td>1173.755530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roll_max_6</td>\n",
       "      <td>986.680913</td>\n",
       "      <td>1171.844827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high</td>\n",
       "      <td>985.417696</td>\n",
       "      <td>1171.054008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>roll_max_3</td>\n",
       "      <td>985.272583</td>\n",
       "      <td>1170.387775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>open</td>\n",
       "      <td>983.847459</td>\n",
       "      <td>1168.904281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>close</td>\n",
       "      <td>983.784924</td>\n",
       "      <td>1169.060546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>roll_mean_3</td>\n",
       "      <td>983.768279</td>\n",
       "      <td>1168.390423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>roll_mean_6</td>\n",
       "      <td>983.758603</td>\n",
       "      <td>1167.885853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feat   train_mean    test_mean\n",
       "0           volume  1117.777953  1473.750262\n",
       "1  vol_roll_mean_6  1117.011942  1474.888073\n",
       "2      roll_max_12   988.732031  1173.755530\n",
       "3       roll_max_6   986.680913  1171.844827\n",
       "4             high   985.417696  1171.054008\n",
       "5       roll_max_3   985.272583  1170.387775\n",
       "6             open   983.847459  1168.904281\n",
       "7            close   983.784924  1169.060546\n",
       "8      roll_mean_3   983.768279  1168.390423\n",
       "9      roll_mean_6   983.758603  1167.885853"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2 ===\n",
      "train_size=2858, test_size=953\n",
      "pos_train: 1108 neg_train: 1750\n",
      "pos_test: 402 neg_test: 551\n",
      "confusion_matrix:\n",
      " [[107 444]\n",
      " [ 70 332]]\n",
      "precision=0.428  recall=0.826  f1=0.564\n",
      "\n",
      "Top feature means (train vs test) for top 10 train-means:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "      <th>train_mean</th>\n",
       "      <th>test_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>volume</td>\n",
       "      <td>1236.606368</td>\n",
       "      <td>1956.527807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vol_roll_mean_6</td>\n",
       "      <td>1236.395760</td>\n",
       "      <td>1957.569780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roll_max_12</td>\n",
       "      <td>1050.312446</td>\n",
       "      <td>1479.845782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roll_max_6</td>\n",
       "      <td>1048.312040</td>\n",
       "      <td>1475.122833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high</td>\n",
       "      <td>1047.206679</td>\n",
       "      <td>1471.956537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>roll_max_3</td>\n",
       "      <td>1046.887551</td>\n",
       "      <td>1471.480965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>close</td>\n",
       "      <td>1045.453341</td>\n",
       "      <td>1467.446139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>open</td>\n",
       "      <td>1045.443027</td>\n",
       "      <td>1467.513924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>roll_mean_3</td>\n",
       "      <td>1045.218790</td>\n",
       "      <td>1467.129091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>roll_mean_6</td>\n",
       "      <td>1045.043199</td>\n",
       "      <td>1466.889934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feat   train_mean    test_mean\n",
       "0           volume  1236.606368  1956.527807\n",
       "1  vol_roll_mean_6  1236.395760  1957.569780\n",
       "2      roll_max_12  1050.312446  1479.845782\n",
       "3       roll_max_6  1048.312040  1475.122833\n",
       "4             high  1047.206679  1471.956537\n",
       "5       roll_max_3  1046.887551  1471.480965\n",
       "6            close  1045.453341  1467.446139\n",
       "7             open  1045.443027  1467.513924\n",
       "8      roll_mean_3  1045.218790  1467.129091\n",
       "9      roll_mean_6  1045.043199  1466.889934"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3 ===\n",
      "train_size=3811, test_size=953\n",
      "pos_train: 1510 neg_train: 2301\n",
      "pos_test: 390 neg_test: 563\n",
      "confusion_matrix:\n",
      " [[258 305]\n",
      " [150 240]]\n",
      "precision=0.440  recall=0.615  f1=0.513\n",
      "\n",
      "Top feature means (train vs test) for top 10 train-means:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "      <th>train_mean</th>\n",
       "      <th>test_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vol_roll_mean_6</td>\n",
       "      <td>1416.856752</td>\n",
       "      <td>1434.562434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>volume</td>\n",
       "      <td>1416.850171</td>\n",
       "      <td>1436.405037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roll_max_12</td>\n",
       "      <td>1157.687326</td>\n",
       "      <td>1627.590178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roll_max_6</td>\n",
       "      <td>1155.002781</td>\n",
       "      <td>1624.117492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high</td>\n",
       "      <td>1153.382401</td>\n",
       "      <td>1622.209609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>roll_max_3</td>\n",
       "      <td>1153.023737</td>\n",
       "      <td>1621.617119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>open</td>\n",
       "      <td>1150.948843</td>\n",
       "      <td>1619.007188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>close</td>\n",
       "      <td>1150.939567</td>\n",
       "      <td>1619.037427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>roll_mean_3</td>\n",
       "      <td>1150.684229</td>\n",
       "      <td>1618.509645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>roll_mean_6</td>\n",
       "      <td>1150.492994</td>\n",
       "      <td>1618.112086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feat   train_mean    test_mean\n",
       "0  vol_roll_mean_6  1416.856752  1434.562434\n",
       "1           volume  1416.850171  1436.405037\n",
       "2      roll_max_12  1157.687326  1627.590178\n",
       "3       roll_max_6  1155.002781  1624.117492\n",
       "4             high  1153.382401  1622.209609\n",
       "5       roll_max_3  1153.023737  1621.617119\n",
       "6             open  1150.948843  1619.007188\n",
       "7            close  1150.939567  1619.037427\n",
       "8      roll_mean_3  1150.684229  1618.509645\n",
       "9      roll_mean_6  1150.492994  1618.112086"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 4 ===\n",
      "train_size=4764, test_size=953\n",
      "pos_train: 1901 neg_train: 2863\n",
      "pos_test: 413 neg_test: 540\n",
      "confusion_matrix:\n",
      " [[257 283]\n",
      " [170 243]]\n",
      "precision=0.462  recall=0.588  f1=0.518\n",
      "\n",
      "Top feature means (train vs test) for top 10 train-means:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "      <th>train_mean</th>\n",
       "      <th>test_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>volume</td>\n",
       "      <td>1420.766793</td>\n",
       "      <td>1712.741868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vol_roll_mean_6</td>\n",
       "      <td>1420.298261</td>\n",
       "      <td>1707.812697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roll_max_12</td>\n",
       "      <td>1251.634446</td>\n",
       "      <td>1893.840262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roll_max_6</td>\n",
       "      <td>1248.792240</td>\n",
       "      <td>1889.710986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high</td>\n",
       "      <td>1247.114691</td>\n",
       "      <td>1887.421364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>roll_max_3</td>\n",
       "      <td>1246.709216</td>\n",
       "      <td>1886.739780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>open</td>\n",
       "      <td>1244.527210</td>\n",
       "      <td>1883.723400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>close</td>\n",
       "      <td>1244.526194</td>\n",
       "      <td>1883.747146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>roll_mean_3</td>\n",
       "      <td>1244.215985</td>\n",
       "      <td>1883.145792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>roll_mean_6</td>\n",
       "      <td>1243.983166</td>\n",
       "      <td>1882.715953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feat   train_mean    test_mean\n",
       "0           volume  1420.766793  1712.741868\n",
       "1  vol_roll_mean_6  1420.298261  1707.812697\n",
       "2      roll_max_12  1251.634446  1893.840262\n",
       "3       roll_max_6  1248.792240  1889.710986\n",
       "4             high  1247.114691  1887.421364\n",
       "5       roll_max_3  1246.709216  1886.739780\n",
       "6             open  1244.527210  1883.723400\n",
       "7            close  1244.526194  1883.747146\n",
       "8      roll_mean_3  1244.215985  1883.145792\n",
       "9      roll_mean_6  1243.983166  1882.715953"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL DIAG_1 — per-fold diagnostics (run immediately)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Ensure required objects exist\n",
    "for var in ['X_train','y_train']:\n",
    "    if var not in globals():\n",
    "        raise RuntimeError(f\"Required variable '{var}' not found. Re-run prior cells to produce it.\")\n",
    "\n",
    "# Use the same purged CV you used before (5 splits, embargo=1)\n",
    "class PurgedTimeSeriesSplit:\n",
    "    def __init__(self, n_splits=5, embargo=1):\n",
    "        self.n_splits = n_splits\n",
    "        self.embargo = int(embargo)\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        test_size = n_samples // (self.n_splits + 1)\n",
    "        indices = np.arange(n_samples)\n",
    "        for i in range(self.n_splits):\n",
    "            train_end = (i + 1) * test_size\n",
    "            test_start = train_end\n",
    "            test_stop = test_start + test_size if (test_start + test_size) <= n_samples else n_samples\n",
    "            embargo_start = max(0, train_end - self.embargo)\n",
    "            train_indices = indices[:embargo_start]\n",
    "            test_indices = indices[test_start:test_stop]\n",
    "            yield train_indices, test_indices\n",
    "\n",
    "purged_cv = PurgedTimeSeriesSplit(n_splits=5, embargo=1)\n",
    "\n",
    "# Choose the pipeline to inspect (use the same variant you used in purged CV earlier)\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "# Run per-fold diagnostics\n",
    "fold_reports = []\n",
    "for fold, (train_idx, test_idx) in enumerate(purged_cv.split(X_train, y_train)):\n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "    print(f\"train_size={len(train_idx)}, test_size={len(test_idx)}\")\n",
    "    # class counts\n",
    "    if len(train_idx)>0:\n",
    "        print(\"pos_train:\", int(y_train.iloc[train_idx].sum()), \"neg_train:\", len(train_idx)-int(y_train.iloc[train_idx].sum()))\n",
    "    print(\"pos_test:\", int(y_train.iloc[test_idx].sum()), \"neg_test:\", len(test_idx)-int(y_train.iloc[test_idx].sum()))\n",
    "    if len(train_idx) == 0:\n",
    "        print(\"Skipping: empty train set\")\n",
    "        continue\n",
    "    # fit & eval\n",
    "    pipe.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n",
    "    ypred = pipe.predict(X_train.iloc[test_idx])\n",
    "    cm = confusion_matrix(y_train.iloc[test_idx], ypred)\n",
    "    prec = precision_score(y_train.iloc[test_idx], ypred, zero_division=0)\n",
    "    rec  = recall_score(y_train.iloc[test_idx], ypred, zero_division=0)\n",
    "    f1   = f1_score(y_train.iloc[test_idx], ypred, zero_division=0)\n",
    "    print(\"confusion_matrix:\\n\", cm)\n",
    "    print(f\"precision={prec:.3f}  recall={rec:.3f}  f1={f1:.3f}\")\n",
    "    fold_reports.append((fold, len(train_idx), len(test_idx), int(y_train.iloc[train_idx].sum()), int(y_train.iloc[test_idx].sum()), cm.tolist(), prec, rec, f1))\n",
    "\n",
    "    # quick feature drift check: compare means of top 10 numeric features between train and test\n",
    "    try:\n",
    "        feat_means_tr = X_train.iloc[train_idx].mean().sort_values(ascending=False)\n",
    "        feat_means_te = X_train.iloc[test_idx].mean()\n",
    "        top_feats = feat_means_tr.index[:10].tolist()\n",
    "        df_comp = pd.DataFrame({\n",
    "            'feat': top_feats,\n",
    "            'train_mean': X_train.iloc[train_idx][top_feats].mean().values,\n",
    "            'test_mean' : X_train.iloc[test_idx][top_feats].mean().values\n",
    "        })\n",
    "        print(\"\\nTop feature means (train vs test) for top 10 train-means:\")\n",
    "        display(df_comp)\n",
    "    except Exception as e:\n",
    "        print(\"Feature drift check failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b2507b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running per-fold diagnostics with embargo=3 and RobustScaler...\n",
      "\n",
      "=== Fold 0 ===\n",
      "train_size=950, test_size=953\n",
      "pos_train: 383 neg_train: 567\n",
      "pos_test: 348 neg_test: 605\n",
      "confusion_matrix:\n",
      " [[284 321]\n",
      " [144 204]]\n",
      "precision=0.389  recall=0.586  f1=0.467  acc=0.512\n",
      "\n",
      "Top feature means (train vs test) for top 6 train-means:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "      <th>train_mean</th>\n",
       "      <th>test_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vol_roll_mean_6</td>\n",
       "      <td>1172.308509</td>\n",
       "      <td>1063.697971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>volume</td>\n",
       "      <td>1170.477895</td>\n",
       "      <td>1064.801679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roll_max_12</td>\n",
       "      <td>999.235095</td>\n",
       "      <td>978.250514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roll_max_6</td>\n",
       "      <td>996.939105</td>\n",
       "      <td>976.438290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high</td>\n",
       "      <td>995.515895</td>\n",
       "      <td>975.332025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>roll_max_3</td>\n",
       "      <td>995.380895</td>\n",
       "      <td>975.179119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feat   train_mean    test_mean\n",
       "0  vol_roll_mean_6  1172.308509  1063.697971\n",
       "1           volume  1170.477895  1064.801679\n",
       "2      roll_max_12   999.235095   978.250514\n",
       "3       roll_max_6   996.939105   976.438290\n",
       "4             high   995.515895   975.332025\n",
       "5       roll_max_3   995.380895   975.179119"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== Fold 1 ===\n",
      "train_size=1903, test_size=953\n",
      "pos_train: 730 neg_train: 1173\n",
      "pos_test: 377 neg_test: 576\n",
      "confusion_matrix:\n",
      " [[559  17]\n",
      " [362  15]]\n",
      "precision=0.469  recall=0.040  f1=0.073  acc=0.602\n",
      "\n",
      "Top feature means (train vs test) for top 6 train-means:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "      <th>train_mean</th>\n",
       "      <th>test_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>volume</td>\n",
       "      <td>1117.790331</td>\n",
       "      <td>1473.750262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vol_roll_mean_6</td>\n",
       "      <td>1116.264976</td>\n",
       "      <td>1474.888073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roll_max_12</td>\n",
       "      <td>988.742154</td>\n",
       "      <td>1173.755530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roll_max_6</td>\n",
       "      <td>986.689932</td>\n",
       "      <td>1171.844827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high</td>\n",
       "      <td>985.424241</td>\n",
       "      <td>1171.054008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>roll_max_3</td>\n",
       "      <td>985.281172</td>\n",
       "      <td>1170.387775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feat   train_mean    test_mean\n",
       "0           volume  1117.790331  1473.750262\n",
       "1  vol_roll_mean_6  1116.264976  1474.888073\n",
       "2      roll_max_12   988.742154  1173.755530\n",
       "3       roll_max_6   986.689932  1171.844827\n",
       "4             high   985.424241  1171.054008\n",
       "5       roll_max_3   985.281172  1170.387775"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== Fold 2 ===\n",
      "train_size=2856, test_size=953\n",
      "pos_train: 1107 neg_train: 1749\n",
      "pos_test: 402 neg_test: 551\n",
      "confusion_matrix:\n",
      " [[110 441]\n",
      " [ 75 327]]\n",
      "precision=0.426  recall=0.813  f1=0.559  acc=0.459\n",
      "\n",
      "Top feature means (train vs test) for top 6 train-means:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "      <th>train_mean</th>\n",
       "      <th>test_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>volume</td>\n",
       "      <td>1236.840336</td>\n",
       "      <td>1956.527807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vol_roll_mean_6</td>\n",
       "      <td>1235.704278</td>\n",
       "      <td>1957.569780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roll_max_12</td>\n",
       "      <td>1050.130823</td>\n",
       "      <td>1479.845782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roll_max_6</td>\n",
       "      <td>1048.135340</td>\n",
       "      <td>1475.122833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high</td>\n",
       "      <td>1047.031404</td>\n",
       "      <td>1471.956537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>roll_max_3</td>\n",
       "      <td>1046.712489</td>\n",
       "      <td>1471.480965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feat   train_mean    test_mean\n",
       "0           volume  1236.840336  1956.527807\n",
       "1  vol_roll_mean_6  1235.704278  1957.569780\n",
       "2      roll_max_12  1050.130823  1479.845782\n",
       "3       roll_max_6  1048.135340  1475.122833\n",
       "4             high  1047.031404  1471.956537\n",
       "5       roll_max_3  1046.712489  1471.480965"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== Fold 3 ===\n",
      "train_size=3809, test_size=953\n",
      "pos_train: 1509 neg_train: 2300\n",
      "pos_test: 390 neg_test: 563\n",
      "confusion_matrix:\n",
      " [[255 308]\n",
      " [144 246]]\n",
      "precision=0.444  recall=0.631  f1=0.521  acc=0.526\n",
      "\n",
      "Top feature means (train vs test) for top 6 train-means:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "      <th>train_mean</th>\n",
       "      <th>test_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>volume</td>\n",
       "      <td>1417.000525</td>\n",
       "      <td>1436.405037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vol_roll_mean_6</td>\n",
       "      <td>1416.974162</td>\n",
       "      <td>1434.562434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roll_max_12</td>\n",
       "      <td>1157.534224</td>\n",
       "      <td>1627.590178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roll_max_6</td>\n",
       "      <td>1154.848270</td>\n",
       "      <td>1624.117492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high</td>\n",
       "      <td>1153.226556</td>\n",
       "      <td>1622.209609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>roll_max_3</td>\n",
       "      <td>1152.868186</td>\n",
       "      <td>1621.617119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feat   train_mean    test_mean\n",
       "0           volume  1417.000525  1436.405037\n",
       "1  vol_roll_mean_6  1416.974162  1434.562434\n",
       "2      roll_max_12  1157.534224  1627.590178\n",
       "3       roll_max_6  1154.848270  1624.117492\n",
       "4             high  1153.226556  1622.209609\n",
       "5       roll_max_3  1152.868186  1621.617119"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== Fold 4 ===\n",
      "train_size=4762, test_size=953\n",
      "pos_train: 1901 neg_train: 2861\n",
      "pos_test: 413 neg_test: 540\n",
      "confusion_matrix:\n",
      " [[233 307]\n",
      " [161 252]]\n",
      "precision=0.451  recall=0.610  f1=0.519  acc=0.509\n",
      "\n",
      "Top feature means (train vs test) for top 6 train-means:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "      <th>train_mean</th>\n",
       "      <th>test_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>volume</td>\n",
       "      <td>1420.561319</td>\n",
       "      <td>1712.741868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vol_roll_mean_6</td>\n",
       "      <td>1420.287187</td>\n",
       "      <td>1707.812697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roll_max_12</td>\n",
       "      <td>1251.444469</td>\n",
       "      <td>1893.840262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roll_max_6</td>\n",
       "      <td>1248.601640</td>\n",
       "      <td>1889.710986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high</td>\n",
       "      <td>1246.922979</td>\n",
       "      <td>1887.421364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>roll_max_3</td>\n",
       "      <td>1246.518359</td>\n",
       "      <td>1886.739780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feat   train_mean    test_mean\n",
       "0           volume  1420.561319  1712.741868\n",
       "1  vol_roll_mean_6  1420.287187  1707.812697\n",
       "2      roll_max_12  1251.444469  1893.840262\n",
       "3       roll_max_6  1248.601640  1889.710986\n",
       "4             high  1246.922979  1887.421364\n",
       "5       roll_max_3  1246.518359  1886.739780"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CELL DIAG_2 — increase embargo and re-run per-fold diagnostics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# safety checks\n",
    "for name in ['X_train','y_train']:\n",
    "    if name not in globals():\n",
    "        raise RuntimeError(f\"Required variable '{name}' not found. Re-run prior parsing/feature cells first.\")\n",
    "\n",
    "# PurgedTimeSeriesSplit but with larger embargo\n",
    "class PurgedTimeSeriesSplit:\n",
    "    def __init__(self, n_splits=5, embargo=3):\n",
    "        self.n_splits = n_splits\n",
    "        self.embargo = int(embargo)\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        test_size = n_samples // (self.n_splits + 1)\n",
    "        indices = np.arange(n_samples)\n",
    "        for i in range(self.n_splits):\n",
    "            train_end = (i + 1) * test_size\n",
    "            test_start = train_end\n",
    "            test_stop = min(n_samples, test_start + test_size)\n",
    "            # apply embargo by removing last `embargo` indices before test from train\n",
    "            embargo_start = max(0, train_end - self.embargo)\n",
    "            train_indices = indices[:embargo_start]\n",
    "            test_indices = indices[test_start:test_stop]\n",
    "            yield train_indices, test_indices\n",
    "\n",
    "purged_cv = PurgedTimeSeriesSplit(n_splits=5, embargo=3)\n",
    "\n",
    "# Use a robust pipeline for diagnostics\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", RobustScaler()), \n",
    "    (\"clf\", LogisticRegression(max_iter=2000, class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "print(\"Running per-fold diagnostics with embargo=3 and RobustScaler...\\n\")\n",
    "for fold, (train_idx, test_idx) in enumerate(purged_cv.split(X_train, y_train)):\n",
    "    print(f\"=== Fold {fold} ===\")\n",
    "    print(f\"train_size={len(train_idx)}, test_size={len(test_idx)}\")\n",
    "    if len(train_idx) == 0:\n",
    "        print(\"Empty train set for this fold — adjust n_splits/embargo.\")\n",
    "        continue\n",
    "    print(\"pos_train:\", int(y_train.iloc[train_idx].sum()), \"neg_train:\", len(train_idx)-int(y_train.iloc[train_idx].sum()))\n",
    "    print(\"pos_test:\", int(y_train.iloc[test_idx].sum()), \"neg_test:\", len(test_idx)-int(y_train.iloc[test_idx].sum()))\n",
    "    # fit & evaluate\n",
    "    pipe.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n",
    "    ypred = pipe.predict(X_train.iloc[test_idx])\n",
    "    cm = confusion_matrix(y_train.iloc[test_idx], ypred)\n",
    "    prec = precision_score(y_train.iloc[test_idx], ypred, zero_division=0)\n",
    "    rec  = recall_score(y_train.iloc[test_idx], ypred, zero_division=0)\n",
    "    f1   = f1_score(y_train.iloc[test_idx], ypred, zero_division=0)\n",
    "    acc  = accuracy_score(y_train.iloc[test_idx], ypred)\n",
    "    print(\"confusion_matrix:\\n\", cm)\n",
    "    print(f\"precision={prec:.3f}  recall={rec:.3f}  f1={f1:.3f}  acc={acc:.3f}\")\n",
    "\n",
    "    # small drift check: top 6 numeric cols by train mean, show train vs test mean\n",
    "    try:\n",
    "        feat_means_tr = X_train.iloc[train_idx].mean().sort_values(ascending=False)\n",
    "        top_feats = feat_means_tr.index[:6].tolist()\n",
    "        df_comp = pd.DataFrame({\n",
    "            'feat': top_feats,\n",
    "            'train_mean': X_train.iloc[train_idx][top_feats].mean().values,\n",
    "            'test_mean' : X_train.iloc[test_idx][top_feats].mean().values\n",
    "        })\n",
    "        print(\"\\nTop feature means (train vs test) for top 6 train-means:\")\n",
    "        display(df_comp)\n",
    "    except Exception as e:\n",
    "        print(\"Feature-drift check failed:\", e)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0034a8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning LogisticRegression with purged CV (embargo=3)...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best CV F1: 0.4422\n",
      "Best params: {'clf__C': np.float64(0.00046415888336127773)}\n",
      "\n",
      "Tuning SVM with purged CV (embargo=3)...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best CV F1: 0.5174\n",
      "Best params: {'clf__kernel': 'rbf', 'clf__gamma': 0.01, 'clf__C': 0.1}\n",
      "\n",
      "Tuning XGBoost with purged CV (embargo=3)...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best CV F1: 0.3311\n",
      "Best params: {'clf__subsample': 0.9, 'clf__n_estimators': 400, 'clf__max_depth': 3, 'clf__learning_rate': 0.1, 'clf__colsample_bytree': 0.9}\n",
      "\n",
      "LR_purged:  Acc=0.449  Prec=0.425  Rec=0.874  F1=0.571  AUC=0.511\n",
      "Confusion matrix:\n",
      " [[116 713]\n",
      " [ 76 526]]\n",
      "\n",
      "SVM_purged:  Acc=0.454  Prec=0.426  Rec=0.855  F1=0.569  AUC=0.524\n",
      "Confusion matrix:\n",
      " [[135 694]\n",
      " [ 87 515]]\n",
      "\n",
      "XGB_purged:  Acc=0.538  Prec=0.407  Rec=0.214  F1=0.281  AUC=0.493\n",
      "Confusion matrix:\n",
      " [[641 188]\n",
      " [473 129]]\n"
     ]
    }
   ],
   "source": [
    "# === CELL B — Leak-safe hyperparameter tuning with PurgedTimeSeriesSplit (embargo=3) ===\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define purged CV (embargo=3)\n",
    "class PurgedTimeSeriesSplit:\n",
    "    def __init__(self, n_splits=5, embargo=3):\n",
    "        self.n_splits = n_splits\n",
    "        self.embargo = int(embargo)\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        test_size = n_samples // (self.n_splits + 1)\n",
    "        idx = np.arange(n_samples)\n",
    "        for i in range(self.n_splits):\n",
    "            train_end = (i + 1) * test_size\n",
    "            test_start = train_end\n",
    "            test_stop = min(n_samples, test_start + test_size)\n",
    "            embargo_start = max(0, train_end - self.embargo)\n",
    "            train_idx = idx[:embargo_start]\n",
    "            test_idx  = idx[test_start:test_stop]\n",
    "            yield train_idx, test_idx\n",
    "\n",
    "purged_cv = PurgedTimeSeriesSplit(n_splits=5, embargo=3)\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# Pipelines\n",
    "pipe_lr  = Pipeline([(\"scaler\", RobustScaler()),\n",
    "                     (\"clf\", LogisticRegression(max_iter=2000, class_weight='balanced', random_state=42))])\n",
    "pipe_svm = Pipeline([(\"scaler\", RobustScaler()),\n",
    "                     (\"clf\", SVC(probability=True, class_weight='balanced', random_state=42))])\n",
    "pipe_xgb = Pipeline([(\"scaler\", RobustScaler()),\n",
    "                     (\"clf\", XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42,\n",
    "                                           scale_pos_weight=int((y_train==0).sum()/(y_train==1).sum()))) ])\n",
    "\n",
    "# Parameter grids\n",
    "param_lr  = {\"clf__C\": np.logspace(-4, 2, 10)}\n",
    "param_svm = {\"clf__C\": [0.01, 0.1, 1, 5, 10], \"clf__gamma\": [\"scale\", 0.01, 0.001], \"clf__kernel\": [\"rbf\"]}\n",
    "param_xgb = {\"clf__n_estimators\": [100,200,400],\n",
    "             \"clf__max_depth\": [3,5,8],\n",
    "             \"clf__learning_rate\": [0.01,0.05,0.1],\n",
    "             \"clf__subsample\": [0.7,0.9],\n",
    "             \"clf__colsample_bytree\": [0.7,0.9]}\n",
    "\n",
    "def tune_model(name, pipe, param_dist, n_iter=10):\n",
    "    print(f\"\\nTuning {name} with purged CV (embargo=3)...\")\n",
    "    search = RandomizedSearchCV(pipe, param_distributions=param_dist, n_iter=n_iter,\n",
    "                                scoring=f1_scorer, cv=purged_cv, n_jobs=-1,\n",
    "                                random_state=42, verbose=2, refit=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    print(f\"Best CV F1: {search.best_score_:.4f}\")\n",
    "    print(\"Best params:\", search.best_params_)\n",
    "    return search.best_estimator_\n",
    "\n",
    "best_lr_purged  = tune_model(\"LogisticRegression\", pipe_lr, param_lr)\n",
    "best_svm_purged = tune_model(\"SVM\", pipe_svm, param_svm)\n",
    "best_xgb_purged = tune_model(\"XGBoost\", pipe_xgb, param_xgb)\n",
    "\n",
    "# Evaluate tuned models on the holdout test set\n",
    "# === corrected evaluation print block ===\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "for name, model in [(\"LR_purged\", best_lr_purged),\n",
    "                    (\"SVM_purged\", best_svm_purged),\n",
    "                    (\"XGB_purged\", best_xgb_purged)]:\n",
    "    y_pred = model.predict(X_test)\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    except Exception:\n",
    "        y_proba = None\n",
    "\n",
    "    acc  = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec  = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1   = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc  = roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
    "\n",
    "    auc_str = f\"{auc:.3f}\" if auc is not None else \"None\"\n",
    "    print(f\"\\n{name}:  Acc={acc:.3f}  Prec={prec:.3f}  Rec={rec:.3f}  F1={f1:.3f}  AUC={auc_str}\")\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5415d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed articles: 70731\n",
      "Date range: 2017-12-07 20:00:00+00:00 → 2019-02-07 23:10:00+00:00\n",
      "Unique sites: 1727\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published</th>\n",
       "      <th>site</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-03 20:33:00+00:00</td>\n",
       "      <td>www.nephrologynews.com</td>\n",
       "      <td>Annual RPA meeting emphasizes physician leader...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 12:36:00+00:00</td>\n",
       "      <td>www.thelincolnianonline.com</td>\n",
       "      <td>Somewhat Positive News Coverage Somewhat Unlik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 08:47:00+00:00</td>\n",
       "      <td>finance.yahoo.com</td>\n",
       "      <td>There is a 40% chance Apple will acquire Netfl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 12:08:00+00:00</td>\n",
       "      <td>ledgergazette.com</td>\n",
       "      <td>Apple Inc. (AAPL) Short Interest Down 8.1% in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 20:02:00+00:00</td>\n",
       "      <td>www.fool.com</td>\n",
       "      <td>2 Warren Buffett Stocks to Consider Buying Now...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-01-01 14:20:00+00:00</td>\n",
       "      <td>www.dailypolitical.com</td>\n",
       "      <td>Apple Inc. (AAPL) Shares Sold by Peoples Bank OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-01-01 21:46:00+00:00</td>\n",
       "      <td>ledgergazette.com</td>\n",
       "      <td>Sumitomo Life Insurance Co. Sells 5,920 Shares...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-01-01 10:07:00+00:00</td>\n",
       "      <td>heraldks.com</td>\n",
       "      <td>Centre Asset Management Has Increased By $1.69...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-01-01 01:29:00+00:00</td>\n",
       "      <td>www.financialnewsusa.com</td>\n",
       "      <td>Here’s How Much The FANG+ Stocks Gained In 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-01-03 00:00:00+00:00</td>\n",
       "      <td>www.bnn.ca</td>\n",
       "      <td>Intel chips design flaw fix causes them to slo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  published                         site  \\\n",
       "0 2018-01-03 20:33:00+00:00       www.nephrologynews.com   \n",
       "1 2018-01-01 12:36:00+00:00  www.thelincolnianonline.com   \n",
       "2 2018-01-01 08:47:00+00:00            finance.yahoo.com   \n",
       "3 2018-01-01 12:08:00+00:00            ledgergazette.com   \n",
       "4 2018-01-01 20:02:00+00:00                 www.fool.com   \n",
       "5 2018-01-01 14:20:00+00:00       www.dailypolitical.com   \n",
       "6 2018-01-01 21:46:00+00:00            ledgergazette.com   \n",
       "7 2018-01-01 10:07:00+00:00                 heraldks.com   \n",
       "8 2018-01-01 01:29:00+00:00     www.financialnewsusa.com   \n",
       "9 2018-01-03 00:00:00+00:00                   www.bnn.ca   \n",
       "\n",
       "                                               title  \n",
       "0  Annual RPA meeting emphasizes physician leader...  \n",
       "1  Somewhat Positive News Coverage Somewhat Unlik...  \n",
       "2  There is a 40% chance Apple will acquire Netfl...  \n",
       "3  Apple Inc. (AAPL) Short Interest Down 8.1% in ...  \n",
       "4  2 Warren Buffett Stocks to Consider Buying Now...  \n",
       "5   Apple Inc. (AAPL) Shares Sold by Peoples Bank OH  \n",
       "6  Sumitomo Life Insurance Co. Sells 5,920 Shares...  \n",
       "7  Centre Asset Management Has Increased By $1.69...  \n",
       "8  Here’s How Much The FANG+ Stocks Gained In 201...  \n",
       "9  Intel chips design flaw fix causes them to slo...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell News_2: Parse all JSON news files ===\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "NEWS_DIR = r\"C:\\Users\\amanb\\OneDrive\\Desktop\\SWM Project\\Extracted_News\"\n",
    "\n",
    "rows = []\n",
    "for root, _, files in os.walk(NEWS_DIR):\n",
    "    for f in files:\n",
    "        if f.lower().endswith(\".json\"):\n",
    "            path = os.path.join(root, f)\n",
    "            try:\n",
    "                with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
    "                    data = json.load(fh)\n",
    "                published = data.get(\"published\") or data.get(\"thread\", {}).get(\"published\")\n",
    "                title = data.get(\"title\") or \"\"\n",
    "                text  = data.get(\"text\") or \"\"\n",
    "                site  = data.get(\"thread\", {}).get(\"site_full\") or data.get(\"thread\", {}).get(\"site\") or \"\"\n",
    "                url   = data.get(\"url\") or data.get(\"thread\", {}).get(\"url\") or \"\"\n",
    "                lang  = data.get(\"language\") or data.get(\"thread\", {}).get(\"language\") or \"unknown\"\n",
    "\n",
    "                # combine title+text for sentiment\n",
    "                full_text = (title + \" \" + text).strip()\n",
    "\n",
    "                rows.append({\n",
    "                    \"published\": published,\n",
    "                    \"title\": title,\n",
    "                    \"text\": text,\n",
    "                    \"site\": site,\n",
    "                    \"url\": url,\n",
    "                    \"language\": lang,\n",
    "                    \"full_text\": full_text\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(\"Error reading\", f, \":\", e)\n",
    "\n",
    "news_df = pd.DataFrame(rows)\n",
    "\n",
    "# --- Basic cleaning ---\n",
    "# Parse datetime with timezone awareness; drop unparsable\n",
    "news_df['published'] = pd.to_datetime(news_df['published'], errors='coerce', utc=True)\n",
    "news_df = news_df.dropna(subset=['published'])\n",
    "\n",
    "# Keep only English\n",
    "news_df = news_df[news_df['language'].str.lower() == 'english']\n",
    "\n",
    "# Drop empty texts\n",
    "news_df = news_df[news_df['full_text'].str.strip().astype(bool)]\n",
    "\n",
    "print(\"Parsed articles:\", len(news_df))\n",
    "print(\"Date range:\", news_df['published'].min(), \"→\", news_df['published'].max())\n",
    "print(\"Unique sites:\", news_df['site'].nunique())\n",
    "\n",
    "display(news_df[['published','site','title']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae8e217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in c:\\users\\amanb\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.3.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: requests in c:\\users\\amanb\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaderSentiment) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\amanb\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amanb\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amanb\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amanb\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (2025.4.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VADER sentiment: 100%|██████████| 70731/70731 [28:04<00:00, 42.00it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER sentiment computed for 70731 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published</th>\n",
       "      <th>site</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-03 20:33:00+00:00</td>\n",
       "      <td>www.nephrologynews.com</td>\n",
       "      <td>0.9983</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.006</td>\n",
       "      <td>Annual RPA meeting emphasizes physician leader...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 12:36:00+00:00</td>\n",
       "      <td>www.thelincolnianonline.com</td>\n",
       "      <td>0.9751</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.029</td>\n",
       "      <td>Somewhat Positive News Coverage Somewhat Unlik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 08:47:00+00:00</td>\n",
       "      <td>finance.yahoo.com</td>\n",
       "      <td>0.8625</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.038</td>\n",
       "      <td>There is a 40% chance Apple will acquire Netfl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 12:08:00+00:00</td>\n",
       "      <td>ledgergazette.com</td>\n",
       "      <td>0.9959</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.018</td>\n",
       "      <td>Apple Inc. (AAPL) Short Interest Down 8.1% in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 20:02:00+00:00</td>\n",
       "      <td>www.fool.com</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.030</td>\n",
       "      <td>2 Warren Buffett Stocks to Consider Buying Now...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-01-01 14:20:00+00:00</td>\n",
       "      <td>www.dailypolitical.com</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.005</td>\n",
       "      <td>Apple Inc. (AAPL) Shares Sold by Peoples Bank OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-01-01 21:46:00+00:00</td>\n",
       "      <td>ledgergazette.com</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.018</td>\n",
       "      <td>Sumitomo Life Insurance Co. Sells 5,920 Shares...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-01-01 10:07:00+00:00</td>\n",
       "      <td>heraldks.com</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.026</td>\n",
       "      <td>Centre Asset Management Has Increased By $1.69...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-01-01 01:29:00+00:00</td>\n",
       "      <td>www.financialnewsusa.com</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.028</td>\n",
       "      <td>Here’s How Much The FANG+ Stocks Gained In 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-01-03 00:00:00+00:00</td>\n",
       "      <td>www.bnn.ca</td>\n",
       "      <td>0.6803</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.032</td>\n",
       "      <td>Intel chips design flaw fix causes them to slo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  published                         site  vader_compound  \\\n",
       "0 2018-01-03 20:33:00+00:00       www.nephrologynews.com          0.9983   \n",
       "1 2018-01-01 12:36:00+00:00  www.thelincolnianonline.com          0.9751   \n",
       "2 2018-01-01 08:47:00+00:00            finance.yahoo.com          0.8625   \n",
       "3 2018-01-01 12:08:00+00:00            ledgergazette.com          0.9959   \n",
       "4 2018-01-01 20:02:00+00:00                 www.fool.com          0.9986   \n",
       "5 2018-01-01 14:20:00+00:00       www.dailypolitical.com          0.9979   \n",
       "6 2018-01-01 21:46:00+00:00            ledgergazette.com          0.9978   \n",
       "7 2018-01-01 10:07:00+00:00                 heraldks.com          0.9990   \n",
       "8 2018-01-01 01:29:00+00:00     www.financialnewsusa.com          0.9964   \n",
       "9 2018-01-03 00:00:00+00:00                   www.bnn.ca          0.6803   \n",
       "\n",
       "   vader_pos  vader_neg                                              title  \n",
       "0      0.137      0.006  Annual RPA meeting emphasizes physician leader...  \n",
       "1      0.076      0.029  Somewhat Positive News Coverage Somewhat Unlik...  \n",
       "2      0.066      0.038  There is a 40% chance Apple will acquire Netfl...  \n",
       "3      0.096      0.018  Apple Inc. (AAPL) Short Interest Down 8.1% in ...  \n",
       "4      0.136      0.030  2 Warren Buffett Stocks to Consider Buying Now...  \n",
       "5      0.114      0.005   Apple Inc. (AAPL) Shares Sold by Peoples Bank OH  \n",
       "6      0.115      0.018  Sumitomo Life Insurance Co. Sells 5,920 Shares...  \n",
       "7      0.149      0.026  Centre Asset Management Has Increased By $1.69...  \n",
       "8      0.173      0.028  Here’s How Much The FANG+ Stocks Gained In 201...  \n",
       "9      0.048      0.032  Intel chips design flaw fix causes them to slo...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of compound scores:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    70731.000000\n",
       "mean         0.702274\n",
       "std          0.605657\n",
       "min         -1.000000\n",
       "25%          0.867900\n",
       "50%          0.990200\n",
       "75%          0.997800\n",
       "max          1.000000\n",
       "Name: vader_compound, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion positive (>0.05): 0.8580678910237378\n",
      "Proportion negative (<-0.05): 0.13373202697544218\n"
     ]
    }
   ],
   "source": [
    "# === Cell News_3 — Compute VADER sentiment on all articles ===\n",
    "%pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Compute sentiment for each article\n",
    "scores = []\n",
    "for text in tqdm(news_df['full_text'], desc=\"VADER sentiment\"):\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        scores.append({'vader_neg': np.nan, 'vader_neu': np.nan,\n",
    "                       'vader_pos': np.nan, 'vader_compound': np.nan})\n",
    "        continue\n",
    "    s = analyzer.polarity_scores(text)\n",
    "    scores.append({'vader_neg': s['neg'], 'vader_neu': s['neu'],\n",
    "                   'vader_pos': s['pos'], 'vader_compound': s['compound']})\n",
    "\n",
    "# Merge scores into dataframe\n",
    "sent_df = pd.DataFrame(scores)\n",
    "news_df = pd.concat([news_df.reset_index(drop=True), sent_df], axis=1)\n",
    "\n",
    "print(\"VADER sentiment computed for\", len(news_df), \"articles\")\n",
    "display(news_df[['published','site','vader_compound','vader_pos','vader_neg','title']].head(10))\n",
    "\n",
    "# quick stats\n",
    "print(\"\\nSummary of compound scores:\")\n",
    "display(news_df['vader_compound'].describe())\n",
    "print(\"Proportion positive (>0.05):\", (news_df['vader_compound']>0.05).mean())\n",
    "print(\"Proportion negative (<-0.05):\", (news_df['vader_compound']<-0.05).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11bc4020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated bins: 14759\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_count</th>\n",
       "      <th>vader_mean</th>\n",
       "      <th>vader_sum</th>\n",
       "      <th>vader_std</th>\n",
       "      <th>pos_count</th>\n",
       "      <th>neg_count</th>\n",
       "      <th>pos_prop</th>\n",
       "      <th>neg_prop</th>\n",
       "      <th>vader_mean_roll_4</th>\n",
       "      <th>vader_mean_roll_12</th>\n",
       "      <th>news_count_roll_4</th>\n",
       "      <th>news_count_roll_12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bin_ts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-12-07 20:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9898</td>\n",
       "      <td>0.9898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.989800</td>\n",
       "      <td>0.989800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-08 21:30:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.994800</td>\n",
       "      <td>0.994800</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-12 01:30:00</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.9976</td>\n",
       "      <td>-0.9976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.330667</td>\n",
       "      <td>0.330667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-12 22:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.497700</td>\n",
       "      <td>0.497700</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-14 11:30:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9277</td>\n",
       "      <td>0.9277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482175</td>\n",
       "      <td>0.583700</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-14 12:30:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9552</td>\n",
       "      <td>0.9552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.471025</td>\n",
       "      <td>0.645617</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-15 02:30:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.970150</td>\n",
       "      <td>0.696086</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-15 12:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.933725</td>\n",
       "      <td>0.715712</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     news_count  vader_mean  vader_sum  vader_std  pos_count  \\\n",
       "bin_ts                                                                         \n",
       "2017-12-07 20:00:00           1      0.9898     0.9898        0.0          1   \n",
       "2017-12-08 21:30:00           1      0.9998     0.9998        0.0          1   \n",
       "2017-12-12 01:30:00           1     -0.9976    -0.9976        0.0          0   \n",
       "2017-12-12 22:00:00           1      0.9988     0.9988        0.0          1   \n",
       "2017-12-14 11:30:00           1      0.9277     0.9277        0.0          1   \n",
       "2017-12-14 12:30:00           1      0.9552     0.9552        0.0          1   \n",
       "2017-12-15 02:30:00           1      0.9989     0.9989        0.0          1   \n",
       "2017-12-15 12:00:00           1      0.8531     0.8531        0.0          1   \n",
       "\n",
       "                     neg_count  pos_prop  neg_prop  vader_mean_roll_4  \\\n",
       "bin_ts                                                                  \n",
       "2017-12-07 20:00:00          0       1.0       0.0           0.989800   \n",
       "2017-12-08 21:30:00          0       1.0       0.0           0.994800   \n",
       "2017-12-12 01:30:00          1       0.0       1.0           0.330667   \n",
       "2017-12-12 22:00:00          0       1.0       0.0           0.497700   \n",
       "2017-12-14 11:30:00          0       1.0       0.0           0.482175   \n",
       "2017-12-14 12:30:00          0       1.0       0.0           0.471025   \n",
       "2017-12-15 02:30:00          0       1.0       0.0           0.970150   \n",
       "2017-12-15 12:00:00          0       1.0       0.0           0.933725   \n",
       "\n",
       "                     vader_mean_roll_12  news_count_roll_4  news_count_roll_12  \n",
       "bin_ts                                                                          \n",
       "2017-12-07 20:00:00            0.989800                1.0                 1.0  \n",
       "2017-12-08 21:30:00            0.994800                2.0                 2.0  \n",
       "2017-12-12 01:30:00            0.330667                3.0                 3.0  \n",
       "2017-12-12 22:00:00            0.497700                4.0                 4.0  \n",
       "2017-12-14 11:30:00            0.583700                4.0                 5.0  \n",
       "2017-12-14 12:30:00            0.645617                4.0                 6.0  \n",
       "2017-12-15 02:30:00            0.696086                4.0                 7.0  \n",
       "2017-12-15 12:00:00            0.715712                4.0                 8.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell News_4 — aggregate news sentiment into 30-minute bins ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "articles_df = news_df.copy()\n",
    "articles_df['published_ts'] = pd.to_datetime(articles_df['published']).dt.tz_convert(None)\n",
    "\n",
    "# Round down to 30-minute bins\n",
    "articles_df['bin_ts'] = articles_df['published_ts'].dt.floor('30T')\n",
    "\n",
    "# Define helpers\n",
    "def is_pos(x): return (x > 0.05)\n",
    "def is_neg(x): return (x < -0.05)\n",
    "\n",
    "# Aggregate per 30-min bin\n",
    "agg = articles_df.groupby('bin_ts').agg(\n",
    "    news_count=('vader_compound','count'),\n",
    "    vader_mean =('vader_compound','mean'),\n",
    "    vader_sum  =('vader_compound','sum'),\n",
    "    vader_std  =('vader_compound','std'),\n",
    "    pos_count  =('vader_compound',lambda s:is_pos(s).sum()),\n",
    "    neg_count  =('vader_compound',lambda s:is_neg(s).sum())\n",
    ")\n",
    "agg['pos_prop'] = agg['pos_count']/agg['news_count']\n",
    "agg['neg_prop'] = agg['neg_count']/agg['news_count']\n",
    "\n",
    "# Fill missing std with 0\n",
    "agg['vader_std'] = agg['vader_std'].fillna(0)\n",
    "\n",
    "# Rolling features (2 h = 4 bins, 6 h = 12 bins)\n",
    "agg['vader_mean_roll_4']  = agg['vader_mean'].rolling(4,  min_periods=1).mean()\n",
    "agg['vader_mean_roll_12'] = agg['vader_mean'].rolling(12, min_periods=1).mean()\n",
    "agg['news_count_roll_4']  = agg['news_count'].rolling(4,  min_periods=1).sum()\n",
    "agg['news_count_roll_12'] = agg['news_count'].rolling(12, min_periods=1).sum()\n",
    "\n",
    "print(\"Aggregated bins:\", len(agg))\n",
    "display(agg.head(8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28f4313e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built model_df_safe with shape: (7152, 36)\n",
      "Target distribution (value:count): {0: 4236, 1: 2916}\n",
      "Target proportions: {0: 0.5922818791946308, 1: 0.4077181208053691}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>return_1</th>\n",
       "      <th>log_return_1</th>\n",
       "      <th>ema_8</th>\n",
       "      <th>ema_21</th>\n",
       "      <th>ema_ratio_8_21</th>\n",
       "      <th>...</th>\n",
       "      <th>roll_mean_12</th>\n",
       "      <th>roll_std_12</th>\n",
       "      <th>roll_max_12</th>\n",
       "      <th>roll_min_12</th>\n",
       "      <th>mom_3</th>\n",
       "      <th>vol_change_1</th>\n",
       "      <th>vol_roll_mean_6</th>\n",
       "      <th>close_future_h</th>\n",
       "      <th>future_return_h</th>\n",
       "      <th>target_thresholded</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-05-12 19:00:00</th>\n",
       "      <td>961.91</td>\n",
       "      <td>962.76</td>\n",
       "      <td>960.63</td>\n",
       "      <td>961.74</td>\n",
       "      <td>1384</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>960.933333</td>\n",
       "      <td>960.771818</td>\n",
       "      <td>1.000168</td>\n",
       "      <td>...</td>\n",
       "      <td>961.275000</td>\n",
       "      <td>0.869741</td>\n",
       "      <td>961.89</td>\n",
       "      <td>960.66</td>\n",
       "      <td>0.615000</td>\n",
       "      <td>0.039877</td>\n",
       "      <td>997.500000</td>\n",
       "      <td>961.34</td>\n",
       "      <td>-0.000416</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-12 19:30:00</th>\n",
       "      <td>961.77</td>\n",
       "      <td>962.27</td>\n",
       "      <td>961.13</td>\n",
       "      <td>961.34</td>\n",
       "      <td>2110</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>961.112593</td>\n",
       "      <td>960.859835</td>\n",
       "      <td>1.000263</td>\n",
       "      <td>...</td>\n",
       "      <td>961.430000</td>\n",
       "      <td>0.671044</td>\n",
       "      <td>961.89</td>\n",
       "      <td>960.66</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.360865</td>\n",
       "      <td>1126.333333</td>\n",
       "      <td>960.20</td>\n",
       "      <td>-0.001186</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 12:30:00</th>\n",
       "      <td>962.39</td>\n",
       "      <td>962.50</td>\n",
       "      <td>959.97</td>\n",
       "      <td>960.20</td>\n",
       "      <td>121</td>\n",
       "      <td>-0.000416</td>\n",
       "      <td>-0.000416</td>\n",
       "      <td>961.163128</td>\n",
       "      <td>960.903486</td>\n",
       "      <td>1.000270</td>\n",
       "      <td>...</td>\n",
       "      <td>961.407500</td>\n",
       "      <td>0.549750</td>\n",
       "      <td>961.89</td>\n",
       "      <td>960.66</td>\n",
       "      <td>-0.316667</td>\n",
       "      <td>0.524566</td>\n",
       "      <td>1372.250000</td>\n",
       "      <td>958.68</td>\n",
       "      <td>-0.001583</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 13:00:00</th>\n",
       "      <td>960.10</td>\n",
       "      <td>960.70</td>\n",
       "      <td>958.09</td>\n",
       "      <td>958.68</td>\n",
       "      <td>372</td>\n",
       "      <td>-0.001186</td>\n",
       "      <td>-0.001187</td>\n",
       "      <td>960.949099</td>\n",
       "      <td>960.839533</td>\n",
       "      <td>1.000114</td>\n",
       "      <td>...</td>\n",
       "      <td>961.166000</td>\n",
       "      <td>0.719917</td>\n",
       "      <td>961.89</td>\n",
       "      <td>960.20</td>\n",
       "      <td>-0.893333</td>\n",
       "      <td>-0.942654</td>\n",
       "      <td>1122.000000</td>\n",
       "      <td>959.97</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 13:30:00</th>\n",
       "      <td>958.66</td>\n",
       "      <td>963.00</td>\n",
       "      <td>957.00</td>\n",
       "      <td>959.97</td>\n",
       "      <td>2512</td>\n",
       "      <td>-0.001583</td>\n",
       "      <td>-0.001584</td>\n",
       "      <td>960.444855</td>\n",
       "      <td>960.643212</td>\n",
       "      <td>0.999794</td>\n",
       "      <td>...</td>\n",
       "      <td>960.751667</td>\n",
       "      <td>1.201939</td>\n",
       "      <td>961.89</td>\n",
       "      <td>958.68</td>\n",
       "      <td>-1.393333</td>\n",
       "      <td>2.074380</td>\n",
       "      <td>997.000000</td>\n",
       "      <td>957.54</td>\n",
       "      <td>-0.002531</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       open    high     low   close  volume  return_1  \\\n",
       "ts                                                                      \n",
       "2017-05-12 19:00:00  961.91  962.76  960.63  961.74    1384  0.001280   \n",
       "2017-05-12 19:30:00  961.77  962.27  961.13  961.34    2110 -0.000156   \n",
       "2017-05-15 12:30:00  962.39  962.50  959.97  960.20     121 -0.000416   \n",
       "2017-05-15 13:00:00  960.10  960.70  958.09  958.68     372 -0.001186   \n",
       "2017-05-15 13:30:00  958.66  963.00  957.00  959.97    2512 -0.001583   \n",
       "\n",
       "                     log_return_1       ema_8      ema_21  ema_ratio_8_21  \\\n",
       "ts                                                                          \n",
       "2017-05-12 19:00:00      0.001280  960.933333  960.771818        1.000168   \n",
       "2017-05-12 19:30:00     -0.000156  961.112593  960.859835        1.000263   \n",
       "2017-05-15 12:30:00     -0.000416  961.163128  960.903486        1.000270   \n",
       "2017-05-15 13:00:00     -0.001187  960.949099  960.839533        1.000114   \n",
       "2017-05-15 13:30:00     -0.001584  960.444855  960.643212        0.999794   \n",
       "\n",
       "                     ...  roll_mean_12  roll_std_12  roll_max_12  roll_min_12  \\\n",
       "ts                   ...                                                        \n",
       "2017-05-12 19:00:00  ...    961.275000     0.869741       961.89       960.66   \n",
       "2017-05-12 19:30:00  ...    961.430000     0.671044       961.89       960.66   \n",
       "2017-05-15 12:30:00  ...    961.407500     0.549750       961.89       960.66   \n",
       "2017-05-15 13:00:00  ...    961.166000     0.719917       961.89       960.20   \n",
       "2017-05-15 13:30:00  ...    960.751667     1.201939       961.89       958.68   \n",
       "\n",
       "                        mom_3  vol_change_1  vol_roll_mean_6  close_future_h  \\\n",
       "ts                                                                             \n",
       "2017-05-12 19:00:00  0.615000      0.039877       997.500000          961.34   \n",
       "2017-05-12 19:30:00  0.310000      0.360865      1126.333333          960.20   \n",
       "2017-05-15 12:30:00 -0.316667      0.524566      1372.250000          958.68   \n",
       "2017-05-15 13:00:00 -0.893333     -0.942654      1122.000000          959.97   \n",
       "2017-05-15 13:30:00 -1.393333      2.074380       997.000000          957.54   \n",
       "\n",
       "                     future_return_h  target_thresholded  \n",
       "ts                                                        \n",
       "2017-05-12 19:00:00        -0.000416                   0  \n",
       "2017-05-12 19:30:00        -0.001186                   0  \n",
       "2017-05-15 12:30:00        -0.001583                   0  \n",
       "2017-05-15 13:00:00         0.001346                   1  \n",
       "2017-05-15 13:30:00        -0.002531                   0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === REBUILD_MODEL_DF_SAFE ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Safety checks\n",
    "if 'df' not in globals():\n",
    "    raise RuntimeError(\"Original parsed DataFrame 'df' not found. Run parsing cell 1-5 first.\")\n",
    "required_cols = {'open','high','low','close','volume'}\n",
    "if not required_cols.issubset(set(df.columns)):\n",
    "    raise RuntimeError(f\"'df' missing required cols. Required: {required_cols}. Found: {list(df.columns)}\")\n",
    "\n",
    "# parameters (use existing if defined)\n",
    "HORIZON = globals().get('HORIZON', 1)\n",
    "THRESHOLD = globals().get('THRESHOLD', 0.0005)\n",
    "\n",
    "def build_clean_model_df(prices_df, horizon=1, threshold=0.0005):\n",
    "    p = prices_df.copy().sort_index()\n",
    "\n",
    "    # create shifted historical series (values used at time t are from <= t-1)\n",
    "    close_prev = p['close'].shift(1)\n",
    "    high_prev = p['high'].shift(1)\n",
    "    low_prev  = p['low'].shift(1)\n",
    "    vol_prev  = p['volume'].shift(1)\n",
    "\n",
    "    # Basic lagged returns (based on close_prev)\n",
    "    p['return_1'] = close_prev.pct_change().fillna(0)\n",
    "    p['log_return_1'] = np.log(close_prev).diff().fillna(0)\n",
    "\n",
    "    # EMAs on shifted close\n",
    "    p['ema_8']  = close_prev.ewm(span=8, adjust=False).mean()\n",
    "    p['ema_21'] = close_prev.ewm(span=21, adjust=False).mean()\n",
    "    p['ema_ratio_8_21'] = p['ema_8'] / (p['ema_21'] + 1e-9)\n",
    "\n",
    "    # MACD on shifted close\n",
    "    ema12 = close_prev.ewm(span=12, adjust=False).mean()\n",
    "    ema26 = close_prev.ewm(span=26, adjust=False).mean()\n",
    "    macd = ema12 - ema26\n",
    "    macd_sig = macd.ewm(span=9, adjust=False).mean()\n",
    "    p['macd'] = macd\n",
    "    p['macd_sig'] = macd_sig\n",
    "    p['macd_hist'] = macd - macd_sig\n",
    "\n",
    "    # RSI (14) on shifted close\n",
    "    delta = close_prev.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    roll_gain = gain.rolling(14, min_periods=1).mean()\n",
    "    roll_loss = loss.rolling(14, min_periods=1).mean()\n",
    "    rs = roll_gain / (roll_loss + 1e-9)\n",
    "    p['rsi_14'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # Bollinger bands (20) on shifted close\n",
    "    ma20 = close_prev.rolling(20, min_periods=1).mean()\n",
    "    std20 = close_prev.rolling(20, min_periods=1).std().fillna(0)\n",
    "    p['bb_upper'] = ma20 + 2 * std20\n",
    "    p['bb_lower'] = ma20 - 2 * std20\n",
    "    p['bb_width'] = (p['bb_upper'] - p['bb_lower']) / (ma20 + 1e-9)\n",
    "\n",
    "    # ATR using shifted highs/lows and an earlier previous close for TR\n",
    "    prev_close_for_tr = p['close'].shift(2)\n",
    "    tr1 = high_prev - low_prev\n",
    "    tr2 = (high_prev - prev_close_for_tr).abs()\n",
    "    tr3 = (low_prev - prev_close_for_tr).abs()\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    p['atr_14'] = tr.rolling(14, min_periods=1).mean()\n",
    "\n",
    "    # Rolling features (example windows)\n",
    "    wins = [3,6,12]\n",
    "    for w in wins:\n",
    "        p[f'roll_mean_{w}'] = close_prev.rolling(window=w, min_periods=1).mean()\n",
    "        p[f'roll_std_{w}'] = close_prev.rolling(window=w, min_periods=1).std().fillna(0)\n",
    "        p[f'roll_max_{w}'] = close_prev.rolling(window=w, min_periods=1).max()\n",
    "        p[f'roll_min_{w}'] = close_prev.rolling(window=w, min_periods=1).min()\n",
    "\n",
    "    # Momentum & volume features (lagged)\n",
    "    p['mom_3'] = close_prev - p['roll_mean_3']\n",
    "    p['vol_change_1'] = vol_prev.pct_change().fillna(0)\n",
    "    p['vol_roll_mean_6'] = vol_prev.rolling(6, min_periods=1).mean()\n",
    "\n",
    "    # Build target (future close at horizon) — NOT to be used as features\n",
    "    p['close_future_h'] = p['close'].shift(-horizon)\n",
    "    p['future_return_h'] = (p['close_future_h'] / p['close']) - 1\n",
    "\n",
    "    # Create clean 1-D integer target series and assign\n",
    "    tgt_ser = (p['future_return_h'] > threshold).astype(int)\n",
    "    tgt_ser = pd.Series(tgt_ser.values, index=p.index, name='target_thresholded', dtype='int64')\n",
    "    p['target_thresholded'] = tgt_ser\n",
    "\n",
    "    # select numeric columns (keep target)\n",
    "    numeric_cols = p.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'target_thresholded' not in numeric_cols:\n",
    "        numeric_cols.append('target_thresholded')\n",
    "    numeric_cols = list(dict.fromkeys(numeric_cols))  # order-preserving unique\n",
    "\n",
    "    model_safe = p[numeric_cols].copy()\n",
    "    model_safe = model_safe.dropna().copy()\n",
    "\n",
    "    # final safety: ensure single-column int Series for target\n",
    "    if isinstance(model_safe['target_thresholded'], pd.DataFrame):\n",
    "        model_safe['target_thresholded'] = model_safe['target_thresholded'].iloc[:, -1]\n",
    "    # Convert with a safe path (work with Series)\n",
    "    tgt_series = pd.Series(model_safe['target_thresholded'].values, index=model_safe.index)\n",
    "    tgt_series = pd.to_numeric(tgt_series, errors='coerce').fillna(0).astype(int)\n",
    "    model_safe['target_thresholded'] = tgt_series\n",
    "\n",
    "    return model_safe\n",
    "\n",
    "# Rebuild\n",
    "model_df_safe = build_clean_model_df(df, horizon=HORIZON, threshold=THRESHOLD)\n",
    "\n",
    "# Verify and print\n",
    "print(\"Built model_df_safe with shape:\", model_df_safe.shape)\n",
    "vals, counts = np.unique(model_df_safe['target_thresholded'].to_numpy(), return_counts=True)\n",
    "print(\"Target distribution (value:count):\", dict(zip(vals.tolist(), counts.tolist())))\n",
    "print(\"Target proportions:\", dict(zip(vals.tolist(), (counts/counts.sum()).tolist())))\n",
    "display(model_df_safe.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18ca77ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price index range: 2017-05-12 19:00:00 → 2019-02-01 19:30:00\n",
      "News bins range : 2017-12-07 20:00:00 → 2019-02-07 23:00:00\n",
      "\n",
      "Joined model_df_news shape: (7152, 48)\n",
      "News columns added: ['news_count', 'vader_mean', 'vader_sum', 'vader_std', 'pos_count', 'neg_count', 'pos_prop', 'neg_prop', 'vader_mean_roll_4', 'vader_mean_roll_12', 'news_count_roll_4', 'news_count_roll_12']\n",
      "\n",
      "Sample head (news cols):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_count</th>\n",
       "      <th>vader_mean</th>\n",
       "      <th>vader_sum</th>\n",
       "      <th>vader_std</th>\n",
       "      <th>pos_prop</th>\n",
       "      <th>neg_prop</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-05-12 19:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-12 19:30:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 12:30:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 13:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 13:30:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 14:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 14:30:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 15:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 15:30:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15 16:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     news_count  vader_mean  vader_sum  vader_std  pos_prop  \\\n",
       "ts                                                                            \n",
       "2017-05-12 19:00:00         0.0         0.0        0.0        0.0       0.0   \n",
       "2017-05-12 19:30:00         0.0         0.0        0.0        0.0       0.0   \n",
       "2017-05-15 12:30:00         0.0         0.0        0.0        0.0       0.0   \n",
       "2017-05-15 13:00:00         0.0         0.0        0.0        0.0       0.0   \n",
       "2017-05-15 13:30:00         0.0         0.0        0.0        0.0       0.0   \n",
       "2017-05-15 14:00:00         0.0         0.0        0.0        0.0       0.0   \n",
       "2017-05-15 14:30:00         0.0         0.0        0.0        0.0       0.0   \n",
       "2017-05-15 15:00:00         0.0         0.0        0.0        0.0       0.0   \n",
       "2017-05-15 15:30:00         0.0         0.0        0.0        0.0       0.0   \n",
       "2017-05-15 16:00:00         0.0         0.0        0.0        0.0       0.0   \n",
       "\n",
       "                     neg_prop  \n",
       "ts                             \n",
       "2017-05-12 19:00:00       0.0  \n",
       "2017-05-12 19:30:00       0.0  \n",
       "2017-05-15 12:30:00       0.0  \n",
       "2017-05-15 13:00:00       0.0  \n",
       "2017-05-15 13:30:00       0.0  \n",
       "2017-05-15 14:00:00       0.0  \n",
       "2017-05-15 14:30:00       0.0  \n",
       "2017-05-15 15:00:00       0.0  \n",
       "2017-05-15 15:30:00       0.0  \n",
       "2017-05-15 16:00:00       0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "News_count describe:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    7152.000000\n",
       "mean        3.261326\n",
       "std         3.746225\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         2.000000\n",
       "75%         5.000000\n",
       "max        39.000000\n",
       "Name: news_count, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of bars with any news: 0.6648489932885906\n",
      "\n",
      "Saved model_df_news in namespace.\n"
     ]
    }
   ],
   "source": [
    "# === NEWS_MERGE: align aggregated news 'agg' with price model_df_safe and join ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if 'agg' not in globals():\n",
    "    raise RuntimeError(\"News aggregate variable 'agg' not found. Run the news-aggregation cell first.\")\n",
    "if 'model_df_safe' not in globals():\n",
    "    raise RuntimeError(\"model_df_safe not found. Run the REBUILD_MODEL_DF_SAFE cell first.\")\n",
    "\n",
    "news_agg_bins = agg.copy()\n",
    "price_df = model_df_safe.copy()\n",
    "\n",
    "print(\"Price index range:\", price_df.index.min(), \"→\", price_df.index.max())\n",
    "print(\"News bins range :\", news_agg_bins.index.min(), \"→\", news_agg_bins.index.max())\n",
    "\n",
    "# Reindex news to price index. Use forward-fill to carry last news state to subsequent price bars\n",
    "# (alternative strategies: nearest, backfill, zero-fill — here we forward-fill then fillna(0)).\n",
    "news_aligned = news_agg_bins.reindex(price_df.index, method='ffill')\n",
    "news_aligned = news_aligned.fillna(0)  # bars with no previous news become zeros\n",
    "\n",
    "# Join\n",
    "model_df_news = price_df.join(news_aligned, how='left').fillna(0)\n",
    "\n",
    "print(\"\\nJoined model_df_news shape:\", model_df_news.shape)\n",
    "print(\"News columns added:\", [c for c in news_aligned.columns])\n",
    "print(\"\\nSample head (news cols):\")\n",
    "display(model_df_news[['news_count','vader_mean','vader_sum','vader_std','pos_prop','neg_prop']].head(10))\n",
    "\n",
    "print(\"\\nNews_count describe:\")\n",
    "display(model_df_news['news_count'].describe())\n",
    "print(\"Proportion of bars with any news:\", (model_df_news['news_count']>0).mean())\n",
    "\n",
    "# Keep globally\n",
    "model_df_news = model_df_news.copy()\n",
    "print(\"\\nSaved model_df_news in namespace.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a7b1791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_df_news shape: (7152, 48)\n",
      "Columns (news subset): ['news_count', 'vader_mean', 'vader_sum', 'vader_std', 'pos_prop', 'neg_prop', 'vader_mean_roll_4', 'vader_mean_roll_12', 'news_count_roll_4', 'news_count_roll_12']\n",
      "\n",
      "Correlation of top news features with future_return_h (abs sorted):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>future_return_h</th>\n",
       "      <th>close_future_h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vader_std</th>\n",
       "      <td>0.026325</td>\n",
       "      <td>0.407263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_prop</th>\n",
       "      <td>0.015121</td>\n",
       "      <td>0.260249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vader_mean</th>\n",
       "      <td>0.011717</td>\n",
       "      <td>0.629371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news_count_roll_12</th>\n",
       "      <td>0.009526</td>\n",
       "      <td>0.612386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vader_sum</th>\n",
       "      <td>0.008044</td>\n",
       "      <td>0.481090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vader_mean_roll_12</th>\n",
       "      <td>0.005413</td>\n",
       "      <td>0.809177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news_count_roll_4</th>\n",
       "      <td>0.005395</td>\n",
       "      <td>0.589795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_prop</th>\n",
       "      <td>0.004824</td>\n",
       "      <td>0.779908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news_count</th>\n",
       "      <td>0.002988</td>\n",
       "      <td>0.524124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vader_mean_roll_4</th>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.759538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    future_return_h  close_future_h\n",
       "vader_std                  0.026325        0.407263\n",
       "neg_prop                   0.015121        0.260249\n",
       "vader_mean                 0.011717        0.629371\n",
       "news_count_roll_12         0.009526        0.612386\n",
       "vader_sum                  0.008044        0.481090\n",
       "vader_mean_roll_12         0.005413        0.809177\n",
       "news_count_roll_4          0.005395        0.589795\n",
       "pos_prop                   0.004824        0.779908\n",
       "news_count                 0.002988        0.524124\n",
       "vader_mean_roll_4          0.001204        0.759538"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Near-equality proportions (proportion of rows nearly equal to future features):\n",
      "   news_count: close_eq=0.0000, ret_eq=0.0018\n",
      "   vader_mean: close_eq=0.0000, ret_eq=0.0018\n",
      "   vader_sum: close_eq=0.0000, ret_eq=0.0018\n",
      "   vader_std: close_eq=0.0000, ret_eq=0.0022\n",
      "   pos_prop: close_eq=0.0000, ret_eq=0.0018\n",
      "   neg_prop: close_eq=0.0000, ret_eq=0.0034\n",
      "   vader_mean_roll_4: close_eq=0.0000, ret_eq=0.0018\n",
      "   vader_mean_roll_12: close_eq=0.0000, ret_eq=0.0018\n",
      "   news_count_roll_4: close_eq=0.0000, ret_eq=0.0018\n",
      "   news_count_roll_12: close_eq=0.0000, ret_eq=0.0018\n",
      "\n",
      "Price index min: 2017-05-12 19:00:00\n",
      "News bins min: 2017-12-07 20:00:00   News bins max: 2019-02-07 23:00:00\n",
      "\n",
      "News columns summary (describe):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>news_count</th>\n",
       "      <td>7152.0</td>\n",
       "      <td>3.261326</td>\n",
       "      <td>3.746225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vader_mean</th>\n",
       "      <td>7152.0</td>\n",
       "      <td>0.475690</td>\n",
       "      <td>0.459892</td>\n",
       "      <td>-0.997600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.568705</td>\n",
       "      <td>0.956725</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vader_sum</th>\n",
       "      <td>7152.0</td>\n",
       "      <td>2.311547</td>\n",
       "      <td>2.929740</td>\n",
       "      <td>-8.722600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.178650</td>\n",
       "      <td>3.904125</td>\n",
       "      <td>28.782400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vader_std</th>\n",
       "      <td>7152.0</td>\n",
       "      <td>0.222318</td>\n",
       "      <td>0.347233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.396081</td>\n",
       "      <td>1.414001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_prop</th>\n",
       "      <td>7152.0</td>\n",
       "      <td>0.575868</td>\n",
       "      <td>0.446341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_prop</th>\n",
       "      <td>7152.0</td>\n",
       "      <td>0.081893</td>\n",
       "      <td>0.182129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vader_mean_roll_4</th>\n",
       "      <td>7152.0</td>\n",
       "      <td>0.479143</td>\n",
       "      <td>0.382893</td>\n",
       "      <td>-0.231869</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.596329</td>\n",
       "      <td>0.831317</td>\n",
       "      <td>0.998962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vader_mean_roll_12</th>\n",
       "      <td>7152.0</td>\n",
       "      <td>0.483985</td>\n",
       "      <td>0.368222</td>\n",
       "      <td>-0.035880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.639314</td>\n",
       "      <td>0.800805</td>\n",
       "      <td>0.997839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news_count_roll_4</th>\n",
       "      <td>7152.0</td>\n",
       "      <td>12.955537</td>\n",
       "      <td>13.041031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news_count_roll_12</th>\n",
       "      <td>7152.0</td>\n",
       "      <td>36.007690</td>\n",
       "      <td>35.104874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>341.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count       mean        std       min  25%        50%  \\\n",
       "news_count          7152.0   3.261326   3.746225  0.000000  0.0   2.000000   \n",
       "vader_mean          7152.0   0.475690   0.459892 -0.997600  0.0   0.568705   \n",
       "vader_sum           7152.0   2.311547   2.929740 -8.722600  0.0   1.178650   \n",
       "vader_std           7152.0   0.222318   0.347233  0.000000  0.0   0.004035   \n",
       "pos_prop            7152.0   0.575868   0.446341  0.000000  0.0   0.800000   \n",
       "neg_prop            7152.0   0.081893   0.182129  0.000000  0.0   0.000000   \n",
       "vader_mean_roll_4   7152.0   0.479143   0.382893 -0.231869  0.0   0.596329   \n",
       "vader_mean_roll_12  7152.0   0.483985   0.368222 -0.035880  0.0   0.639314   \n",
       "news_count_roll_4   7152.0  12.955537  13.041031  0.000000  0.0  12.000000   \n",
       "news_count_roll_12  7152.0  36.007690  35.104874  0.000000  0.0  36.000000   \n",
       "\n",
       "                          75%         max  \n",
       "news_count           5.000000   39.000000  \n",
       "vader_mean           0.956725    1.000000  \n",
       "vader_sum            3.904125   28.782400  \n",
       "vader_std            0.396081    1.414001  \n",
       "pos_prop             1.000000    1.000000  \n",
       "neg_prop             0.000000    1.000000  \n",
       "vader_mean_roll_4    0.831317    0.998962  \n",
       "vader_mean_roll_12   0.800805    0.997839  \n",
       "news_count_roll_4   20.000000  120.000000  \n",
       "news_count_roll_12  56.000000  341.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample tail where news_count>0 (last 12 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_count</th>\n",
       "      <th>vader_mean</th>\n",
       "      <th>vader_sum</th>\n",
       "      <th>vader_mean_roll_4</th>\n",
       "      <th>future_return_h</th>\n",
       "      <th>close_future_h</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-02-01 14:00:00</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.942543</td>\n",
       "      <td>6.5978</td>\n",
       "      <td>0.660967</td>\n",
       "      <td>0.004044</td>\n",
       "      <td>1643.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 14:30:00</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.769722</td>\n",
       "      <td>6.9275</td>\n",
       "      <td>0.907756</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>1648.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 15:00:00</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.903833</td>\n",
       "      <td>5.4230</td>\n",
       "      <td>0.901880</td>\n",
       "      <td>-0.003232</td>\n",
       "      <td>1643.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 15:30:00</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.972386</td>\n",
       "      <td>6.8067</td>\n",
       "      <td>0.897121</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>1646.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 16:00:00</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.881160</td>\n",
       "      <td>4.4058</td>\n",
       "      <td>0.881775</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1648.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 16:30:00</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.337800</td>\n",
       "      <td>1.0134</td>\n",
       "      <td>0.773795</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>1649.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 17:00:00</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.972256</td>\n",
       "      <td>8.7503</td>\n",
       "      <td>0.790900</td>\n",
       "      <td>-0.001485</td>\n",
       "      <td>1646.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 17:30:00</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.920350</td>\n",
       "      <td>1.8407</td>\n",
       "      <td>0.777891</td>\n",
       "      <td>-0.002933</td>\n",
       "      <td>1642.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 18:00:00</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.688440</td>\n",
       "      <td>6.8844</td>\n",
       "      <td>0.729711</td>\n",
       "      <td>-0.005633</td>\n",
       "      <td>1632.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 18:30:00</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.728400</td>\n",
       "      <td>7.2840</td>\n",
       "      <td>0.827361</td>\n",
       "      <td>-0.002180</td>\n",
       "      <td>1629.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 19:00:00</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.775833</td>\n",
       "      <td>2.3275</td>\n",
       "      <td>0.778256</td>\n",
       "      <td>-0.001246</td>\n",
       "      <td>1627.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 19:30:00</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.904883</td>\n",
       "      <td>5.4293</td>\n",
       "      <td>0.774389</td>\n",
       "      <td>-0.001358</td>\n",
       "      <td>1624.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     news_count  vader_mean  vader_sum  vader_mean_roll_4  \\\n",
       "ts                                                                          \n",
       "2019-02-01 14:00:00         7.0    0.942543     6.5978           0.660967   \n",
       "2019-02-01 14:30:00         9.0    0.769722     6.9275           0.907756   \n",
       "2019-02-01 15:00:00         6.0    0.903833     5.4230           0.901880   \n",
       "2019-02-01 15:30:00         7.0    0.972386     6.8067           0.897121   \n",
       "2019-02-01 16:00:00         5.0    0.881160     4.4058           0.881775   \n",
       "2019-02-01 16:30:00         3.0    0.337800     1.0134           0.773795   \n",
       "2019-02-01 17:00:00         9.0    0.972256     8.7503           0.790900   \n",
       "2019-02-01 17:30:00         2.0    0.920350     1.8407           0.777891   \n",
       "2019-02-01 18:00:00        10.0    0.688440     6.8844           0.729711   \n",
       "2019-02-01 18:30:00        10.0    0.728400     7.2840           0.827361   \n",
       "2019-02-01 19:00:00         3.0    0.775833     2.3275           0.778256   \n",
       "2019-02-01 19:30:00         6.0    0.904883     5.4293           0.774389   \n",
       "\n",
       "                     future_return_h  close_future_h  \n",
       "ts                                                    \n",
       "2019-02-01 14:00:00         0.004044         1643.56  \n",
       "2019-02-01 14:30:00         0.003298         1648.98  \n",
       "2019-02-01 15:00:00        -0.003232         1643.65  \n",
       "2019-02-01 15:30:00         0.001813         1646.63  \n",
       "2019-02-01 16:00:00         0.001300         1648.77  \n",
       "2019-02-01 16:30:00         0.000309         1649.28  \n",
       "2019-02-01 17:00:00        -0.001485         1646.83  \n",
       "2019-02-01 17:30:00        -0.002933         1642.00  \n",
       "2019-02-01 18:00:00        -0.005633         1632.75  \n",
       "2019-02-01 18:30:00        -0.002180         1629.19  \n",
       "2019-02-01 19:00:00        -0.001246         1627.16  \n",
       "2019-02-01 19:30:00        -0.001358         1624.95  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "If nothing above looks suspicious (no near-equality with future, correlations small), proceed to training.\n"
     ]
    }
   ],
   "source": [
    "# === NEWS_LEAK_CHECK ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if 'model_df_news' not in globals():\n",
    "    raise RuntimeError(\"model_df_news not found. Run NEWS_MERGE cell first.\")\n",
    "\n",
    "dfn = model_df_news.copy()\n",
    "\n",
    "print(\"model_df_news shape:\", dfn.shape)\n",
    "print(\"Columns (news subset):\", [c for c in dfn.columns if 'vader' in c or 'news' in c or 'pos_prop' in c or 'neg_prop' in c])\n",
    "\n",
    "# 1) Simple correlation of news features with future_return_h and close_future_h\n",
    "num = dfn.select_dtypes(include=[np.number]).copy()\n",
    "corrs = num.corr()\n",
    "print(\"\\nCorrelation of top news features with future_return_h (abs sorted):\")\n",
    "news_feats = [c for c in num.columns if c.startswith('vader') or c.startswith('news_count') or c.endswith('_prop') or c.endswith('_roll_4') or c.endswith('_roll_12')]\n",
    "c_with_future = corrs.loc[news_feats, ['future_return_h','close_future_h']].abs().sort_values(by='future_return_h', ascending=False)\n",
    "display(c_with_future)\n",
    "\n",
    "# 2) Check near-equality: any news column nearly equal to close_future_h or future_return_h (>1% of rows)\n",
    "def near_prop(a,b,rtol=1e-6,atol=1e-9):\n",
    "    match = np.isclose(a,b,rtol=rtol,atol=atol)\n",
    "    return float(match.mean())\n",
    "\n",
    "print(\"\\nNear-equality proportions (proportion of rows nearly equal to future features):\")\n",
    "for feat in news_feats:\n",
    "    p_close = near_prop(num[feat].values, num['close_future_h'].values, rtol=1e-4, atol=1e-6)\n",
    "    p_ret   = near_prop(num[feat].values, num['future_return_h'].values, rtol=1e-4, atol=1e-9)\n",
    "    if p_close > 0.01 or p_ret > 0.01:\n",
    "        print(f\"  -> SUSPICIOUS: {feat}: close_eq={p_close:.4f}, ret_eq={p_ret:.4f}\")\n",
    "    else:\n",
    "        # show small ones optionally\n",
    "        print(f\"   {feat}: close_eq={p_close:.4f}, ret_eq={p_ret:.4f}\")\n",
    "\n",
    "# 3) Time alignment sanity: earliest news timestamp vs first price bar, fraction of news before first price row\n",
    "if hasattr(dfn.index, 'min'):\n",
    "    print(\"\\nPrice index min:\", dfn.index.min())\n",
    "if 'agg' in globals():\n",
    "    print(\"News bins min:\", agg.index.min(), \"  News bins max:\", agg.index.max())\n",
    "\n",
    "# 4) Quick distribution checks for news columns\n",
    "print(\"\\nNews columns summary (describe):\")\n",
    "display(dfn[news_feats].describe().T)\n",
    "\n",
    "# 5) Show last 12 rows where news_count>0 alongside future_return_h to eyeball alignment\n",
    "print(\"\\nSample tail where news_count>0 (last 12 rows):\")\n",
    "display(dfn[dfn['news_count']>0][['news_count','vader_mean','vader_sum','vader_mean_roll_4','future_return_h','close_future_h']].tail(12))\n",
    "\n",
    "print(\"\\nIf nothing above looks suspicious (no near-equality with future, correlations small), proceed to training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c63571a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count: 45\n",
      "Train samples: 7152  positives: 2916  negatives: 4236  scale_pos_weight: 1.453\n",
      "\n",
      "=== Fold 0  train_size=5718, test_size=1431  pos_train=2354 pos_test=560 ===\n",
      "LR_balanced:  F1=0.227  Prec=0.437  Rec=0.154  Acc=0.591  AUC=0.519\n",
      "Confusion matrix:\n",
      " [[760 111]\n",
      " [474  86]]\n",
      "SVM_balanced:  F1=0.004  Prec=1.000  Rec=0.002  Acc=0.609  AUC=0.470\n",
      "Confusion matrix:\n",
      " [[871   0]\n",
      " [559   1]]\n",
      "XGB_spw:  F1=0.374  Prec=0.428  Rec=0.332  Acc=0.565  AUC=0.541\n",
      "Confusion matrix:\n",
      " [[622 249]\n",
      " [374 186]]\n",
      "\n",
      "=== Fold 1  train_size=5715, test_size=1431  pos_train=2364 pos_test=549 ===\n",
      "LR_balanced:  F1=0.399  Prec=0.410  Rec=0.388  Acc=0.551  AUC=0.520\n",
      "Confusion matrix:\n",
      " [[576 306]\n",
      " [336 213]]\n",
      "SVM_balanced:  F1=0.102  Prec=0.421  Rec=0.058  Acc=0.608  AUC=0.494\n",
      "Confusion matrix:\n",
      " [[838  44]\n",
      " [517  32]]\n",
      "XGB_spw:  F1=0.442  Prec=0.414  Rec=0.474  Acc=0.541  AUC=0.546\n",
      "Confusion matrix:\n",
      " [[514 368]\n",
      " [289 260]]\n",
      "\n",
      "=== Fold 2  train_size=5716, test_size=1430  pos_train=2309 pos_test=607 ===\n",
      "LR_balanced:  F1=0.550  Prec=0.428  Rec=0.773  Acc=0.464  AUC=0.509\n",
      "Confusion matrix:\n",
      " [[195 628]\n",
      " [138 469]]\n",
      "SVM_balanced:  F1=0.116  Prec=0.414  Rec=0.068  Acc=0.564  AUC=0.512\n",
      "Confusion matrix:\n",
      " [[765  58]\n",
      " [566  41]]\n",
      "XGB_spw:  F1=0.487  Prec=0.442  Rec=0.542  Acc=0.515  AUC=0.529\n",
      "Confusion matrix:\n",
      " [[408 415]\n",
      " [278 329]]\n",
      "\n",
      "=== Fold 3  train_size=5716, test_size=1430  pos_train=2316 pos_test=599 ===\n",
      "LR_balanced:  F1=0.445  Prec=0.454  Rec=0.436  Acc=0.544  AUC=0.532\n",
      "Confusion matrix:\n",
      " [[517 314]\n",
      " [338 261]]\n",
      "SVM_balanced:  F1=0.075  Prec=0.379  Rec=0.042  Acc=0.570  AUC=0.526\n",
      "Confusion matrix:\n",
      " [[790  41]\n",
      " [574  25]]\n",
      "XGB_spw:  F1=0.395  Prec=0.429  Rec=0.366  Acc=0.530  AUC=0.509\n",
      "Confusion matrix:\n",
      " [[539 292]\n",
      " [380 219]]\n",
      "\n",
      "=== Fold 4  train_size=5719, test_size=1430  pos_train=2314 pos_test=601 ===\n",
      "LR_balanced:  F1=0.492  Prec=0.429  Rec=0.577  Acc=0.499  AUC=0.517\n",
      "Confusion matrix:\n",
      " [[367 462]\n",
      " [254 347]]\n",
      "SVM_balanced:  F1=0.523  Prec=0.434  Rec=0.659  Acc=0.495  AUC=0.528\n",
      "Confusion matrix:\n",
      " [[312 517]\n",
      " [205 396]]\n",
      "XGB_spw:  F1=0.440  Prec=0.441  Rec=0.439  Acc=0.531  AUC=0.514\n",
      "Confusion matrix:\n",
      " [[495 334]\n",
      " [337 264]]\n",
      "\n",
      "=== Summary across folds ===\n",
      "LR_balanced: mean F1 = 0.4227  std = 0.1099  (n_folds=5)\n",
      "SVM_balanced: mean F1 = 0.1641  std = 0.1837  (n_folds=5)\n",
      "XGB_spw: mean F1 = 0.4275  std = 0.0397  (n_folds=5)\n"
     ]
    }
   ],
   "source": [
    "# === NEWS_MODEL_TRAIN ===\n",
    "# Train & evaluate models on numeric + news using purged time-series CV with embargo.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "if 'model_df_news' not in globals():\n",
    "    raise RuntimeError(\"model_df_news not found. Run NEWS_MERGE first.\")\n",
    "\n",
    "# configuration\n",
    "EMBARGO = 3                # number of bars to embargo on each side of a test block\n",
    "N_SPLITS = 5\n",
    "USE_ROBUST_SCALER = True   # robust scaler helps when news_count distributions are skewed\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "dfm = model_df_news.copy()\n",
    "\n",
    "# Prepare features and target (drop leakage cols)\n",
    "drop_cols = ['close_future_h', 'future_return_h']  # not used as features\n",
    "X = dfm.drop(columns=drop_cols + ['target_thresholded'])\n",
    "y = dfm['target_thresholded']\n",
    "\n",
    "# keep only numeric features (safe)\n",
    "X = X.select_dtypes(include=[np.number]).copy()\n",
    "feature_names = X.columns.tolist()\n",
    "print(\"Feature count:\", len(feature_names))\n",
    "\n",
    "# helper: purged contiguous splits\n",
    "def purged_time_series_splits(n_splits, n_samples, embargo):\n",
    "    \"\"\"\n",
    "    Yields (train_idx, test_idx) where data is split into n_splits contiguous blocks.\n",
    "    For each fold, we remove `embargo` bars on either side of test block from the train indices.\n",
    "    \"\"\"\n",
    "    indices = np.arange(n_samples)\n",
    "    fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=int)\n",
    "    fold_sizes[: n_samples % n_splits] += 1\n",
    "    current = 0\n",
    "    blocks = []\n",
    "    for fs in fold_sizes:\n",
    "        block = indices[current: current+fs]\n",
    "        blocks.append(block)\n",
    "        current += fs\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        test_idx = blocks[i]\n",
    "        # build train blocks excluding embargo windows around test block\n",
    "        train_idx = np.hstack([b for j,b in enumerate(blocks) if j!=i])\n",
    "        # apply embargo: remove indices within `embargo` bars before/after test block\n",
    "        start_emb = test_idx[0] - embargo\n",
    "        end_emb = test_idx[-1] + embargo\n",
    "        if embargo>0:\n",
    "            train_idx = train_idx[(train_idx < start_emb) | (train_idx > end_emb)]\n",
    "        yield train_idx.astype(int), test_idx.astype(int)\n",
    "\n",
    "# Models to run (class-imbalance handled)\n",
    "models = {\n",
    "    'LR_balanced': Pipeline([\n",
    "        ('scaler', RobustScaler() if USE_ROBUST_SCALER else StandardScaler()),\n",
    "        ('clf', LogisticRegression(max_iter=2000, class_weight='balanced', random_state=RANDOM_STATE))\n",
    "    ]),\n",
    "    'SVM_balanced': Pipeline([\n",
    "        ('scaler', RobustScaler() if USE_ROBUST_SCALER else StandardScaler()),\n",
    "        ('clf', SVC(probability=True, class_weight='balanced', random_state=RANDOM_STATE))\n",
    "    ]),\n",
    "    'XGB_spw': Pipeline([\n",
    "        ('scaler', RobustScaler() if USE_ROBUST_SCALER else StandardScaler()),\n",
    "        ('clf', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE, n_jobs=-1))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# compute scale_pos_weight for XGB (train global ratio)\n",
    "pos = y.sum()\n",
    "neg = len(y) - pos\n",
    "scale_pos_weight = float(neg / (pos + 1e-9))\n",
    "print(f\"Train samples: {len(y)}  positives: {int(pos)}  negatives: {int(neg)}  scale_pos_weight: {scale_pos_weight:.3f}\")\n",
    "\n",
    "# storage\n",
    "fold_results = {name: [] for name in models.keys()}\n",
    "\n",
    "# run purged CV\n",
    "n_samples = len(X)\n",
    "splitter = purged_time_series_splits(N_SPLITS, n_samples, EMBARGO)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(splitter):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    print(f\"\\n=== Fold {fold}  train_size={len(train_idx)}, test_size={len(test_idx)}  pos_train={int(y_train.sum())} pos_test={int(y_test.sum())} ===\")\n",
    "\n",
    "    for name, pipe in models.items():\n",
    "        # set XGB scale_pos_weight if needed\n",
    "        if 'XGB' in name:\n",
    "            # set internal param\n",
    "            pipe.named_steps['clf'].set_params(scale_pos_weight= ( (len(y_train)-y_train.sum()) / (y_train.sum()+1e-9) ) )\n",
    "\n",
    "        # fit\n",
    "        pipe.fit(X_train, y_train)\n",
    "\n",
    "        # predict\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        try:\n",
    "            y_proba = pipe.predict_proba(X_test)[:,1]\n",
    "        except Exception:\n",
    "            y_proba = None\n",
    "\n",
    "        # metrics\n",
    "        acc  = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "        rec  = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1   = f1_score(y_test, y_pred, zero_division=0)\n",
    "        auc  = roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
    "\n",
    "        print(f\"{name}:  F1={f1:.3f}  Prec={prec:.3f}  Rec={rec:.3f}  Acc={acc:.3f}  AUC={'None' if auc is None else f'{auc:.3f}'}\")\n",
    "        # optional: confusion matrix\n",
    "        print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "        # save\n",
    "        fold_results[name].append({'fold':fold,'f1':f1,'prec':prec,'rec':rec,'acc':acc,'auc':auc})\n",
    "\n",
    "# Summarize\n",
    "print(\"\\n=== Summary across folds ===\")\n",
    "for name, entries in fold_results.items():\n",
    "    # remove any NaNs\n",
    "    vals = np.array([e['f1'] for e in entries], dtype=float)\n",
    "    vals = vals[~np.isnan(vals)]\n",
    "    if len(vals)==0:\n",
    "        print(f\"{name}: no valid folds\")\n",
    "        continue\n",
    "    print(f\"{name}: mean F1 = {vals.mean():.4f}  std = {vals.std():.4f}  (n_folds={len(vals)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "351162f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using articles DataFrame variable: news_df shape: (70731, 11)\n",
      "Number of articles to score: 70731\n",
      "Device: cpu | CUDA available: False | torch version: 2.6.0+cpu\n",
      "Attempting to load model with use_safetensors=True (preferred when available)...\n",
      "Loaded model with safetensors.\n",
      "Model id2label: {0: 'Neutral', 1: 'Positive', 2: 'Negative'}\n",
      "Using batch_size: 8 (reduce if you hit OOM)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FinBERT batches: 100%|██████████| 8842/8842 [5:20:02<00:00,  2.17s/it]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 70731 articles in 19202.6s (3.7 articles/s)\n",
      "Saved merged price+finbert as 'model_df_news_with_finbert' shape: (7152, 59)\n",
      "Saved per-article FinBERT scores as 'finbert_article_scores' rows: 70731\n",
      "\n",
      "Aggregated sample (tail):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_fin_count</th>\n",
       "      <th>finbert_pos_mean</th>\n",
       "      <th>finbert_neg_mean</th>\n",
       "      <th>finbert_neu_mean</th>\n",
       "      <th>finbert_compound_mean</th>\n",
       "      <th>finbert_compound_sum</th>\n",
       "      <th>finbert_compound_std</th>\n",
       "      <th>finbert_compound_roll_4</th>\n",
       "      <th>finbert_compound_roll_12</th>\n",
       "      <th>news_fin_count_roll_4</th>\n",
       "      <th>news_fin_count_roll_12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bin_ts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-02-07 19:30:00</th>\n",
       "      <td>3</td>\n",
       "      <td>0.024459</td>\n",
       "      <td>0.242214</td>\n",
       "      <td>0.733327</td>\n",
       "      <td>-0.217755</td>\n",
       "      <td>-0.653264</td>\n",
       "      <td>0.4019</td>\n",
       "      <td>0.130156</td>\n",
       "      <td>0.166452</td>\n",
       "      <td>9.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-07 20:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.073334</td>\n",
       "      <td>0.926581</td>\n",
       "      <td>-0.073249</td>\n",
       "      <td>-0.073249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.033218</td>\n",
       "      <td>0.174137</td>\n",
       "      <td>9.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-07 20:30:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.072751</td>\n",
       "      <td>0.151897</td>\n",
       "      <td>8.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-07 21:00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>0.994738</td>\n",
       "      <td>-0.004614</td>\n",
       "      <td>-0.009229</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.073905</td>\n",
       "      <td>0.151568</td>\n",
       "      <td>7.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-07 22:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.019463</td>\n",
       "      <td>0.138806</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-07 23:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.034831</td>\n",
       "      <td>0.965021</td>\n",
       "      <td>-0.034683</td>\n",
       "      <td>-0.034683</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.009822</td>\n",
       "      <td>0.056228</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     news_fin_count  finbert_pos_mean  finbert_neg_mean  \\\n",
       "bin_ts                                                                    \n",
       "2019-02-07 19:30:00               3          0.024459          0.242214   \n",
       "2019-02-07 20:00:00               1          0.000085          0.073334   \n",
       "2019-02-07 20:30:00               1          0.000012          0.000014   \n",
       "2019-02-07 21:00:00               2          0.000324          0.004938   \n",
       "2019-02-07 22:00:00               1          0.000015          0.000002   \n",
       "2019-02-07 23:00:00               1          0.000148          0.034831   \n",
       "\n",
       "                     finbert_neu_mean  finbert_compound_mean  \\\n",
       "bin_ts                                                         \n",
       "2019-02-07 19:30:00          0.733327              -0.217755   \n",
       "2019-02-07 20:00:00          0.926581              -0.073249   \n",
       "2019-02-07 20:30:00          0.999974              -0.000002   \n",
       "2019-02-07 21:00:00          0.994738              -0.004614   \n",
       "2019-02-07 22:00:00          0.999983               0.000013   \n",
       "2019-02-07 23:00:00          0.965021              -0.034683   \n",
       "\n",
       "                     finbert_compound_sum  finbert_compound_std  \\\n",
       "bin_ts                                                            \n",
       "2019-02-07 19:30:00             -0.653264                0.4019   \n",
       "2019-02-07 20:00:00             -0.073249                   NaN   \n",
       "2019-02-07 20:30:00             -0.000002                   NaN   \n",
       "2019-02-07 21:00:00             -0.009229                0.0000   \n",
       "2019-02-07 22:00:00              0.000013                   NaN   \n",
       "2019-02-07 23:00:00             -0.034683                   NaN   \n",
       "\n",
       "                     finbert_compound_roll_4  finbert_compound_roll_12  \\\n",
       "bin_ts                                                                   \n",
       "2019-02-07 19:30:00                 0.130156                  0.166452   \n",
       "2019-02-07 20:00:00                 0.033218                  0.174137   \n",
       "2019-02-07 20:30:00                -0.072751                  0.151897   \n",
       "2019-02-07 21:00:00                -0.073905                  0.151568   \n",
       "2019-02-07 22:00:00                -0.019463                  0.138806   \n",
       "2019-02-07 23:00:00                -0.009822                  0.056228   \n",
       "\n",
       "                     news_fin_count_roll_4  news_fin_count_roll_12  \n",
       "bin_ts                                                              \n",
       "2019-02-07 19:30:00                    9.0                    42.0  \n",
       "2019-02-07 20:00:00                    9.0                    41.0  \n",
       "2019-02-07 20:30:00                    8.0                    34.0  \n",
       "2019-02-07 21:00:00                    7.0                    32.0  \n",
       "2019-02-07 22:00:00                    5.0                    25.0  \n",
       "2019-02-07 23:00:00                    5.0                    25.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-article sample (tail):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finbert_compound</th>\n",
       "      <th>finbert_neutral</th>\n",
       "      <th>finbert_positive</th>\n",
       "      <th>finbert_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70725</th>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.999665</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>1.833660e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70726</th>\n",
       "      <td>0.990019</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>0.992912</td>\n",
       "      <td>2.893622e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70727</th>\n",
       "      <td>-0.010311</td>\n",
       "      <td>0.986855</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>1.172769e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70728</th>\n",
       "      <td>0.999933</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.999933</td>\n",
       "      <td>1.602052e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70729</th>\n",
       "      <td>0.682281</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.840664</td>\n",
       "      <td>1.583828e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70730</th>\n",
       "      <td>-0.073249</td>\n",
       "      <td>0.926581</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>7.333373e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       finbert_compound  finbert_neutral  finbert_positive  finbert_negative\n",
       "70725          0.000298         0.999665          0.000317      1.833660e-05\n",
       "70726          0.990019         0.004194          0.992912      2.893622e-03\n",
       "70727         -0.010311         0.986855          0.001417      1.172769e-02\n",
       "70728          0.999933         0.000066          0.999933      1.602052e-07\n",
       "70729          0.682281         0.000953          0.840664      1.583828e-01\n",
       "70730         -0.073249         0.926581          0.000085      7.333373e-02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === FINBERT (robust) — uses GPU if available, otherwise CPU ===\n",
    "# Paste & run in your notebook. Assumes your news DataFrame is present (news_df / news / articles).\n",
    "import sys, math, time, importlib, subprocess\n",
    "import pandas as pd, numpy as np, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def maybe_install(pkg):\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except Exception:\n",
    "        print(f\"Installing missing package: {pkg}\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "# Try ensure transformers/tqdm present (torch likely already installed by you)\n",
    "for pkg in (\"transformers\",\"tqdm\"):\n",
    "    maybe_install(pkg)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 0) Find articles DataFrame\n",
    "_possible_names = [\"news_df\", \"news\", \"articles\", \"articles_df\", \"parsed_articles\", \"df_articles\"]\n",
    "articles_df = None\n",
    "for nm in _possible_names:\n",
    "    if nm in globals() and isinstance(globals()[nm], pd.DataFrame):\n",
    "        articles_df = globals()[nm]\n",
    "        print(\"Using articles DataFrame variable:\", nm, \"shape:\", articles_df.shape)\n",
    "        break\n",
    "if articles_df is None:\n",
    "    raise RuntimeError(\"Could not find your articles DataFrame in variables: \" + \", \".join(_possible_names))\n",
    "\n",
    "# 1) Build texts (title + text or fallback)\n",
    "text_cols_try = [\"title\",\"text\",\"summary\",\"description\",\"content\"]\n",
    "texts = []\n",
    "used_idxs = []\n",
    "for idx, row in articles_df.iterrows():\n",
    "    parts = []\n",
    "    for c in text_cols_try:\n",
    "        if c in articles_df.columns and pd.notna(row.get(c, None)):\n",
    "            parts.append(str(row.get(c, \"\")))\n",
    "    txt = \" \".join(parts).strip()\n",
    "    if not txt:\n",
    "        txt = str(row.get(\"title\", \"\") or row.get(\"url\", \"\") or \"\")\n",
    "    texts.append(txt)\n",
    "    used_idxs.append(idx)\n",
    "n = len(texts)\n",
    "print(\"Number of articles to score:\", n)\n",
    "\n",
    "# 2) Device selection\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(\"Device:\", device, \"| CUDA available:\", use_cuda, \"| torch version:\", torch.__version__)\n",
    "\n",
    "# 3) Load FinBERT model (try safetensors first)\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "model = None\n",
    "# Try load with safetensors to avoid older-torch torch.load requirement\n",
    "try:\n",
    "    print(\"Attempting to load model with use_safetensors=True (preferred when available)...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, use_safetensors=True)\n",
    "    print(\"Loaded model with safetensors.\")\n",
    "except Exception as e_s:\n",
    "    print(\"safetensors load failed:\", repr(e_s))\n",
    "    print(\"Attempting normal from_pretrained (this may require torch>=2.6).\")\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        print(\"Loaded model with normal weights.\")\n",
    "    except Exception as e2:\n",
    "        # Surface friendly error and re-raise\n",
    "        msg = (\n",
    "            \"Failed to load model. If the error mentions 'torch.load' or 'upgrade torch >=2.6',\\n\"\n",
    "            \"please upgrade PyTorch to >=2.6 (conda install pytorch pytorch-cuda -c pytorch -c nvidia) \"\n",
    "            \"or ensure the model has safetensors weights. Original error:\\n\" + repr(e2)\n",
    "        )\n",
    "        raise RuntimeError(msg) from e2\n",
    "\n",
    "# send to device\n",
    "model.to(device)\n",
    "model.eval()\n",
    "id2label = getattr(model.config, \"id2label\", None)\n",
    "print(\"Model id2label:\", id2label)\n",
    "\n",
    "# 4) Batch inference — adapt batch size to device\n",
    "batch_size = 64 if use_cuda else 8   # smaller on CPU to avoid memory/time issues\n",
    "print(\"Using batch_size:\", batch_size, \"(reduce if you hit OOM)\")\n",
    "\n",
    "all_probs = []\n",
    "start = time.time()\n",
    "n_batches = math.ceil(n / batch_size)\n",
    "\n",
    "for i in tqdm(range(n_batches), desc=\"FinBERT batches\"):\n",
    "    batch_texts = texts[i*batch_size:(i+1)*batch_size]\n",
    "    enc = tokenizer(batch_texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    # move tensors to device\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model(**enc)\n",
    "        logits = out.logits\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()  # move to CPU for stacking\n",
    "        all_probs.append(probs)\n",
    "\n",
    "# stack\n",
    "if len(all_probs):\n",
    "    all_probs = np.vstack(all_probs)\n",
    "else:\n",
    "    all_probs = np.zeros((0, model.config.num_labels))\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"Done: {n} articles in {elapsed:.1f}s ({(n/elapsed) if elapsed>0 else 0:.1f} articles/s)\")\n",
    "\n",
    "# 5) Build per-article DataFrame\n",
    "if id2label:\n",
    "    id2label_norm = {int(k): v for k, v in id2label.items()}\n",
    "    labels = [id2label_norm[i] for i in sorted(id2label_norm.keys())]\n",
    "else:\n",
    "    # fallback labeling order (common for FinBERT)\n",
    "    labels = [\"Neutral\",\"Positive\",\"Negative\"]\n",
    "\n",
    "cols = [f\"finbert_{lab.lower()}\" for lab in labels]\n",
    "prob_df = pd.DataFrame(all_probs, index=used_idxs, columns=cols)\n",
    "\n",
    "# compound: positive - negative if available else max-min\n",
    "if \"finbert_positive\" in prob_df.columns and \"finbert_negative\" in prob_df.columns:\n",
    "    prob_df[\"finbert_compound\"] = prob_df[\"finbert_positive\"] - prob_df[\"finbert_negative\"]\n",
    "else:\n",
    "    prob_df[\"finbert_compound\"] = prob_df.max(axis=1) - prob_df.min(axis=1)\n",
    "\n",
    "# attach published timestamp (if present)\n",
    "if \"published\" in articles_df.columns:\n",
    "    prob_df[\"published\"] = pd.to_datetime(articles_df[\"published\"], utc=True).loc[prob_df.index]\n",
    "else:\n",
    "    for col in (\"date\",\"ts\",\"timestamp\"):\n",
    "        if col in articles_df.columns:\n",
    "            prob_df[\"published\"] = pd.to_datetime(articles_df[col], utc=True).loc[prob_df.index]\n",
    "            break\n",
    "    else:\n",
    "        prob_df[\"published\"] = pd.NaT\n",
    "        print(\"Warning: no published/date column found; published will be NaT in the results.\")\n",
    "\n",
    "# 6) Aggregate into 30-minute bins (adjust '30T' if your bars use other frequency)\n",
    "prob_df[\"bin_ts\"] = prob_df[\"published\"].dt.tz_convert(None).dt.floor(\"30T\")\n",
    "agg = prob_df.groupby(\"bin_ts\").agg(\n",
    "    news_fin_count = (\"finbert_compound\", \"size\"),\n",
    "    finbert_pos_mean = (\"finbert_positive\", \"mean\") if \"finbert_positive\" in prob_df.columns else (\"finbert_compound\",\"mean\"),\n",
    "    finbert_neg_mean = (\"finbert_negative\", \"mean\") if \"finbert_negative\" in prob_df.columns else (\"finbert_compound\",\"mean\"),\n",
    "    finbert_neu_mean = (\"finbert_neutral\", \"mean\") if \"finbert_neutral\" in prob_df.columns else (\"finbert_compound\",\"mean\"),\n",
    "    finbert_compound_mean = (\"finbert_compound\", \"mean\"),\n",
    "    finbert_compound_sum = (\"finbert_compound\", \"sum\"),\n",
    "    finbert_compound_std = (\"finbert_compound\", \"std\")\n",
    ")\n",
    "agg[\"finbert_compound_roll_4\"] = agg[\"finbert_compound_mean\"].rolling(4, min_periods=1).mean()\n",
    "agg[\"finbert_compound_roll_12\"] = agg[\"finbert_compound_mean\"].rolling(12, min_periods=1).mean()\n",
    "agg[\"news_fin_count_roll_4\"] = agg[\"news_fin_count\"].rolling(4, min_periods=1).sum()\n",
    "agg[\"news_fin_count_roll_12\"] = agg[\"news_fin_count\"].rolling(12, min_periods=1).sum()\n",
    "\n",
    "# 7) Merge into price dataset if available (model_df_news preferred)\n",
    "if \"model_df_news\" in globals():\n",
    "    target_df = globals()[\"model_df_news\"].copy()\n",
    "    idx = pd.to_datetime(target_df.index).tz_localize(None)\n",
    "    agg_reindexed = agg.reindex(idx, fill_value=0)\n",
    "    merged = target_df.join(agg_reindexed, how=\"left\").fillna(0)\n",
    "    globals()[\"model_df_news_with_finbert\"] = merged\n",
    "    print(\"Saved merged price+finbert as 'model_df_news_with_finbert' shape:\", merged.shape)\n",
    "elif \"model_df_safe\" in globals():\n",
    "    target_df = globals()[\"model_df_safe\"].copy()\n",
    "    idx = pd.to_datetime(target_df.index).tz_localize(None)\n",
    "    agg_reindexed = agg.reindex(idx, fill_value=0)\n",
    "    merged = target_df.join(agg_reindexed, how=\"left\").fillna(0)\n",
    "    globals()[\"model_df_safe_with_finbert\"] = merged\n",
    "    print(\"Saved merged price+finbert as 'model_df_safe_with_finbert' shape:\", merged.shape)\n",
    "else:\n",
    "    globals()[\"news_finbert_agg\"] = agg\n",
    "    print(\"Saved finbert aggregation only as 'news_finbert_agg' rows:\", len(agg))\n",
    "\n",
    "# 8) Save per-article scores\n",
    "globals()[\"finbert_article_scores\"] = prob_df\n",
    "print(\"Saved per-article FinBERT scores as 'finbert_article_scores' rows:\", len(prob_df))\n",
    "\n",
    "# show tiny samples\n",
    "print(\"\\nAggregated sample (tail):\")\n",
    "display(agg.tail(6))\n",
    "print(\"\\nPer-article sample (tail):\")\n",
    "display(prob_df[[\"finbert_compound\"] + [c for c in prob_df.columns if c.startswith(\"finbert_\")][:3]].tail(6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e83f177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'news_finbert_agg' found in globals(), skipping parquet save for that.\n",
      "Saved files summary:\n",
      " - finbert_article_scores -> saved_outputs\\finbert_article_scores.pkl  (2.7MB)\n",
      " - model_df_news_with_finbert -> saved_outputs\\model_df_news_with_finbert.parquet  (2.2MB)\n",
      "\n",
      "Manifest written to saved_outputs\\save_manifest.txt\n",
      "You can move the 'saved_outputs' folder to another machine and reload with Cell B below.\n"
     ]
    }
   ],
   "source": [
    "# === SAVE RESULTS: run this cell to persist FinBERT outputs ===\n",
    "import os, sys, pathlib, pickle\n",
    "import pandas as pd\n",
    "\n",
    "out_dir = pathlib.Path(\"saved_outputs\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "saved = []\n",
    "\n",
    "# helper to print size nicely\n",
    "def sizeof(path):\n",
    "    try:\n",
    "        s = path.stat().st_size\n",
    "        for unit in [\"B\",\"KB\",\"MB\",\"GB\"]:\n",
    "            if s < 1024.0:\n",
    "                return f\"{s:.1f}{unit}\"\n",
    "            s /= 1024.0\n",
    "        return f\"{s:.1f}TB\"\n",
    "    except Exception:\n",
    "        return \"n/a\"\n",
    "\n",
    "# 1) per-article scores (prob_df)\n",
    "if \"finbert_article_scores\" in globals():\n",
    "    p = out_dir / \"finbert_article_scores.pkl\"\n",
    "    fin = globals()[\"finbert_article_scores\"]\n",
    "    fin.to_pickle(p, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    saved.append((\"finbert_article_scores\", p))\n",
    "else:\n",
    "    print(\"finbert_article_scores not found in globals(), skipping.\")\n",
    "\n",
    "# 2) aggregated bins\n",
    "if \"news_finbert_agg\" in globals():\n",
    "    p = out_dir / \"news_finbert_agg.parquet\"\n",
    "    globals()[\"news_finbert_agg\"].to_parquet(p, index=True)\n",
    "    saved.append((\"news_finbert_agg\", p))\n",
    "elif \"news_finbert_agg\" not in globals():\n",
    "    # maybe you saved as 'news_finbert_agg' or as 'news_finbert_agg' inside another name\n",
    "    print(\"No 'news_finbert_agg' found in globals(), skipping parquet save for that.\")\n",
    "\n",
    "# 3) merged model df (price + news)\n",
    "for cand in (\"model_df_news_with_finbert\",\"model_df_safe_with_finbert\",\"model_df_news\"):\n",
    "    if cand in globals():\n",
    "        p = out_dir / f\"{cand}.parquet\"\n",
    "        # convert index to datetimelike if needed\n",
    "        df = globals()[cand].copy()\n",
    "        try:\n",
    "            df.to_parquet(p, index=True)\n",
    "            saved.append((cand, p))\n",
    "        except Exception as e:\n",
    "            # fallback to pickle\n",
    "            p2 = out_dir / f\"{cand}.pkl\"\n",
    "            df.to_pickle(p2, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            saved.append((cand, p2))\n",
    "        break\n",
    "\n",
    "# 4) small metadata summary\n",
    "meta_p = out_dir / \"save_manifest.txt\"\n",
    "with open(meta_p, \"w\", encoding=\"utf8\") as f:\n",
    "    f.write(\"Saved files:\\n\")\n",
    "    for name, path in saved:\n",
    "        f.write(f\"{name}\\t{path}\\t{sizeof(path)}\\n\")\n",
    "\n",
    "print(\"Saved files summary:\")\n",
    "for name, path in saved:\n",
    "    print(f\" - {name} -> {path}  ({sizeof(path)})\")\n",
    "\n",
    "print(\"\\nManifest written to\", meta_p)\n",
    "print(\"You can move the 'saved_outputs' folder to another machine and reload with Cell B below.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89504180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded finbert_article_scores: (70731, 6)\n",
      "news_finbert_agg.parquet not found\n",
      "Loaded model_df_news_with_finbert (7152, 59)\n",
      "\n",
      "Done. Variables available in your notebook: finbert_article_scores, news_finbert_agg (if present), plus any merged dataset.\n"
     ]
    }
   ],
   "source": [
    "# === LOAD RESULTS: run this to restore saved outputs ===\n",
    "import pathlib, pickle\n",
    "import pandas as pd\n",
    "\n",
    "out_dir = pathlib.Path(\"saved_outputs\")\n",
    "if not out_dir.exists():\n",
    "    raise RuntimeError(f\"saved_outputs folder not found at {out_dir.resolve()}\")\n",
    "\n",
    "# try load per-article\n",
    "p1 = out_dir / \"finbert_article_scores.pkl\"\n",
    "if p1.exists():\n",
    "    finbert_article_scores = pd.read_pickle(p1)\n",
    "    globals()[\"finbert_article_scores\"] = finbert_article_scores\n",
    "    print(\"Loaded finbert_article_scores:\", finbert_article_scores.shape)\n",
    "else:\n",
    "    print(\"finbert_article_scores.pkl not found\")\n",
    "\n",
    "# load aggregates\n",
    "p2 = out_dir / \"news_finbert_agg.parquet\"\n",
    "if p2.exists():\n",
    "    news_finbert_agg = pd.read_parquet(p2)\n",
    "    globals()[\"news_finbert_agg\"] = news_finbert_agg\n",
    "    print(\"Loaded news_finbert_agg:\", news_finbert_agg.shape)\n",
    "else:\n",
    "    print(\"news_finbert_agg.parquet not found\")\n",
    "\n",
    "# load merged model df (try parquet then pickle)\n",
    "for cand in (\"model_df_news_with_finbert.parquet\",\"model_df_safe_with_finbert.parquet\",\"model_df_news.parquet\"):\n",
    "    p = out_dir / cand\n",
    "    if p.exists():\n",
    "        df = pd.read_parquet(p)\n",
    "        varname = p.stem\n",
    "        globals()[varname] = df\n",
    "        print(\"Loaded\", varname, df.shape)\n",
    "        break\n",
    "else:\n",
    "    # try pickle fallback\n",
    "    for cand in (\"model_df_news_with_finbert.pkl\",\"model_df_safe_with_finbert.pkl\",\"model_df_news.pkl\"):\n",
    "        p = out_dir / cand\n",
    "        if p.exists():\n",
    "            df = pd.read_pickle(p)\n",
    "            globals()[p.stem] = df\n",
    "            print(\"Loaded\", p.stem, df.shape)\n",
    "            break\n",
    "    else:\n",
    "        print(\"No merged price+finbert dataset found in saved_outputs.\")\n",
    "\n",
    "print(\"\\nDone. Variables available in your notebook: finbert_article_scores, news_finbert_agg (if present), plus any merged dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34c750e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_outputs exists: True\n",
      "\n",
      "saved_outputs\\finbert_article_scores.pkl - 2.70 MB\n",
      "saved_outputs\\model_df_news_with_finbert.parquet - 2.25 MB\n",
      "saved_outputs\\save_manifest.txt - 0.2 KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell: verify saved files\n",
    "import pathlib, os\n",
    "out_dir = pathlib.Path(\"saved_outputs\")\n",
    "print(\"saved_outputs exists:\", out_dir.exists())\n",
    "print()\n",
    "\n",
    "if out_dir.exists():\n",
    "    for p in sorted(out_dir.rglob(\"*\")):\n",
    "        if p.is_file():\n",
    "            s = p.stat().st_size\n",
    "            human = f\"{s/1024**2:.2f} MB\" if s>1024**2 else f\"{s/1024:.1f} KB\"\n",
    "            print(p.relative_to(out_dir.parent), \"-\", human)\n",
    "else:\n",
    "    print(\"No saved_outputs folder found. Did the save cell run successfully?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdf95b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model_df_news_with_finbert\n",
      "Total numeric columns available: 56\n",
      "Numeric-only feature count: 40\n",
      "FinBERT/news feature count: 16\n",
      "Train size: 5721 Test size: 1431\n",
      "\n",
      "=== Baseline: numeric-only ===\n",
      "\n",
      "Numeric-only - LogisticRegression (balanced)\n",
      "{'acc': 0.48846960167714887, 'prec': 0.41854636591478694, 'rec': 0.5548172757475083, 'f1': 0.47714285714285715, 'auc': 0.5124714962990273}\n",
      "\n",
      "Numeric-only - XGBoost\n",
      "{'acc': 0.5345911949685535, 'prec': 0.43073593073593075, 'rec': 0.33056478405315615, 'f1': 0.37406015037593987, 'auc': 0.5027491794540915}\n",
      "\n",
      "=== Combined: numeric + FinBERT/news ===\n",
      "\n",
      "Numeric+FinBERT - LogisticRegression (balanced)\n",
      "{'acc': 0.5024458420684835, 'prec': 0.4260752688172043, 'rec': 0.526578073089701, 'f1': 0.4710252600297177, 'auc': 0.515214664427782}\n",
      "\n",
      "Numeric+FinBERT - XGBoost\n",
      "{'acc': 0.5129280223619846, 'prec': 0.4, 'rec': 0.31561461794019935, 'f1': 0.3528319405756732, 'auc': 0.514469260086002}\n"
     ]
    }
   ],
   "source": [
    "# Cell: quick numeric vs numeric+finbert comparison (time-ordered split)\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# choose dataset\n",
    "if 'model_df_news_with_finbert' in globals():\n",
    "    df = globals()['model_df_news_with_finbert'].copy()\n",
    "    print(\"Using model_df_news_with_finbert\")\n",
    "elif 'model_df_news' in globals():\n",
    "    df = globals()['model_df_news'].copy()\n",
    "    print(\"Using model_df_news\")\n",
    "else:\n",
    "    raise RuntimeError(\"No merged dataset found in globals (model_df_news_with_finbert or model_df_news).\")\n",
    "\n",
    "# ensure ts index and sorted\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df = df.sort_index()\n",
    "\n",
    "# target: use previously built column if present, else build same as before:\n",
    "if 'target_thresholded' in df.columns:\n",
    "    y = df['target_thresholded'].astype(int).copy()\n",
    "elif 'future_return_h' in df.columns:\n",
    "    y = (df['future_return_h'] > 0).astype(int)\n",
    "else:\n",
    "    raise RuntimeError(\"No target column found (target_thresholded or future_return_h).\")\n",
    "\n",
    "# select numeric features (drop leakage / future columns)\n",
    "drop_like = ['future','close_future','close_next','target','target_thresholded']\n",
    "cols = [c for c in df.columns if (np.issubdtype(df[c].dtype, np.number) and not any(dl in c for dl in drop_like))]\n",
    "print(\"Total numeric columns available:\", len(cols))\n",
    "\n",
    "# pick baseline numeric features (all numeric except FinBERT ones)\n",
    "fin_cols = [c for c in cols if c.startswith(\"finbert_\") or c.startswith(\"news_fin_\") or c.startswith(\"vader_\")]\n",
    "num_cols = [c for c in cols if c not in fin_cols]\n",
    "\n",
    "print(\"Numeric-only feature count:\", len(num_cols))\n",
    "print(\"FinBERT/news feature count:\", len(fin_cols))\n",
    "\n",
    "# trim to rows without NaN in features/target\n",
    "use_df = df[num_cols + fin_cols].copy()\n",
    "mask = y.notna() & use_df.notna().all(axis=1)\n",
    "use_df = use_df.loc[mask]\n",
    "y = y.loc[mask]\n",
    "\n",
    "# time-split: last 20% as test\n",
    "n = len(use_df)\n",
    "split = int(n * 0.8)\n",
    "X_train = use_df.iloc[:split]\n",
    "X_test  = use_df.iloc[split:]\n",
    "y_train = y.iloc[:split]\n",
    "y_test  = y.iloc[split:]\n",
    "\n",
    "print(\"Train size:\", len(X_train), \"Test size:\", len(X_test))\n",
    "\n",
    "def train_and_eval(X_tr, X_te, y_tr, y_te, desc=\"Model\"):\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_s = scaler.fit_transform(X_tr)\n",
    "    X_te_s = scaler.transform(X_te)\n",
    "\n",
    "    # LR with balanced class weight\n",
    "    lr = LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42)\n",
    "    lr.fit(X_tr_s, y_tr)\n",
    "    p = lr.predict(X_te_s)\n",
    "    try:\n",
    "        proba = lr.predict_proba(X_te_s)[:,1]\n",
    "    except Exception:\n",
    "        proba = None\n",
    "    metrics = {\n",
    "        \"acc\": accuracy_score(y_te, p),\n",
    "        \"prec\": precision_score(y_te, p, zero_division=0),\n",
    "        \"rec\": recall_score(y_te, p, zero_division=0),\n",
    "        \"f1\": f1_score(y_te, p, zero_division=0),\n",
    "        \"auc\": roc_auc_score(y_te, proba) if proba is not None else None\n",
    "    }\n",
    "    print(f\"\\n{desc} - LogisticRegression (balanced)\")\n",
    "    print(metrics)\n",
    "\n",
    "    # XGBoost (scale_pos_weight)\n",
    "    spw = (y_tr==0).sum() / max(1, (y_tr==1).sum())\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, scale_pos_weight=spw)\n",
    "    xgb.fit(X_tr_s, y_tr)\n",
    "    p2 = xgb.predict(X_te_s)\n",
    "    proba2 = xgb.predict_proba(X_te_s)[:,1] if hasattr(xgb, \"predict_proba\") else None\n",
    "    metrics2 = {\n",
    "        \"acc\": accuracy_score(y_te, p2),\n",
    "        \"prec\": precision_score(y_te, p2, zero_division=0),\n",
    "        \"rec\": recall_score(y_te, p2, zero_division=0),\n",
    "        \"f1\": f1_score(y_te, p2, zero_division=0),\n",
    "        \"auc\": roc_auc_score(y_te, proba2) if proba2 is not None else None\n",
    "    }\n",
    "    print(f\"\\n{desc} - XGBoost\")\n",
    "    print(metrics2)\n",
    "    return metrics, metrics2\n",
    "\n",
    "# 1) numeric-only\n",
    "if len(num_cols) == 0:\n",
    "    print(\"No numeric columns found for baseline — aborting numeric-only test.\")\n",
    "else:\n",
    "    print(\"\\n=== Baseline: numeric-only ===\")\n",
    "    _ = train_and_eval(X_train[num_cols], X_test[num_cols], y_train, y_test, desc=\"Numeric-only\")\n",
    "\n",
    "# 2) numeric + FinBERT/news\n",
    "if len(fin_cols) == 0:\n",
    "    print(\"\\nNo FinBERT/news columns found — skipping combined test.\")\n",
    "else:\n",
    "    print(\"\\n=== Combined: numeric + FinBERT/news ===\")\n",
    "    _ = train_and_eval(X_train[num_cols + fin_cols], X_test[num_cols + fin_cols], y_train, y_test, desc=\"Numeric+FinBERT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59ed1e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 56 numeric features for tuning (example: ['open', 'high', 'low', 'close', 'volume', 'return_1', 'log_return_1', 'ema_8'])\n",
      "Train size: 5721  Test size: 1431\n",
      "Starting purged-CV tuning: n_splits= 5 embargo= 3\n",
      "\n",
      "Tuning LogisticRegression (balanced) ...\n",
      "Params: {'clf__penalty': 'l2', 'clf__C': 0.0005, 'clf__solver': 'saga'} -> mean_f1=0.3510 std=0.2108\n",
      "Params: {'clf__penalty': 'l2', 'clf__C': 0.01, 'clf__solver': 'saga'} -> mean_f1=0.3985 std=0.1396\n",
      "Params: {'clf__penalty': 'l2', 'clf__C': 0.1, 'clf__solver': 'saga'} -> mean_f1=0.4120 std=0.1266\n",
      "Params: {'clf__penalty': 'l2', 'clf__C': 1.0, 'clf__solver': 'saga'} -> mean_f1=0.4140 std=0.1265\n",
      "Params: {'clf__penalty': 'l2', 'clf__C': 10.0, 'clf__solver': 'saga'} -> mean_f1=0.4145 std=0.1266\n",
      "\n",
      "Tuning SVM (RBF, class_weight=balanced) ...\n",
      "Params: {'clf__C': 0.1, 'clf__gamma': 0.01} -> mean_f1=0.4224 std=0.1744\n",
      "Params: {'clf__C': 1.0, 'clf__gamma': 0.01} -> mean_f1=0.3427 std=0.0660\n",
      "Params: {'clf__C': 5.0, 'clf__gamma': 0.01} -> mean_f1=0.3182 std=0.0661\n",
      "Params: {'clf__C': 1.0, 'clf__gamma': 'scale'} -> mean_f1=0.3703 std=0.1019\n",
      "\n",
      "Tuning XGBoost ...\n",
      "scale_pos_weight (train): 1.472\n",
      "Params: {'clf__n_estimators': 200, 'clf__max_depth': 3, 'clf__learning_rate': 0.05, 'clf__subsample': 0.7, 'clf__colsample_bytree': 0.7, 'clf__scale_pos_weight': 1.472342264477096} -> mean_f1=0.4037 std=0.1180\n",
      "Params: {'clf__n_estimators': 400, 'clf__max_depth': 3, 'clf__learning_rate': 0.05, 'clf__subsample': 0.9, 'clf__colsample_bytree': 0.9, 'clf__scale_pos_weight': 1.472342264477096} -> mean_f1=0.3841 std=0.1188\n",
      "Params: {'clf__n_estimators': 200, 'clf__max_depth': 6, 'clf__learning_rate': 0.05, 'clf__subsample': 0.8, 'clf__colsample_bytree': 0.8, 'clf__scale_pos_weight': 1.472342264477096} -> mean_f1=0.3837 std=0.1106\n",
      "\n",
      "Tuning finished in 203.1s\n",
      "\n",
      "Best (LR): {'clf__penalty': 'l2', 'clf__C': 10.0, 'clf__solver': 'saga'}  -> mean_f1: 0.41450558574051516\n",
      "Best (SVM): {'clf__C': 0.1, 'clf__gamma': 0.01}  -> mean_f1: 0.422408519140255\n",
      "Best (XGB): {'clf__n_estimators': 200, 'clf__max_depth': 3, 'clf__learning_rate': 0.05, 'clf__subsample': 0.7, 'clf__colsample_bytree': 0.7, 'clf__scale_pos_weight': 1.472342264477096}  -> mean_f1: 0.40366345073003573\n",
      "\n",
      "Evaluating chosen best models on holdout test set:\n",
      "LR_purged: {'acc': 0.49825296995108315, 'prec': 0.4244791666666667, 'rec': 0.5415282392026578, 'f1': 0.4759124087591241, 'auc': 0.5156094081249073, 'confusion': array([[387, 442],\n",
      "       [276, 326]])}\n",
      "SVM_purged: {'acc': 0.43116701607267643, 'prec': 0.42395982783357244, 'rec': 0.9817275747508306, 'f1': 0.5921843687374749, 'auc': 0.5094948883696886, 'confusion': array([[ 26, 803],\n",
      "       [ 11, 591]])}\n",
      "XGB_purged: {'acc': 0.5136268343815513, 'prec': 0.4276923076923077, 'rec': 0.46179401993355484, 'f1': 0.4440894568690096, 'auc': 0.5114896464939948, 'confusion': array([[457, 372],\n",
      "       [324, 278]])}\n",
      "\n",
      "Done. Best estimators stored as globals: best_lr_purged, best_svm_purged, best_xgb_purged\n"
     ]
    }
   ],
   "source": [
    "# === CELL: Purged-time-series hyperparameter tuning (run this) ===\n",
    "import math, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# --- PurgedTimeSeriesSplit (simple, embargo in number of rows) ---\n",
    "class PurgedTimeSeriesSplit:\n",
    "    def __init__(self, n_splits=5, embargo=1):\n",
    "        if n_splits < 2:\n",
    "            raise ValueError(\"n_splits must be >=2\")\n",
    "        self.n_splits = n_splits\n",
    "        self.embargo = int(embargo)\n",
    "\n",
    "    def split(self, X):\n",
    "        n = len(X)\n",
    "        fold_size = n // self.n_splits\n",
    "        indices = np.arange(n)\n",
    "        for i in range(self.n_splits):\n",
    "            test_start = i * fold_size\n",
    "            # last fold takes remainder\n",
    "            test_end = ((i + 1) * fold_size) if i < (self.n_splits - 1) else n\n",
    "            test_idx = indices[test_start:test_end]\n",
    "            # train indices are all indices before test_start and after test_end\n",
    "            # apply embargo: remove up to `embargo` rows following test_end and preceding test_start\n",
    "            train_left_end = test_start - self.embargo\n",
    "            train_right_start = test_end + self.embargo\n",
    "            left = indices[:max(0, train_left_end)]\n",
    "            right = indices[train_right_start:] if train_right_start < n else np.array([], dtype=int)\n",
    "            train_idx = np.concatenate([left, right]) if len(left) or len(right) else np.array([], dtype=int)\n",
    "            yield train_idx, test_idx\n",
    "\n",
    "    def get_n_splits(self, X=None):\n",
    "        return self.n_splits\n",
    "\n",
    "# --- 0) Basic checks and prepare dataset ---\n",
    "if \"model_df_news_with_finbert\" not in globals():\n",
    "    raise RuntimeError(\"model_df_news_with_finbert not found in globals(). Make sure you ran the merge step.\")\n",
    "\n",
    "df = model_df_news_with_finbert.copy()\n",
    "label_col = \"target_thresholded\"\n",
    "if label_col not in df.columns:\n",
    "    raise RuntimeError(f\"Label column '{label_col}' not found in dataframe.\")\n",
    "\n",
    "# Select numeric features and drop leakage columns if present\n",
    "drop_cols = {label_col, \"future_return_h\", \"close_future_h\", \"close_next\", \"ts_str\", \"date\", \"time\"}\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "features = [c for c in num_cols if c not in drop_cols]\n",
    "print(f\"Using {len(features)} numeric features for tuning (example: {features[:8]})\")\n",
    "\n",
    "X = df[features].copy().reset_index(drop=True)\n",
    "y = df[label_col].astype(int).reset_index(drop=True)\n",
    "\n",
    "# Train/test split (time-based): keep same split approach you used before (last ~20% as test)\n",
    "n = len(X)\n",
    "test_size = int(math.ceil(n * 0.20))\n",
    "train_size = n - test_size\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "print(f\"Train size: {len(X_train)}  Test size: {len(X_test)}\")\n",
    "\n",
    "# --- 1) Setup Purged CV and param grids ---\n",
    "n_splits = 5\n",
    "embargo = 3   # adjust as needed (in rows); you used 1 or 3 earlier\n",
    "ptscv = PurgedTimeSeriesSplit(n_splits=n_splits, embargo=embargo)\n",
    "\n",
    "# small, sensible param grids (keeps run-time reasonable)\n",
    "lr_grid = [\n",
    "    {\"clf__penalty\": \"l2\", \"clf__C\": 0.0005, \"clf__solver\": \"saga\"},\n",
    "    {\"clf__penalty\": \"l2\", \"clf__C\": 0.01, \"clf__solver\": \"saga\"},\n",
    "    {\"clf__penalty\": \"l2\", \"clf__C\": 0.1, \"clf__solver\": \"saga\"},\n",
    "    {\"clf__penalty\": \"l2\", \"clf__C\": 1.0, \"clf__solver\": \"saga\"},\n",
    "    {\"clf__penalty\": \"l2\", \"clf__C\": 10.0, \"clf__solver\": \"saga\"},\n",
    "]\n",
    "\n",
    "svm_grid = [\n",
    "    {\"clf__C\": 0.1, \"clf__gamma\": 0.01},\n",
    "    {\"clf__C\": 1.0, \"clf__gamma\": 0.01},\n",
    "    {\"clf__C\": 5.0, \"clf__gamma\": 0.01},\n",
    "    {\"clf__C\": 1.0, \"clf__gamma\": \"scale\"},\n",
    "]\n",
    "\n",
    "xgb_grid = [\n",
    "    {\"clf__n_estimators\": 200, \"clf__max_depth\": 3, \"clf__learning_rate\": 0.05, \"clf__subsample\": 0.7, \"clf__colsample_bytree\": 0.7},\n",
    "    {\"clf__n_estimators\": 400, \"clf__max_depth\": 3, \"clf__learning_rate\": 0.05, \"clf__subsample\": 0.9, \"clf__colsample_bytree\": 0.9},\n",
    "    {\"clf__n_estimators\": 200, \"clf__max_depth\": 6, \"clf__learning_rate\": 0.05, \"clf__subsample\": 0.8, \"clf__colsample_bytree\": 0.8},\n",
    "]\n",
    "\n",
    "# pipelines\n",
    "pipe_lr = Pipeline([(\"scaler\", RobustScaler()), (\"clf\", LogisticRegression(class_weight=\"balanced\", max_iter=5000, random_state=42))])\n",
    "pipe_svm = Pipeline([(\"scaler\", RobustScaler()), (\"clf\", SVC(kernel=\"rbf\", probability=True, class_weight=\"balanced\", random_state=42))])\n",
    "pipe_xgb = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42))])\n",
    "\n",
    "# Helper to evaluate a pipeline on purged CV folds\n",
    "def evaluate_params(pipe, param_dict, X_train, y_train, ptscv):\n",
    "    pipe = Pipeline(pipe.steps)  # copy\n",
    "    pipe.set_params(**param_dict)\n",
    "    f1s = []\n",
    "    aucs = []\n",
    "    folds_info = []\n",
    "    for fold_i, (tr_idx, te_idx) in enumerate(ptscv.split(X_train)):\n",
    "        if len(tr_idx) == 0 or len(te_idx) == 0:\n",
    "            f1s.append(np.nan)\n",
    "            aucs.append(np.nan)\n",
    "            folds_info.append(None)\n",
    "            continue\n",
    "        Xtr, Xte = X_train.iloc[tr_idx], X_train.iloc[te_idx]\n",
    "        ytr, yte = y_train.iloc[tr_idx], y_train.iloc[te_idx]\n",
    "        try:\n",
    "            pipe.fit(Xtr, ytr)\n",
    "            ypred = pipe.predict(Xte)\n",
    "            yproba = None\n",
    "            try:\n",
    "                yproba = pipe.predict_proba(Xte)[:,1]\n",
    "            except Exception:\n",
    "                yproba = None\n",
    "            f1 = f1_score(yte, ypred, zero_division=0)\n",
    "            auc = roc_auc_score(yte, yproba) if yproba is not None else np.nan\n",
    "            f1s.append(f1); aucs.append(auc)\n",
    "            folds_info.append((fold_i, len(tr_idx), len(te_idx), f1))\n",
    "        except Exception as e:\n",
    "            # on error mark NaN and continue\n",
    "            f1s.append(np.nan); aucs.append(np.nan)\n",
    "            folds_info.append((\"error\", str(e)))\n",
    "    # compute mean across valid folds\n",
    "    valid = [v for v in f1s if not (isinstance(v, float) and np.isnan(v))]\n",
    "    mean_f1 = np.nan if len(valid) == 0 else float(np.nanmean(valid))\n",
    "    std_f1 = np.nanstd(valid) if len(valid) else np.nan\n",
    "    return {\"mean_f1\": mean_f1, \"std_f1\": std_f1, \"folds\": folds_info, \"f1s\": f1s, \"aucs\": aucs}\n",
    "\n",
    "# --- 2) Run grid search (simple brute-force over small grids) ---\n",
    "results_summary = {\"lr\": [], \"svm\": [], \"xgb\": []}\n",
    "\n",
    "start_all = time.time()\n",
    "print(\"Starting purged-CV tuning: n_splits=\", n_splits, \"embargo=\", embargo)\n",
    "# Logistic Regression\n",
    "print(\"\\nTuning LogisticRegression (balanced) ...\")\n",
    "for params in lr_grid:\n",
    "    res = evaluate_params(pipe_lr, params, X_train.reset_index(drop=True), y_train.reset_index(drop=True), ptscv)\n",
    "    results_summary[\"lr\"].append((params, res))\n",
    "    print(f\"Params: {params} -> mean_f1={res['mean_f1']:.4f} std={res['std_f1']:.4f}\")\n",
    "\n",
    "# SVM\n",
    "print(\"\\nTuning SVM (RBF, class_weight=balanced) ...\")\n",
    "for params in svm_grid:\n",
    "    # set 'clf__kernel' already in pipeline; pass C and gamma\n",
    "    p = {\"clf__C\": params[\"clf__C\"], \"clf__gamma\": params[\"clf__gamma\"]}\n",
    "    res = evaluate_params(pipe_svm, p, X_train.reset_index(drop=True), y_train.reset_index(drop=True), ptscv)\n",
    "    results_summary[\"svm\"].append((p, res))\n",
    "    print(f\"Params: {p} -> mean_f1={res['mean_f1']:.4f} std={res['std_f1']:.4f}\")\n",
    "\n",
    "# XGBoost (use scale_pos_weight computed from train)\n",
    "print(\"\\nTuning XGBoost ...\")\n",
    "# compute scale_pos_weight from training set\n",
    "pos = int(y_train.sum())\n",
    "neg = len(y_train) - pos\n",
    "spw = max(1.0, float(neg) / max(1, pos))\n",
    "print(\"scale_pos_weight (train):\", round(spw,3))\n",
    "for params in xgb_grid:\n",
    "    p = params.copy()\n",
    "    p[\"clf__scale_pos_weight\"] = spw\n",
    "    res = evaluate_params(pipe_xgb, p, X_train.reset_index(drop=True), y_train.reset_index(drop=True), ptscv)\n",
    "    results_summary[\"xgb\"].append((p, res))\n",
    "    print(f\"Params: {p} -> mean_f1={res['mean_f1']:.4f} std={res['std_f1']:.4f}\")\n",
    "\n",
    "elapsed_all = time.time() - start_all\n",
    "print(f\"\\nTuning finished in {elapsed_all:.1f}s\")\n",
    "\n",
    "# --- 3) Pick best for each estimator and fit on full training data ---\n",
    "def pick_best(results_list):\n",
    "    best = None\n",
    "    best_score = -np.inf\n",
    "    for params, res in results_list:\n",
    "        score = res[\"mean_f1\"] if res[\"mean_f1\"] is not None else -np.inf\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best = (params, res)\n",
    "    return best\n",
    "\n",
    "best_lr_params, best_lr_res = pick_best(results_summary[\"lr\"])\n",
    "best_svm_params, best_svm_res = pick_best(results_summary[\"svm\"])\n",
    "best_xgb_params, best_xgb_res = pick_best(results_summary[\"xgb\"])\n",
    "\n",
    "print(\"\\nBest (LR):\", best_lr_params, \" -> mean_f1:\", best_lr_res[\"mean_f1\"])\n",
    "print(\"Best (SVM):\", best_svm_params, \" -> mean_f1:\", best_svm_res[\"mean_f1\"])\n",
    "print(\"Best (XGB):\", best_xgb_params, \" -> mean_f1:\", best_xgb_res[\"mean_f1\"])\n",
    "\n",
    "# Fit final estimators on the entire train set using best params\n",
    "final_lr = Pipeline([(\"scaler\", RobustScaler()), (\"clf\", LogisticRegression(class_weight=\"balanced\", max_iter=5000, random_state=42))])\n",
    "final_lr.set_params(**best_lr_params)\n",
    "final_lr.fit(X_train, y_train)\n",
    "\n",
    "final_svm = Pipeline([(\"scaler\", RobustScaler()), (\"clf\", SVC(kernel=\"rbf\", probability=True, class_weight=\"balanced\", random_state=42))])\n",
    "final_svm.set_params(**best_svm_params)\n",
    "final_svm.fit(X_train, y_train)\n",
    "\n",
    "final_xgb = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42))])\n",
    "final_xgb.set_params(**best_xgb_params)\n",
    "final_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Save to globals\n",
    "globals()[\"best_lr_purged\"] = final_lr\n",
    "globals()[\"best_svm_purged\"] = final_svm\n",
    "globals()[\"best_xgb_purged\"] = final_xgb\n",
    "globals()[\"purged_tuning_results\"] = results_summary\n",
    "\n",
    "# --- 4) Quick evaluation on holdout test set ---\n",
    "def eval_on_test(pipe, X_test, y_test):\n",
    "    ypred = pipe.predict(X_test)\n",
    "    try:\n",
    "        yproba = pipe.predict_proba(X_test)[:,1]\n",
    "    except Exception:\n",
    "        yproba = None\n",
    "    out = {\n",
    "        \"acc\": accuracy_score(y_test, ypred),\n",
    "        \"prec\": precision_score(y_test, ypred, zero_division=0),\n",
    "        \"rec\": recall_score(y_test, ypred, zero_division=0),\n",
    "        \"f1\": f1_score(y_test, ypred, zero_division=0),\n",
    "        \"auc\": (roc_auc_score(y_test, yproba) if yproba is not None else None),\n",
    "        \"confusion\": confusion_matrix(y_test, ypred)\n",
    "    }\n",
    "    return out\n",
    "\n",
    "print(\"\\nEvaluating chosen best models on holdout test set:\")\n",
    "print(\"LR_purged:\", eval_on_test(final_lr, X_test, y_test))\n",
    "print(\"SVM_purged:\", eval_on_test(final_svm, X_test, y_test))\n",
    "print(\"XGB_purged:\", eval_on_test(final_xgb, X_test, y_test))\n",
    "\n",
    "print(\"\\nDone. Best estimators stored as globals: best_lr_purged, best_svm_purged, best_xgb_purged\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26e96702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best_lr_purged -> saved_models\\best_lr_purged.pkl\n",
      "Saved best_svm_purged -> saved_models\\best_svm_purged.pkl\n",
      "Saved best_xgb_purged -> saved_models\\best_xgb_purged.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save tuned purged models to disk (pickles)\n",
    "import joblib, pathlib\n",
    "save_dir = pathlib.Path(\"saved_models\")\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for name in [\"best_lr_purged\", \"best_svm_purged\", \"best_xgb_purged\"]:\n",
    "    if name in globals():\n",
    "        path = save_dir / f\"{name}.pkl\"\n",
    "        joblib.dump(globals()[name], path)\n",
    "        print(f\"Saved {name} -> {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b279afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# best_lr_purged = joblib.load(\"saved_models/best_lr_purged.pkl\")\n",
    "# import joblib\n",
    "# best_lr_purged = joblib.load(\"saved_models/best_lr_purged.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
